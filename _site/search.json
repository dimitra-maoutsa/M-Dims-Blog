[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/11_02-from_pdes_to_gradient_flows.html",
    "href": "posts/11_02-from_pdes_to_gradient_flows.html",
    "title": "From PDEs to gradient flows for deterministic particle dynamics",
    "section": "",
    "text": "We consider PDEs that describe the evolution of a density \\(\\rho_t(x)\\) that evolves in time, with \\(x\\in \\mathcal{R}^D\\). We describe the temporal evolution of the density \\(\\rho_t(x)\\), e.g., a density of particles at location \\(x\\). One fundamental equation is the continuity equation, which prescribes how the the density \\(\\rho_t(x)\\) evolves in time according to laws of mass conservation. In particular, the continuity equation declares that the time derivative of the density \\(\\rho_t(x)\\) plus some velocity field \\(v\\) \\[\\begin{equation}\n    \\partial_t \\rho_t(x) + \\nabla \\cdot \\left(  v(x,t) \\rho_t(x) \\right) = 0,\n\\end{equation}\\] given some initial condition \\(\\rho_0(x) = \\rho^0(x)\\). The velocity field in this equation prescribes spatial transformation of the density as time evolves, i.e., how a density of particles starting from \\(\\rho^0(x)\\) moves according to the velocity field \\(v(x,t)\\).\nThe continuity equation admits a useful discretisation in terms of particles. We can consider an associated ordinary differential equation. We can consider the evolution of \\(N\\) particles in the Euclidean space, evolving according to the ODE \\[\\begin{equation}\n    \\frac{dX_i(t)}{dt} = v(X_i(t),t) ,\n\\end{equation}\\] given some initial conditions \\(X_i(0)\\). There is a close correspondence between this system of ODEs (the particles) and the solutions of the PDE. In particular, if the velocity field is sufficiently nice (globally Lipschitz in space) , then as long as the iniital conditions of the particles are drawn from the density representing the initial condition of the PDE, i.e. if I take the empirical measure at each of these locations, i.e., \\[\\begin{equation}\n    \\hat{\\rho}_0 = \\frac{1}{N} \\sum^N_{i=1} \\delta(x-X_i(0)) \\xrightarrow{N \\rightarrow \\infty} \\rho_0(x),\n\\end{equation}\\] then the evolving locations of these Dirac masses converges according to the Wasserstein metric to the continuum solution of the PDE, \\[\\begin{equation}\n    \\hat{\\rho}_t = \\frac{1}{N} \\sum^N_{i=1} \\delta(x-X_i(t)) \\xrightarrow{N \\rightarrow \\infty} \\rho_t(x),\n\\end{equation}\\]\nBecause of this close connection between the PDEs and the particle representation a lot of PDEs have a natural discretisation in terms of particles. The PDE conserves mass, i.e. whatever the integral of the initial data is that integral will be preserved over time. Also particle representation preserves positivity.\nIn general this equation is a Wasserstein gradient flow for an arbitrary velocity field.\nThe difference between the transient empirical solution \\(\\hat{\\rho}_t\\) and the exact solution \\(\\rho_t\\) within the time interval \\(t \\in[0,T]\\) will be bounded by a constant weighted by the initial distance of the initial condition\n\\[\\begin{equation}\n    \\mathcal{W}_2(\\hat{\\rho}_t,\\rho_t) \\leq C_{T,\\|  \\nabla v\\|_{\\infty}} \\mathcal{W}_2(\\hat{\\rho}_0, \\rho_0)\n\\end{equation}\\]\n##Fokker Planck equations as gradient flows An equation that is a Wasserstein gradient flow and has attracted a lot of interest in the last years is the Fokker-Planck equation. It describes the evolution of a density according to adrift term \\[\\begin{equation}\n    \\partial_t \\rho_t(x) = \\nabla \\cdot \\left( \\nabla V \\rho_t(x)\\right) + \\nabla \\nabla \\rho_t(x).\n\\end{equation}\\] This has an associated particle method, a stochastic one, because we have the diffusion present \\[\\begin{equation}\n    dX_t = -V(X_t)dt + \\sqrt{2} dW_t\n\\end{equation}\\] The empirical measure represented in terms of particles will converge almost surely to the solution of the PDE.\nThis PDE has a corresponding particle discretisation. Each particle evolves according to this Ordinary differential equation.\nSince the density interacts with itself, the particles interact with each other through the second term.\nThis equation has a Wasserstein gradient flow structure. It is the gradient flow of this energy \\[\\begin{equation}\n    \\mathcal{E}(\\rho) = \\int V(x) \\rho(x) dx + \\int \\rho(x) \\log \\rho(x) dx\n\\end{equation}\\] it has an external potential term, and the second term is the negative entropy. The particles are going at a direction negative of the gradient of the energy landscape for conservative systems, where the potential is small, to make the energy smaller. The diffusion term forces the density to spread out, so this term makes the entropy smaller.\nFokker-Planck equation can be viewed as a gradient flow. The central point of this idea is to define a manifold on which the Fokker-Planck system is a dynamical system on the manifold and evolving according to its gradient.\nBy understanding convexity properties of the function V that represents the potential can inform us about convexity properties of the energy landscape, and from there we can recover properties of the Fokker-Planck equation, i.e. contraction of solutions, exponential convergence to the equilibrium, etc.\nThe particle solution is not an Wasserstein gradient flow of this energy. The reason for this is that I could write the PDE as a continuity type of equation \\[\\begin{equation}\n    \\partial_t \\rho_t(x) = \\nabla \\cdot \\left[ \\underbrace{( \\nabla V + \\frac{\\nabla \\rho}{\\rho})}_{\\text{velocity field:} v(x,t)} \\rho \\right]\n\\end{equation}\\] But this is a weird velocity field and for a general density the particle method will not be well defined. Due to the diffusion the instantaneous Dirac masses will not remain Dirac masses.\nThe Wasserstein space is the space of probability measures on \\(\\mathcal{R}^N\\) with the metric induced by the Wasserstein distance.\nThe seminal work of Jordan-Kinderlehrer-Otto~ established the view of the Fokker–Planck equation as a gradient flow of the Kullback Leibler divergence functional on a probability space equipped with a Wasserstein metric. The solution of the Fokker–Planck equation with drift forces arising as a gradient of a potential \\(V(x)\\), i.e., \\(f(x)=\\nabla V(x)\\) was identified as the gradient flow of the free energy with respect to the Wasserstein metric. For a gradient system, the free energy difference between two states \\(\\delta F\\) amounts to the negative entropy production times the temperature \\(\\delta F = - \\mathcal{T} \\mathcal{S}\\), where \\(\\mathcal{S}\\) stands for the entropy production. Thus this formulation may be viewed as a maximum entropy principle for the Fokker Planck.\nThe Fokker-Planck equation can be viewed as as the gradient flow in the Wasserstein metric of the relative entropy functional \\[\\begin{equation}\n    S(\\rho) = \\int_{\\mathcal{R}^d} \\rho(x) \\log\\left( \\frac{\\rho(x)}{e^{-V(x)}} \\right)dx.\n\\end{equation}\\]\nMajor computational challenge of the jko scheme is how to computationally efficiently compute the optimal trnasport cost.\nThe optimal transport yields geodesics in the Wasserstein space [cite Villani old and new]. The evolution of the probability density described by the Fokker–Planck equation amounts to the a curve in the Wasserstein space, the actual length of this curve is identified as the distance between the initial and the terminal points if this curve is a geodesic. All other curves the connect the two measures have larger dissipation. Optimal transport protocols correspond to geodesics in the Wasserstein space and can be employed in an equivalent definition of curvature."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/08_03_probability_flow_dynamics.html",
    "href": "posts/08_03_probability_flow_dynamics.html",
    "title": "Probability flow dynamics for constraining stochastic nonlinear systems",
    "section": "",
    "text": "Biological and physical systems are often subjected to intrinsic or extrinsic noise sources that influence their dynamics. Characteristic examples include molecular reactions and chemical kinetics CITE, populations of animal species, biological neurons CITE, and evolution CITE,CITE. Stochastic differential equations (SDEs) effectively capture the phenomenology of the dynamics of such systems, at different precision scales by both considering deterministic and stochastic forces affecting their state variables \\(X_t \\in  \\mathcal{R}^d\\) following\n\\[dX_t = f(X_t,t) dt  + \\sigma dW_t. \\]\nIn Eq.\\((1)\\) the drift \\(f(\\cdot,\\cdot): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) is a smooth typically nonlinear function that captures the deterministic part of the driving forces, while \\(W\\) stands for a k–dimensional (\\(k\\leq d\\)) vector of independent Wiener processes acting as white noise sources, representing contributions from unaccounted degrees of freedom, thermal fluctuations, or external perturbations. We denote the noise strength by \\(\\sigma \\in \\mathcal{R}\\)1, and define the noise covariance as \\({D =\\sigma ^2}\\). In the following we refer to this system as the system.\nUnder multiple independent realisations, the stochastic nature of Eq.\\((1)\\) gives rise to an ensemble of trajectories starting from an initial state \\(X_0=x_0\\). This ensemble captures the likely evolution of the considered system at later time points. We may characterise the unfolding of this trajectory ensemble in terms of a probability density \\(p_t(x)\\), whose evolution is governed by the Fokker–Planck equation\n\\[\\frac{\\partial p_t(x)}{\\partial t}\n= \\nabla\\cdot \\left[- f(x,t) p_t (x) + \\frac{\\sigma^2}{2} \\nabla p_t(x)\\right]\\] \\[ \\hspace{-57pt}= {\\cal{L}}_f^\\dagger p_t(x) ,\\]\nwith initial condition \\(p_0(x) = \\delta(x-x_0)\\), and \\(\\mathcal{L}_f^\\dagger\\) denoting the Fokker–Planck operator. Due to the stochastic nature of the system of Eq.\\((1)\\), exact pinpointing of its state at some later time point \\(T\\) is in general not possible.\nYet, often, we desire to drive biophysical and biochemical stochastic processes to predefined target states within a specified time interval. Characteristic examples include designing artificial selection strategies for population dynamics cite, or triggering phenotype switches during cell fate determination cite. Similar needs for manipulation are also relevant for non-biological, but rather technical systems, e.g. for control of robotic or artificial limbs cite, cite. In all these settings, external system interventions become essential.\nHere, we are interested in introducing constraints \\(\\mathcal{C}\\) to the dynamics of the system of Eq.(\\(1\\)) acting within a predefined time interval \\({0 \\leq t \\leq T}\\). The set of possible constraints \\(\\mathcal{C}\\) comprises terminal \\(\\chi(X_T)\\), and/or path constraints $U(x,t), tT $, depending on whether the desired limiting conditions apply for the entire interval or only to the terminal time point. The path constraints $U(x,t): ^{d} $ penalise specific trajectories (paths) to render specific regions of the state space more (un)likely to be visited, while the function \\(\\chi(x): \\mathcal{R}^{d} \\rightarrow \\mathcal{R}\\) influences the terminal system state \\(X_T\\).\nTo incorporate the constraints \\(\\mathcal{C}\\) into the system, we define a modified dynamics, the controlled dynamics, through a change of probability measure of the path ensemble \\(\\mathbb{P}_f\\) induced by the uncontrolled system. More precisely, we consider the path measure \\(\\mathbb{Q}\\) (Appendix A), induced by the controlled system, as equivalent to a reweighting of paths \\(X_{0:T}\\) generated from the uncontrolled dynamics (Eq.\\((1)\\)) over the time interval \\([0,\\; T]\\). Individual path weights are thus given by the likelihood ratio (Radon–Nikodym derivative)\n\\[\\frac{d\\mathbb{Q}}{d\\mathbb{P}_f} (X_{0:T}) = \\frac{\\chi(X_T)}{Z} \\exp\\left[- \\int_0^T U(X_t,t) dt \\right],\\]\nwhere \\(Z\\) denotes the normalising constant\n\\[Z = \\Bigg \\langle \\chi(X_T) \\exp\\left(- \\int_0^T U(X_t,t) dt \\right) \\Bigg\\rangle_{\\mathbb{P}_f},\\]\nand \\(\\langle \\cdot \\rangle_{\\mathbb{P}_f}\\) denotes the expectation over paths of the uncontrolled system.\nAccording to the Girsanov’s theorem, the controlled process defined by the weights of Eq.\\((4)\\) is also a diffusion process with the same diffusion constant \\(\\sigma\\), but with a modified, time-dependent drift function \\(g(x,t): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) cite, cite. Thus, we express the controlled dynamics as a time- and state- dependent perturbation \\(u(x,t): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) of the deterministic forces \\(f(x,t)\\) acting on the system\n\\[ dX_t = \\Big( f(X_t,t)   + u(X_t,t) \\Big) \\; dt + \\sigma dW_t \\] \\[= \\hspace{25pt}g(X_t,t)\\;\\hspace{5pt} dt \\hspace{30pt}+ \\sigma dW_t.\\]\nOur goal is to identify the optimal time- and state-dependent interventions \\(u(x,t)\\) that minimise intervention costs and path constraints captured by the cost function\n\\[S(x,u,t) =  \\frac{1}{2} u(x,t)^T H u(x,t)+ U(x,t),\\]\nwhile also drive the system towards a predefined target state \\(x^*\\) by time \\(T\\), if a terminal constraint is pertinent. The first part of the cost function penalises large intervention values \\(u(x,t)\\), with \\(H \\in \\mathcal{R}^{d \\times d}\\) determining the cost of intervention along each system dimension, whereas the path cost \\(U(x,t)\\) constrains the transient behaviour of the system.\nSolutions of this type of stochastic control problems rest on the Bellman’s principle of optimality, according to which an optimal solution over an interval \\([0,\\;T]\\) consists of optimal sub-solutions over the respective sub-intervals \\([t',\\;T]\\) with later starting times \\(t'\\), and appropriate initial conditions cite. This sequence of sub-problems with interdependent initial conditions requires the cost function \\(S(x,u,t)\\) to be minimized over the entire time interval \\([0,\\;T]\\). Therefore, here, we minimize the total expected cost in that interval defined as the sum of the terminal cost \\(\\chi(X_T)\\) and the time integrated path and intervention costs\n\\[ J(x,t=0) = \\min_{u} \\Big\\langle  \\int_{t=0}^T S(x,u,t') \\,  dt' -  \\ln \\chi(X_T) \\Big\\rangle_{\\mathbb{Q}}. \\] In Eq.\\((6)\\), the brackets \\(\\langle \\cdot \\rangle_{\\mathbb{Q}}\\) denote the expectation over the entire path probability measure \\(\\mathbb{Q}\\).\nTo establish the optimality of the interventions, we demand the cost functional \\(J(x,t)\\) to follow the Hamilton–Jacobi–Bellman (HJB) equation (Appendix),\n\\[  -\\frac{\\partial}{\\partial t} J(X_t,t) = \\min_u \\Bigg[ \\frac{1}{2} u^T(X_t) H u(X_t) + U(X_t,t)\\] \\[\\hspace{95pt} + g(X_t,t) \\nabla_x J(X_t,t) + \\frac{1}{2} \\text{Tr}[D \\frac{\\partial^2}{\\partial x^2} J(X_t,t)]  \\Bigg] \\] a nonlinear partial differential equation (PDE) with a terminal condition \\(J(x,T)= \\ln \\chi(X_T)\\), which is, therefore, solved backwards in time. The gradient of the solution of this equation\n\\[u^*(x,t) = - H^{-1}  \\nabla J(x,t),\\]\nprovides the optimal state- and time-dependent interventions for the considered system with constraints \\(\\mathcal{C}\\). Yet, without investigating the structure of the solution, direct solving a second-order nonlinear PDE requires computationally demanding calculations, that grow exponentially with increasing system dimension.\nTo simplify matters, we linearise the Hamilton–Jacobi–Bellman equation by employing a logarithmic variable transformation, \\(J(x,t) = - \\log( \\phi(x,t))\\), proposed initially by Nelson in cite, and introduced in the context of stochastic control by Fleming in cite (Hopf-Cole transform). This requires the minimal assumption of the control costs \\(H\\) and noise covariance \\(D\\) being inversely proportional along each state dimension, \\(H \\propto D^{-1}=\\sigma^{-2}\\), known in the literature as the path integral control condition cite.\nThe logarithmic variable transformation allows us to express the resulting controlled drift\n\\[g(x,t)  = f(x,t) +  \\sigma^2 \\nabla \\ln \\phi(x,t), \\]\nin terms of the solution $_t(x) (x,t) $ of a linear backward partial differential equation\n\\[\\frac{\\partial \\phi_t(x)}{\\partial t} + {\\cal{L}}_f \\phi_t(x) - U(x,t) \\phi_t(x) = 0 ,\\]\nwith terminal condition $_T(x) = (X_T) $, and with \\(\\mathcal{L}_f\\) denoting the adjoint Fokker–Planck operator (Appendix)."
  },
  {
    "objectID": "posts/08_03_probability_flow_dynamics.html#dynamics-of-constrained-densities",
    "href": "posts/08_03_probability_flow_dynamics.html#dynamics-of-constrained-densities",
    "title": "Probability flow dynamics for constraining stochastic nonlinear systems",
    "section": "",
    "text": "Biological and physical systems are often subjected to intrinsic or extrinsic noise sources that influence their dynamics. Characteristic examples include molecular reactions and chemical kinetics CITE, populations of animal species, biological neurons CITE, and evolution CITE,CITE. Stochastic differential equations (SDEs) effectively capture the phenomenology of the dynamics of such systems, at different precision scales by both considering deterministic and stochastic forces affecting their state variables \\(X_t \\in  \\mathcal{R}^d\\) following\n\\[dX_t = f(X_t,t) dt  + \\sigma dW_t. \\]\nIn Eq.\\((1)\\) the drift \\(f(\\cdot,\\cdot): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) is a smooth typically nonlinear function that captures the deterministic part of the driving forces, while \\(W\\) stands for a k–dimensional (\\(k\\leq d\\)) vector of independent Wiener processes acting as white noise sources, representing contributions from unaccounted degrees of freedom, thermal fluctuations, or external perturbations. We denote the noise strength by \\(\\sigma \\in \\mathcal{R}\\)1, and define the noise covariance as \\({D =\\sigma ^2}\\). In the following we refer to this system as the system.\nUnder multiple independent realisations, the stochastic nature of Eq.\\((1)\\) gives rise to an ensemble of trajectories starting from an initial state \\(X_0=x_0\\). This ensemble captures the likely evolution of the considered system at later time points. We may characterise the unfolding of this trajectory ensemble in terms of a probability density \\(p_t(x)\\), whose evolution is governed by the Fokker–Planck equation\n\\[\\frac{\\partial p_t(x)}{\\partial t}\n= \\nabla\\cdot \\left[- f(x,t) p_t (x) + \\frac{\\sigma^2}{2} \\nabla p_t(x)\\right]\\] \\[ \\hspace{-57pt}= {\\cal{L}}_f^\\dagger p_t(x) ,\\]\nwith initial condition \\(p_0(x) = \\delta(x-x_0)\\), and \\(\\mathcal{L}_f^\\dagger\\) denoting the Fokker–Planck operator. Due to the stochastic nature of the system of Eq.\\((1)\\), exact pinpointing of its state at some later time point \\(T\\) is in general not possible.\nYet, often, we desire to drive biophysical and biochemical stochastic processes to predefined target states within a specified time interval. Characteristic examples include designing artificial selection strategies for population dynamics cite, or triggering phenotype switches during cell fate determination cite. Similar needs for manipulation are also relevant for non-biological, but rather technical systems, e.g. for control of robotic or artificial limbs cite, cite. In all these settings, external system interventions become essential.\nHere, we are interested in introducing constraints \\(\\mathcal{C}\\) to the dynamics of the system of Eq.(\\(1\\)) acting within a predefined time interval \\({0 \\leq t \\leq T}\\). The set of possible constraints \\(\\mathcal{C}\\) comprises terminal \\(\\chi(X_T)\\), and/or path constraints $U(x,t), tT $, depending on whether the desired limiting conditions apply for the entire interval or only to the terminal time point. The path constraints $U(x,t): ^{d} $ penalise specific trajectories (paths) to render specific regions of the state space more (un)likely to be visited, while the function \\(\\chi(x): \\mathcal{R}^{d} \\rightarrow \\mathcal{R}\\) influences the terminal system state \\(X_T\\).\nTo incorporate the constraints \\(\\mathcal{C}\\) into the system, we define a modified dynamics, the controlled dynamics, through a change of probability measure of the path ensemble \\(\\mathbb{P}_f\\) induced by the uncontrolled system. More precisely, we consider the path measure \\(\\mathbb{Q}\\) (Appendix A), induced by the controlled system, as equivalent to a reweighting of paths \\(X_{0:T}\\) generated from the uncontrolled dynamics (Eq.\\((1)\\)) over the time interval \\([0,\\; T]\\). Individual path weights are thus given by the likelihood ratio (Radon–Nikodym derivative)\n\\[\\frac{d\\mathbb{Q}}{d\\mathbb{P}_f} (X_{0:T}) = \\frac{\\chi(X_T)}{Z} \\exp\\left[- \\int_0^T U(X_t,t) dt \\right],\\]\nwhere \\(Z\\) denotes the normalising constant\n\\[Z = \\Bigg \\langle \\chi(X_T) \\exp\\left(- \\int_0^T U(X_t,t) dt \\right) \\Bigg\\rangle_{\\mathbb{P}_f},\\]\nand \\(\\langle \\cdot \\rangle_{\\mathbb{P}_f}\\) denotes the expectation over paths of the uncontrolled system.\nAccording to the Girsanov’s theorem, the controlled process defined by the weights of Eq.\\((4)\\) is also a diffusion process with the same diffusion constant \\(\\sigma\\), but with a modified, time-dependent drift function \\(g(x,t): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) cite, cite. Thus, we express the controlled dynamics as a time- and state- dependent perturbation \\(u(x,t): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) of the deterministic forces \\(f(x,t)\\) acting on the system\n\\[ dX_t = \\Big( f(X_t,t)   + u(X_t,t) \\Big) \\; dt + \\sigma dW_t \\] \\[= \\hspace{25pt}g(X_t,t)\\;\\hspace{5pt} dt \\hspace{30pt}+ \\sigma dW_t.\\]\nOur goal is to identify the optimal time- and state-dependent interventions \\(u(x,t)\\) that minimise intervention costs and path constraints captured by the cost function\n\\[S(x,u,t) =  \\frac{1}{2} u(x,t)^T H u(x,t)+ U(x,t),\\]\nwhile also drive the system towards a predefined target state \\(x^*\\) by time \\(T\\), if a terminal constraint is pertinent. The first part of the cost function penalises large intervention values \\(u(x,t)\\), with \\(H \\in \\mathcal{R}^{d \\times d}\\) determining the cost of intervention along each system dimension, whereas the path cost \\(U(x,t)\\) constrains the transient behaviour of the system.\nSolutions of this type of stochastic control problems rest on the Bellman’s principle of optimality, according to which an optimal solution over an interval \\([0,\\;T]\\) consists of optimal sub-solutions over the respective sub-intervals \\([t',\\;T]\\) with later starting times \\(t'\\), and appropriate initial conditions cite. This sequence of sub-problems with interdependent initial conditions requires the cost function \\(S(x,u,t)\\) to be minimized over the entire time interval \\([0,\\;T]\\). Therefore, here, we minimize the total expected cost in that interval defined as the sum of the terminal cost \\(\\chi(X_T)\\) and the time integrated path and intervention costs\n\\[ J(x,t=0) = \\min_{u} \\Big\\langle  \\int_{t=0}^T S(x,u,t') \\,  dt' -  \\ln \\chi(X_T) \\Big\\rangle_{\\mathbb{Q}}. \\] In Eq.\\((6)\\), the brackets \\(\\langle \\cdot \\rangle_{\\mathbb{Q}}\\) denote the expectation over the entire path probability measure \\(\\mathbb{Q}\\).\nTo establish the optimality of the interventions, we demand the cost functional \\(J(x,t)\\) to follow the Hamilton–Jacobi–Bellman (HJB) equation (Appendix),\n\\[  -\\frac{\\partial}{\\partial t} J(X_t,t) = \\min_u \\Bigg[ \\frac{1}{2} u^T(X_t) H u(X_t) + U(X_t,t)\\] \\[\\hspace{95pt} + g(X_t,t) \\nabla_x J(X_t,t) + \\frac{1}{2} \\text{Tr}[D \\frac{\\partial^2}{\\partial x^2} J(X_t,t)]  \\Bigg] \\] a nonlinear partial differential equation (PDE) with a terminal condition \\(J(x,T)= \\ln \\chi(X_T)\\), which is, therefore, solved backwards in time. The gradient of the solution of this equation\n\\[u^*(x,t) = - H^{-1}  \\nabla J(x,t),\\]\nprovides the optimal state- and time-dependent interventions for the considered system with constraints \\(\\mathcal{C}\\). Yet, without investigating the structure of the solution, direct solving a second-order nonlinear PDE requires computationally demanding calculations, that grow exponentially with increasing system dimension.\nTo simplify matters, we linearise the Hamilton–Jacobi–Bellman equation by employing a logarithmic variable transformation, \\(J(x,t) = - \\log( \\phi(x,t))\\), proposed initially by Nelson in cite, and introduced in the context of stochastic control by Fleming in cite (Hopf-Cole transform). This requires the minimal assumption of the control costs \\(H\\) and noise covariance \\(D\\) being inversely proportional along each state dimension, \\(H \\propto D^{-1}=\\sigma^{-2}\\), known in the literature as the path integral control condition cite.\nThe logarithmic variable transformation allows us to express the resulting controlled drift\n\\[g(x,t)  = f(x,t) +  \\sigma^2 \\nabla \\ln \\phi(x,t), \\]\nin terms of the solution $_t(x) (x,t) $ of a linear backward partial differential equation\n\\[\\frac{\\partial \\phi_t(x)}{\\partial t} + {\\cal{L}}_f \\phi_t(x) - U(x,t) \\phi_t(x) = 0 ,\\]\nwith terminal condition $_T(x) = (X_T) $, and with \\(\\mathcal{L}_f\\) denoting the adjoint Fokker–Planck operator (Appendix)."
  },
  {
    "objectID": "posts/08_03_probability_flow_dynamics.html#footnotes",
    "href": "posts/08_03_probability_flow_dynamics.html#footnotes",
    "title": "Probability flow dynamics for constraining stochastic nonlinear systems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the sake of brevity, we consider here a state independent diffusion, but the formalism easily generalises for a state dependent diffusion \\(\\sigma(x)\\), as outlined in the Appendix.}↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quatro",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nFrom PDEs to gradient flows for deterministic particle dynamics\n\n\n\njupyter\n\n\n\nIntroduction to deterministic particle methods for PDEs.\n\n\n\n\n\nNov 2, 2022\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nProbability flow dynamics for constraining stochastic nonlinear systems\n\n\n\njupyter\n\n\n\nDetailed description of Deterministic Particle Flow control of (upcoming paper) on controling stochastic nonlinear systems by deterministacally perturbing their dynamics.\n\n\n\n\n\nAug 3, 2021\n\n\nDimitra Maoutsa\n\n\n\n\n\nNo matching items"
  }
]