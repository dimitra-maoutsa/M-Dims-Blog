[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a semi-organised collection of living texts on theoretical and computational neuroscience, nonlinear dynamics, probability theory, and machine learning theory, or whatever else manages to trigger my interest. \nSome early entries are adapted relics of my PhD thesis (including those bits that didn‚Äôt survive the final edits), while others are working notes that help me understand things better and might one day become something more formal.\nNew posts will appear irregularly (when I feel like it). Old ones will get updated when I gain new insights, realise I was wrong, or I am bored and feel like procrastinating by refining them.\nExpect mostly math, neurons, and the occasional rabbit hole commentary.\nThis blog is maintained by Dimitra Maoutsa, and is powered by quarto.\nSome thumbnails have been generated by DALL¬∑E."
  },
  {
    "objectID": "posts/22_11_02-from_pdes_to_gradient_flows.html",
    "href": "posts/22_11_02-from_pdes_to_gradient_flows.html",
    "title": "From PDEs to gradient flows for deterministic particle dynamics",
    "section": "",
    "text": "Let us consider PDEs that describe the evolution of a density \\(\\rho_t(x)\\) that evolves in time, with \\(x\\in \\mathcal{R}^D\\). We want to describe the temporal evolution of the density \\(\\rho_t(x)\\), e.g., a density of particles at location \\(x\\).\nOne fundamental equation for this is the continuity equation, which prescribes how the the density \\(\\rho_t(x)\\) evolves in time according to laws of mass conservation. In particular, the continuity equation expresses the conservation of mass by expressing that the time derivative of the density \\(\\rho_t(x)\\) plus the divergence of the product of a velocity field \\(v(x,t)\\) and the density must vanish \\[\\begin{equation}\n    \\partial_t \\rho_t(x) + \\nabla \\cdot \\left(  v(x,t) \\rho_t(x) \\right) = 0,\n\\end{equation}\\] given some initial condition \\(\\rho_0(x) = \\rho^0(x)\\). The velocity field in this equation prescribes a spatial transformation of the density as time evolves, i.e., how a density of particles starting from \\(\\rho^0(x)\\) moves according to the velocity field \\(v(x,t)\\).\nThe continuity equation admits a useful discretisation in terms of particles. We can define an associated ordinary differential equation for their evolving positions. In fact, we can consider the evolution of \\(N\\) particles in the Euclidean space, evolving according to the ODE \\[\\begin{equation}\n    \\frac{dX_i(t)}{dt} = v(X_i(t),t) ,\n\\end{equation}\\] given some initial conditions \\(X_i(0)\\).\nThere is a close correspondence between this system of ODEs (the particles) and the solutions of the PDE. In particular, if the velocity field is sufficiently nice (globally Lipschitz in space), then as long as the iniital conditions of the particles are drawn from the density representing the initial condition of the PDE, i.e.¬†if we take the empirical measure at each of the particle locations, i.e., \\[\\begin{equation}\n    \\hat{\\rho}_0 = \\frac{1}{N} \\sum^N_{i=1} \\delta(x-X_i(0)) \\xrightarrow{N \\rightarrow \\infty} \\rho_0(x),\n\\end{equation}\\] then the evolving locations of the Dirac masses (particles) converges according to the Wasserstein metric to the continuum solution of the PDE, \\[\\begin{equation}\n    \\hat{\\rho}_t = \\frac{1}{N} \\sum^N_{i=1} \\delta(x-X_i(t)) \\xrightarrow{N \\rightarrow \\infty} \\rho_t(x).\n\\end{equation}\\]\nBecause of this close connection between the PDEs and the particle representation, a lot of PDEs have a natural discretisation in terms of particles. The PDE conserves mass, i.e.¬†whatever the integral of the initial condition is, that integral will be preserved over time. And the particle representation preserves positivity.\nIn general this equation is a Wasserstein gradient flow for an arbitrary velocity field.\nThe difference between the transient empirical solution \\(\\hat{\\rho}_t\\) and the exact solution \\(\\rho_t\\) within the time interval \\(t \\in[0,T]\\) will be bounded by a constant weighted by the initial distance of the initial condition\n\\[\\begin{equation}\n    \\mathcal{W}_2(\\hat{\\rho}_t,\\rho_t) \\leq C_{T,\\|  \\nabla v\\|_{\\infty}} \\mathcal{W}_2(\\hat{\\rho}_0, \\rho_0).\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/22_11_02-from_pdes_to_gradient_flows.html#fokker-planck-equations-as-gradient-flows",
    "href": "posts/22_11_02-from_pdes_to_gradient_flows.html#fokker-planck-equations-as-gradient-flows",
    "title": "From PDEs to gradient flows for deterministic particle dynamics",
    "section": "Fokker Planck equations as gradient flows",
    "text": "Fokker Planck equations as gradient flows\nAn equation that is a Wasserstein gradient flow and has attracted a lot of interest in the last years is the Fokker-Planck equation. It describes the evolution of a density according to a drift term \\[\\begin{equation}\n    \\partial_t \\rho_t(x) = \\nabla \\cdot \\left( \\nabla V \\rho_t(x)\\right) + \\nabla \\nabla \\rho_t(x).\n\\end{equation}\\] This equation has an associated particle method, a stochastic one, because we have the diffusion term present \\[\\begin{equation}\n    dX_t = -V(X_t)dt + \\sqrt{2} dW_t.\n\\end{equation}\\] The empirical measure represented in terms of particles will converge almost surely to the solution of the PDE.\nHowever, this PDE has a corresponding particle discretisation, where each particle evolves according to an Ordinary differential equation\n[ prof flow ode ]\nSince the density interacts with itself, the particles interact with each other through the second term.\nThis equation has a Wasserstein gradient flow structure. It is the gradient flow of the following energy \\[\\begin{equation}\n    \\mathcal{E}(\\rho) = \\int V(x) \\rho(x) dx + \\int \\rho(x) \\log \\rho(x) dx\n\\end{equation}\\] it has an external potential term, and the second term is the negative entropy. The particles are going at a direction negative of the gradient of the energy landscape for conservative systems, where the potential is small, to make the energy smaller. The diffusion term forces the density to spread out, so this term makes the entropy bigger.\nFokker-Planck equation can be viewed as a gradient flow. The central point of this idea is to define a manifold on which the Fokker-Planck system is a dynamical system on the manifold and evolving according to its gradient.\nBy understanding the convexity properties of the function \\(V(x)\\) that represents the potential, can inform us about convexity properties of the energy landscape, and from there we can recover properties of the Fokker-Planck equation, i.e.¬†contraction of solutions, exponential convergence to the equilibrium, etc.\nThe particle solution is not a Wasserstein gradient flow of this energy. The reason for this is that I could write the PDE as a continuity type of equation \\[\\begin{equation}\n    \\partial_t \\rho_t(x) = \\nabla \\cdot \\left[ \\underbrace{( \\nabla V + \\frac{\\nabla \\rho}{\\rho})}_{\\text{velocity field:} v(x,t)} \\rho \\right]\n\\end{equation}\\] But this is a weird velocity field and for a general density the particle method will not be well defined. Due to the diffusion the instantaneous Dirac masses will not remain Dirac masses.\nThe Wasserstein space is the space of probability measures on \\(\\mathcal{R}^N\\) with the metric induced by the Wasserstein distance.\nThe seminal work of Jordan-Kinderlehrer-Otto~ established the view of the Fokker‚ÄìPlanck equation as a gradient flow of the Kullback Leibler divergence functional on a probability space equipped with a Wasserstein metric. The solution of the Fokker‚ÄìPlanck equation with drift forces arising as a gradient of a potential \\(V(x)\\), i.e., \\(f(x)=\\nabla V(x)\\) was identified as the gradient flow of the free energy with respect to the Wasserstein metric. For a gradient system, the free energy difference between two states \\(\\delta F\\) amounts to the negative entropy production times the temperature \\(\\delta F = - \\mathcal{T} \\mathcal{S}\\), where \\(\\mathcal{S}\\) stands for the entropy production. Thus this formulation may be viewed as a maximum entropy principle for the Fokker Planck.\nThe Fokker-Planck equation can be viewed as as the gradient flow in the Wasserstein metric of the relative entropy functional \\[\\begin{equation}\n    S(\\rho) = \\int_{\\mathcal{R}^d} \\rho(x) \\log\\left( \\frac{\\rho(x)}{e^{-V(x)}} \\right)dx.\n\\end{equation}\\]\nHowever, a major computational challenge of this known as JKO scheme is how to computationally efficiently compute the optimal transport cost.\nThe optimal transport yields geodesics in the Wasserstein space [cite Villani old and new]. The evolution of the probability density described by the Fokker‚ÄìPlanck equation amounts to the a curve in the Wasserstein space, the actual length of this curve is identified as the distance between the initial and the terminal points if this curve is a geodesic. All other curves that connect the two measures have larger dissipation. Optimal transport protocols correspond to geodesics in the Wasserstein space and can be employed in an equivalent definition of curvature."
  },
  {
    "objectID": "posts/NMA_mentoring.html",
    "href": "posts/NMA_mentoring.html",
    "title": "Successfully mentored six project groups at NMA summer schools",
    "section": "",
    "text": "In the last 2-3 weeks I had the luck to meet (online) and mentor several young researchers, aspiring computational neuroscientists and NeuroAI researchers. I had the honor to mentor two groups on their research projects (one working on the Motor RNN project (group Manifold Mechanics) and another one working on inter-area communications using the IBL dataset (group 404)) for the Neuromatch Computational Neuroscience summer course, and two pods, each with two groups, from the NeuroAI course (two groups working on biologically informed network architectures for robotic control, and two on bilogically plaussible learning algorithms). I learned so much from them over these weeks, and I hope they also gained a lot by working on their projects.\nI couldn‚Äôt be more lucky12 meeting such motivated, hardworking, and brilliant young minds. Hope our trajectories will cross again at some point in the future."
  },
  {
    "objectID": "posts/NMA_mentoring.html#footnotes",
    "href": "posts/NMA_mentoring.html#footnotes",
    "title": "Successfully mentored six project groups at NMA summer schools",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd I call it luck, because the last time I mentored (or at least tried to mentor) a group for NMA, things went downhill, and that was definitely not NMA‚Äôs fault. Within a week after two of the mentees started following my GitHub profile (I made them unfollow me later but still have screenshot proof is someone is interested), they started refusing to collaborate with the third member, and wanted to either ensure their first authorship in the upcoming micropublication, or abandon the project. Incidentally, this happened exactly at the time when I left my previous postdoc lab, at a time when some people (I don‚Äôt know who) were trying to track my whereabouts and block my path forward. So, this could either be a complete coincidence, that I happened to find myself in two toxic situations simultaneously, be rooted in my own behavior (or in someone trying to paint that image of me), or perhaps these two events were somehow connected. We‚Äôll never know. üòâ As another coincidence, the project I was mentoring happened to use the same dataset as my main postdoc project, though it was tackling a completely different question. So, this time, I was lucky enough to keep everything private until the projects were completed, and we faced absolutely no issues. ;)‚Ü©Ô∏é\nFunny enough, after a bit of digging to understand what was going on, since such a coincidence felt really strange to me, I found that one of those two mentees had, since August or September 2024, a post on their LinkedIn profile showcasing their NMA summer school presentation (on the same project). They were also announcing their participation in the Impact Scholar program and looking for collaborators. So, it was quite straightforward for anyone to connect the dots, especially since at that time I was applying for positions and was sharing my CV, where I was mentioning this mentoring activity as ongoing. ;)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/Three_factor_rules.html",
    "href": "posts/Three_factor_rules.html",
    "title": "Glad that I got e-mail confimration from former postdoc advisor to pursue my ideas independently (lol)",
    "section": "",
    "text": "I am glad when leaving from my previous lab that I least I received a written confirmation that declared that I can pursue the idea I proposed independently. This of course after my former advisor ensured that the competing labs who would be willing to work on this rejected me, and thinking I would be unable to work on this independtly. For the record, I was supposed to supervise a Master student working on this together with my former collaborator Matt Getz. However, on 30th of October I received an email that this suppervision could no longer happen.\nI just wonder what happened on 29th of October 2024 ;)\nI provide the email here to ensure that when I will be presenting my work (which does not use anything we were using in the similar project in my previous lab), no one will be able to distort the truth and accuse me of misatributing ideas or work or what not.\nFor the record this is the last stage of the work I was inovled in my previous lab on a similar topic presented at the Bernstein Conference 2024 Bernstein 2024 poster .1"
  },
  {
    "objectID": "posts/Three_factor_rules.html#footnotes",
    "href": "posts/Three_factor_rules.html#footnotes",
    "title": "Glad that I got e-mail confimration from former postdoc advisor to pursue my ideas independently (lol)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou might need to refresh several times to see the poster, but you can also use this link if you cannot see the poster there: wayback link‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/Satelite_StatPhys29.html",
    "href": "posts/Satelite_StatPhys29.html",
    "title": "Presented my recent work at a Satellite meeting of StatPhys29",
    "section": "",
    "text": "A few days ago I presented my recent work at a Satellite StatPhys29 meeting on Collective Dynamics and Information Processing in Neural Systems[ official website ] that took place in Venice, Italy.\n\nI will come back with highlights, because at least two very talened young researchers need to be mentioned!"
  },
  {
    "objectID": "posts/Low_rank_tensors.html",
    "href": "posts/Low_rank_tensors.html",
    "title": "Low tensor rank learning of neural dynamics",
    "section": "",
    "text": "Back in September 2023, after the Bernstein Conference, I came back from Alex Cayco-Gajic‚Äôs workshop talk quite inspired. The low tensor rank framework she presented (work together with Arthur Pellegrino and Angus Chadwick Pellegrino, Cayco Gajic, and Chadwick (2023)) had struck me as one of the more conceptually elegant approaches to track how neural population dynamics change over learning. I very enthusiastically shared my excitement in the internal post-Bernstein group meeting, as one does when ideas resonate1.\n\n\n\nphoto from my presentation containing photo of Alex‚Äôs presentation :)\n\n\nIt turned out that sharing my excitement about someone else‚Äôs work2 didn‚Äôt land equally well with everyone in the room, but that‚Äôs life. The dynamics that followed are perhaps best saved for a more informal conversation.\nA few months later, in December 2023, I noticed the call for the ICLR 2024 blogpost track, and thought this would be a great opportunity to spotlight this work. I began drafting a piece, but for a mix of personal and political reasons (and admittedly, some competing deadlines), I set it aside3.\nSince I am still excited about this work, stay tuned here, I will soon revive this post.\nIn the meantime, I recommend going straight to the source, it‚Äôs a great read."
  },
  {
    "objectID": "posts/Low_rank_tensors.html#footnotes",
    "href": "posts/Low_rank_tensors.html#footnotes",
    "title": "Low tensor rank learning of neural dynamics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI even reached out a few days later to congratulate her on the talk (a breach of my usually well-fortified social inertia) and to ask for a copy of a paper I couldn‚Äôt access.‚Ü©Ô∏é\nespecially that specific someone‚Ü©Ô∏é\n\n\n\nICLR blogpost track pull request\n\n\n‚Ü©Ô∏é"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html",
    "href": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html",
    "title": "Tutorial: Learning on Tangent Data",
    "section": "",
    "text": "In this notebook, we demonstrate how any standard machine learning algorithm can be used on data that live on a manifold yet respecting its geometry. In the previous notebooks we saw that linear operations (mean, linear weighting) don‚Äôt work on manifold. However, to each point on a manifold, is associated a tangent space, which is a vector space, where all our off-the-shelf ML operations are well defined!\nWe will use the logarithm map to go from points of the manifolds to vectors in the tangent space at a reference point. This will enable to use a simple logistic regression to classify our data."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#set-up",
    "href": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#set-up",
    "title": "Tutorial: Learning on Tangent Data",
    "section": "Set up",
    "text": "Set up\nBefore starting this tutorial, we set the working directory to be the root of the geomstats repository. In order to have the code working on your machine, you need to change this path to the path of your geomstats repository.\n\nimport os\nimport subprocess\n\ngeomstats_gitroot_path = subprocess.check_output(\n    ['git', 'rev-parse', '--show-toplevel'], \n    universal_newlines=True)\n\nos.chdir(geomstats_gitroot_path[:-1])\n\nprint('Working directory: ', os.getcwd())\n\nWorking directory:  /Users/nina/Google Drive/code/geomstats\n\n\nWe import the backend that will be used for geomstats computations and set a seed for reproducibility of the results.\n\nimport geomstats.backend as gs\n\ngs.random.seed(2020)\n\nINFO: Using numpy backend\n\n\nWe import the visualization tools.\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#the-data",
    "href": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#the-data",
    "title": "Tutorial: Learning on Tangent Data",
    "section": "The Data",
    "text": "The Data\nWe use data from the MSLP 2014 Schizophrenia Challenge. The dataset correponds to the Functional Connectivity Networks (FCN) extracted from resting-state fMRIs of 86 patients at 28 Regions Of Interest (ROIs). Roughly, an FCN corresponds to a correlation matrix and can be seen as a point on the manifold of Symmetric Positive-Definite (SPD) matrices. Patients are separated in two classes: schizophrenic and control. The goal will be to classify them.\nFirst we load the data (reshaped as matrices):\n\nimport geomstats.datasets.utils as data_utils\n\ndata, patient_ids, labels = data_utils.load_connectomes()\n\nWe plot the first two connectomes from the MSLP dataset with their corresponding labels.\n\nlabels_str = ['Healthy', 'Schizophrenic']\n\nfig = plt.figure(figsize=(8, 4))\n\nax = fig.add_subplot(121)\nimgplot = ax.imshow(data[0])\nax.set_title(labels_str[labels[0]])\n\nax = fig.add_subplot(122)\nimgplot = ax.imshow(data[1])\nax.set_title(labels_str[labels[1]])\n\nplt.show()\n\n\n\n\n\n\n\n\nIn order to compare with a standard Euclidean method, we also flatten the data:\n\nflat_data, _, _ = data_utils.load_connectomes(as_vectors=True)\nprint(flat_data.shape)\n\n(86, 378)"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#the-manifold",
    "href": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#the-manifold",
    "title": "Tutorial: Learning on Tangent Data",
    "section": "The Manifold",
    "text": "The Manifold\nAs mentionned above, correlation matrices are SPD matrices. Because multiple metrics could be used on SPD matrices, we also import two of the most commonly used ones: the Log-Euclidean metric and the Affine-Invariant metric [PFA2006]. We can use the SPD module from geomstats to handle all the geometry, and check that our data indeed belongs to the manifold of SPD matrices:\n\nimport geomstats.geometry.spd_matrices as spd\n\nmanifold = spd.SPDMatrices(28)\nai_metric = spd.SPDMetricAffine(28)\nle_metric = spd.SPDMetricLogEuclidean(28)\nprint(gs.all(manifold.belongs(data)))\n\nTrue"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#the-transformer",
    "href": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#the-transformer",
    "title": "Tutorial: Learning on Tangent Data",
    "section": "The Transformer",
    "text": "The Transformer\nGreat! Now, although the sum of two SPD matrices is an SPD matrix, their difference or their linear combination with non-positive weights are not necessarily! Therefore we need to work in a tangent space to perform simple machine learning. But worry not, all the geometry is handled by geomstats, thanks to the preprocessing module.\n\nfrom geomstats.learning.preprocessing import ToTangentSpace\n\nWhat ToTangentSpace does is simple: it computes the Frechet Mean of the data set (covered in the previous tutorial), then takes the log of each data point from the mean. This results in a set of tangent vectors, and in the case of the SPD manifold, these are simply symmetric matrices. It then squeezes them to a 1d-vector of size dim = 28 * (28 + 1) / 2, and thus outputs an array of shape [n_patients, dim], which can be fed to your favorite scikit-learn algorithm.\nBecause the mean of the input data is computed, ToTangentSpace should be used in a pipeline (as e.g.¬†scikit-learn‚Äôs StandardScaler) not to leak information from the test set at train time.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\n\npipeline = Pipeline(\n    steps=[('feature_ext', ToTangentSpace(geometry=ai_metric)),\n           ('classifier', LogisticRegression(C=2))])\n\nWe now have all the material to classify connectomes, and we evaluate the model with cross validation. With the affine-invariant metric we obtain:\n\nresult = cross_validate(pipeline, data, labels)\nprint(result['test_score'].mean())\n\n0.7098039215686274\n\n\nAnd with the log-Euclidean metric:\n\npipeline = Pipeline(\n    steps=[('feature_ext', ToTangentSpace(geometry=le_metric)),\n           ('classifier', LogisticRegression(C=2))])\n\nresult = cross_validate(pipeline, data, labels)\nprint(result['test_score'].mean())\n\n0.6862745098039216\n\n\nBut wait, why do the results depend on the metric used? You may remember from the previous notebooks that the Riemannian metric defines the notion of geodesics and distance on the manifold. Both notions are used to compute the Frechet Mean and the logarithms, so changing the metric changes the results, and some metrics may be more suitable than others for different applications.\nWe can finally compare to a standard Euclidean logistic regression on the flattened data:\n\nflat_result = cross_validate(LogisticRegression(), flat_data, labels)\nprint(flat_result['test_score'].mean())\n\n0.7333333333333334"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#conclusion",
    "href": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#conclusion",
    "title": "Tutorial: Learning on Tangent Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn this example using Riemannian geometry does not make a big difference compared to applying logistic regression in the ambiant Euclidean space, but there are published results that show how useful geometry can be with this type of data (e.g [NDV2014], [WAZ2918]). We saw how to use the representation of points on the manifold as tangent vectors at a reference point to fit any machine learning algorithm, and compared the effect of different metrics on the space of symmetric positive-definite matrices"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#references",
    "href": "wl-mechanics/src/geomstats/notebooks/03_simple_machine_learning_on_tangent_spaces.html#references",
    "title": "Tutorial: Learning on Tangent Data",
    "section": "References",
    "text": "References\n.. [PFA2006] Pennec, X., Fillard, P. & Ayache, N. A Riemannian Framework for Tensor Computing. Int J Comput Vision 66, 41‚Äì66 (2006). https://doi.org/10.1007/s11263-005-3222-z\n.. [NDV2014] Bernard Ng, Martin Dressler, Ga√´l Varoquaux, Jean-Baptiste Poline, Michael Greicius, et al.. Transport on Riemannian Manifold for Functional Connectivity-based Classification. MICCAI - 17th International Conference on Medical Image Computing and Computer Assisted Intervention, Polina Golland, Sep 2014, Boston, United States. hal-01058521\n.. [WAZ2918] Wong E., Anderson J.S., Zielinski B.A., Fletcher P.T. (2018) Riemannian Regression and Classification Models of Brain Networks Applied to Autism. In: Wu G., Rekik I., Schirmer M., Chung A., Munsell B. (eds) Connectomics in NeuroImaging. CNI 2018. Lecture Notes in Computer Science, vol 11083. Springer, Cham"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html",
    "href": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html",
    "title": "Tutorial: From vector spaces to manifolds",
    "section": "",
    "text": "Before starting this tutorial, we set the working directory to be the root of the geomstats repository. In order to have the code working on your machine, you need to change this path to the path of your geomstats repository.\n\nimport os\nimport subprocess\n\ngeomstats_gitroot_path = subprocess.check_output(\n    ['git', 'rev-parse', '--show-toplevel'], \n    universal_newlines=True)\n\nos.chdir(geomstats_gitroot_path[:-1])\n\nprint('Working directory: ', os.getcwd())\n\nWorking directory:  /home/nguigui/PycharmProjects/geomstats2\n\n\nWe import the backend that will be used for geomstats computations and set a seed for reproducibility of the results.\n\nimport geomstats.backend as gs\n\ngs.random.seed(2020)\n\nINFO: Using tensorflow backend\n\n\nFinally, we import the visualization module.\n\nimport matplotlib\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\nimport geomstats.visualization as visualization\n\nvisualization.tutorial_matplotlib()"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#set-up",
    "href": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#set-up",
    "title": "Tutorial: From vector spaces to manifolds",
    "section": "",
    "text": "Before starting this tutorial, we set the working directory to be the root of the geomstats repository. In order to have the code working on your machine, you need to change this path to the path of your geomstats repository.\n\nimport os\nimport subprocess\n\ngeomstats_gitroot_path = subprocess.check_output(\n    ['git', 'rev-parse', '--show-toplevel'], \n    universal_newlines=True)\n\nos.chdir(geomstats_gitroot_path[:-1])\n\nprint('Working directory: ', os.getcwd())\n\nWorking directory:  /home/nguigui/PycharmProjects/geomstats2\n\n\nWe import the backend that will be used for geomstats computations and set a seed for reproducibility of the results.\n\nimport geomstats.backend as gs\n\ngs.random.seed(2020)\n\nINFO: Using tensorflow backend\n\n\nFinally, we import the visualization module.\n\nimport matplotlib\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\nimport geomstats.visualization as visualization\n\nvisualization.tutorial_matplotlib()"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#from-vector-spaces-to-manifolds",
    "href": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#from-vector-spaces-to-manifolds",
    "title": "Tutorial: From vector spaces to manifolds",
    "section": "From vector spaces to manifolds",
    "text": "From vector spaces to manifolds\nIn the previous tutorial, we visualized data that naturally belong to manifolds, i.e.¬†generalizations of vector spaces that are allowed to have curvature.\nA simple example of such data is the coordinates of cities on the surface of the earth: they belong to a sphere, which is a manifold.\n\nimport geomstats.datasets.utils as data_utils\n\ndata, names = data_utils.load_cities()\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\nvisualization.plot(data[15:20], ax=ax, space='S2', label=names[15:20], s=80, alpha=0.5)\n\nax.set_title('Cities on the earth.');\n\nWARNING: findfont: Font family ['times'] not found. Falling back to DejaVu Sans.\nWARNING: findfont: Font family ['times'] not found. Falling back to DejaVu Sans.\nWARNING: findfont: Font family ['times'] not found. Falling back to DejaVu Sans.\nWARNING: findfont: Font family ['times'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n\n\n\n\nThe purpose of this tutorial is to show how we can perform elementary computations on such data."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#from-addition-to-exponential-map",
    "href": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#from-addition-to-exponential-map",
    "title": "Tutorial: From vector spaces to manifolds",
    "section": "From addition to exponential map",
    "text": "From addition to exponential map\nThe elementary operations on a vector space are: addition, substraction and multiplication by a scalar. We can add a vector to a point, substract two points to get a vector, or multiply a vector by a scalar value.\n\n%matplotlib inline\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111)\n\npoint_a = gs.array([0, 1])\npoint_b = gs.array([1, 2])\nvector = point_b - point_a\n\nax.scatter(point_a, point_b, label='Points')\nax.arrow(gs.to_numpy(point_a[0]), gs.to_numpy(point_a[1]), \n         dx=gs.to_numpy(vector[0]), dy=gs.to_numpy(vector[1]), \n         width=0.008, length_includes_head=True, color='black')\n\nax.legend();\n\n\n\n\n\n\n\n\nFor points on a manifold, like the sphere, the same operations are not permitted. Indeed, adding a vector to a point will not give a point that belongs to the manifold.\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\nparis = data[19]\nvector = gs.array([1, 0, 0.8])\n\nax = visualization.plot(paris, ax=ax, space='S2', s=200, alpha=0.8, label='Paris')\n\narrow = visualization.Arrow3D(paris, vector=vector)\narrow.draw(ax, color='black')\nax.legend();\n\n\n\n\n\n\n\n\nThe exponential map is the operation that generalizes the addition of a vector to a point, on manifolds.\nThe exponential map takes a point and a tangent vector as inputs, and outputs the point on the manifold that is reached by ‚Äúshooting‚Äù with the tangent vector. ‚ÄúShooting‚Äù means taking the path of shortest length. This path is called a ‚Äúgeodesic‚Äù.\n\nfrom geomstats.geometry.hypersphere import Hypersphere\n\nsphere = Hypersphere(dim=2)\n\nparis = data[19]\nvector = gs.array([1, 0, 0.8])\ntangent_vector = sphere.to_tangent(vector, base_point=paris)\n\nresult = sphere.metric.exp(tangent_vector, base_point=paris)\n\ngeodesic = sphere.metric.geodesic(\n        initial_point=paris,\n        initial_tangent_vec=tangent_vector)\n\npoints_on_geodesic = geodesic(gs.linspace(0., 1., 30))\n\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\n\nax = visualization.plot(paris, ax=ax, space='S2', s=100, alpha=0.8, label='Paris')\nax = visualization.plot(result, ax=ax, space='S2', s=100, alpha=0.8, label='End point')\nax = visualization.plot(\n    points_on_geodesic, ax=ax, space='S2', color='black', label='Geodesic')\n\narrow = visualization.Arrow3D(paris, vector=tangent_vector)\narrow.draw(ax, color='black')\nax.legend();"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#from-substraction-to-logarithm-map",
    "href": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#from-substraction-to-logarithm-map",
    "title": "Tutorial: From vector spaces to manifolds",
    "section": "From substraction to logarithm map",
    "text": "From substraction to logarithm map\nThe logarithm map is the operation that generalizes the substraction of two points, that gives a vector.\nThe logarithm map takes two points on the manifold as inputs, and outputs the tangent vector that is required to ‚Äúshoot‚Äù from one point to the other.\n\nparis = data[19]\nbeijing = data[15]\n\nlog = sphere.metric.log(point=beijing, base_point=paris)\n\ngeodesic = sphere.metric.geodesic(\n        initial_point=paris,\n        end_point=beijing)\n\npoints_on_geodesic = geodesic(gs.linspace(0., 1., 30))\n\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\nax = visualization.plot(paris, ax=ax, space='S2', s=100, alpha=0.8, label='Paris')\nax = visualization.plot(beijing, ax=ax, space='S2', s=100, alpha=0.8, label='Beijing')\nax = visualization.plot(\n    points_on_geodesic, ax=ax, space='S2', color='black', label='Geodesic')\n\narrow = visualization.Arrow3D(paris, vector=log)\narrow.draw(ax, color='black')\nax.legend();"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#geodesics",
    "href": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#geodesics",
    "title": "Tutorial: From vector spaces to manifolds",
    "section": "Geodesics",
    "text": "Geodesics\nSo far, we have given examples of geodesics on the sphere. The sphere is a simple manifold that is easy to visualize. Yet, geomstats provides many more manifolds, on which the exp and log are defined. Let‚Äôs present a few more."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#geodesics-on-the-hyperbolic-plane",
    "href": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#geodesics-on-the-hyperbolic-plane",
    "title": "Tutorial: From vector spaces to manifolds",
    "section": "Geodesics on the hyperbolic plane",
    "text": "Geodesics on the hyperbolic plane\nWe consider the hyperbolic geometry here. We define two points on the hyperbolic plane and compute the geodesic between them.\n\nfrom geomstats.geometry.hyperboloid import Hyperboloid\n\nhyperbolic = Hyperboloid(dim=2, coords_type='extrinsic')\n\ninitial_point = gs.array([gs.sqrt(2.), 1., 0.])\nend_point = gs.array([2.5, 2.5])\nend_point = hyperbolic.from_coordinates(end_point, 'intrinsic')\n\ngeodesic = hyperbolic.metric.geodesic(\n    initial_point=initial_point, end_point=end_point)\n\npoints = geodesic(gs.linspace(0., 1., 10))\n\nWe use the visualization module to plot the two points and the geodesic between them. We can choose the visualization we prefer for points on the hyperbolic plane. First we visualize with the Poincare disk representation.\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111)\n\nrepresentation = 'H2_poincare_disk'\n\nax = visualization.plot(\n    initial_point, ax=ax, space=representation, s=50, label='Initial point');\nax = visualization.plot(\n    end_point, ax=ax, space=representation, s=50, label='End point');\n\nax = visualization.plot(\n    points[1:-1], ax=ax, space=representation, s=5, color='black', label='Geodesic');\nax.set_title('Geodesic on the hyperbolic plane in Poincare disk representation')\nax.legend();\n\n\n\n\n\n\n\n\nWe can visualize the same geodesic in Klein disk representation.\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111)\n\nrepresentation = 'H2_klein_disk'\n\nax = visualization.plot(\n    initial_point, ax=ax, space=representation, s=50, label='Initial point');\nax = visualization.plot(\n    end_point, ax=ax, space=representation, s=50, label='End point');\n\nax = visualization.plot(\n    points[1:-1], ax=ax, space=representation, s=5, color='black', label='Geodesic');\nax.set_title('Geodesic on the hyperbolic plane in Klein disk representation')\nax.legend();"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#geodesics-on-the-special-euclidean-group-se3",
    "href": "wl-mechanics/src/geomstats/notebooks/02_from_vector_spaces_to_manifolds.html#geodesics-on-the-special-euclidean-group-se3",
    "title": "Tutorial: From vector spaces to manifolds",
    "section": "Geodesics on the special euclidean group SE(3)",
    "text": "Geodesics on the special euclidean group SE(3)\nWe consider the special euclidean group in 3D, which is the group of 3D rotations and 3D translations. One element of this group can be represented by a frame, oriented by the 3D rotation, and located by the 3D translation from the origin.\nWe create two points in SE(3), and compute the geodesic between them.\n\nfrom geomstats.geometry.special_euclidean import SpecialEuclidean\n\nse3 = SpecialEuclidean(n=3, point_type='vector')\nmetric = se3.left_canonical_metric\n\ninitial_point = se3.identity\ninitial_tangent_vec = gs.array([1.8, 0.2, 0.3, 3., 3., 1.])\ngeodesic = metric.geodesic(\n    initial_point=initial_point,\n    initial_tangent_vec=initial_tangent_vec)\n\npoints = geodesic(gs.linspace(-3., 3., 40))\n\nWe visualize the geodesic in the group SE(3), which is a path of frames in 3D.\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\n\nvisualization.plot(points, ax=ax, space='SE3_GROUP');"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html",
    "href": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html",
    "title": "Shape analysis of curves with the Square Root Velocity metric",
    "section": "",
    "text": "In this notebook, we demonstrate how to compute distances between curves in a way that does not depend on parametrization, i.e.¬†that only depends on the shapes of the curves. This is achieved using the Square Root Velocity metric (see SKJJ2011) on the space of parametrized curves, and by quotienting out the action of reparametrization through an optimal matching algorithm (see LAB2017). We will use the discrete_curves.py module. Translation and rotation can also be quotiented out using the align method of the pre-shape.py module, but we will not deal with these aspects here. See this usecase for details on the pre_shape.py module, or this other usecase for an application where both modules are used."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html#setup",
    "title": "Shape analysis of curves with the Square Root Velocity metric",
    "section": "Setup",
    "text": "Setup\n\nimport os\nimport subprocess\n\ngeomstats_gitroot_path = subprocess.check_output(\n    ['git', 'rev-parse', '--show-toplevel'], \n    universal_newlines=True)\n\nos.chdir(geomstats_gitroot_path[:-1])\n\nprint('Working directory: ', os.getcwd())\n\nWorking directory:  /Users/alicelebrigant/ownCloud/Python/SpyderProjects/geomstats\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport geomstats.backend as gs\nfrom geomstats.geometry.euclidean import Euclidean\nfrom geomstats.geometry.discrete_curves import DiscreteCurves\n\nINFO: Using numpy backend"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html#example-1-plane-curves",
    "href": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html#example-1-plane-curves",
    "title": "Shape analysis of curves with the Square Root Velocity metric",
    "section": "Example 1: plane curves",
    "text": "Example 1: plane curves\nWe start with a basic example in \\(\\mathbb R^2\\).\n\nr2 = Euclidean(dim=2)\ncurves_r2 = DiscreteCurves(ambient_manifold=r2)\n\nparametrized_curve_a = lambda x: gs.transpose(gs.array([1 + 2 * gs.sin(gs.pi * x), 3 + 2 * gs.cos(gs.pi * x)]))\nparametrized_curve_b = lambda x: gs.transpose(gs.array([5 * gs.ones(len(x)), 4 * (1 - x) + 1]))\n\nIn practice, we work with discrete curves, i.e.¬†sample points from the parametrized curves.\n\nn_sampling_points = 20\nsampling_points = gs.linspace(0., 1., n_sampling_points + 1)\ncurve_a = parametrized_curve_a(sampling_points)  \ncurve_b = parametrized_curve_b(sampling_points) \n\nplt.figure(figsize=(7, 7))\nplt.plot(curve_a[:, 0], curve_a[:, 1], 'o-b')\nplt.plot(curve_b[:, 0], curve_b[:, 1], 'o-r')\nplt.show()\n\n\n\n\n\n\n\n\n\nDistance between parametrized curves\nThe metric we use to compare parametrized curves is the so-called Square Root Velocity metric, that computes an \\(L^2\\) distance between the velocities of the curves, suitably renormalized to get reparametrization invariance. See SKJJ2011 for more details.\n\ncurves_r2.square_root_velocity_metric.dist(point_a=curve_a, point_b=curve_b)\n\narray([9.68542047])\n\n\nThe distance, as any riemannian distance, is computed as the length of the geodesic.\n\ngeod_fun = curves_r2.square_root_velocity_metric.geodesic(initial_curve=curve_a, end_curve=curve_b)\n\nn_times = 20\ntimes = gs.linspace(0., 1., n_times)\ngeod = geod_fun(times)\n\nplt.figure(figsize=(7, 7))\nplt.plot(geod[0, :, 0], geod[0, :, 1], 'o-b')\nfor i in range(1, n_times - 1):\n    plt.plot(geod[i, :, 0], geod[i, :, 1], 'o-k')\nplt.plot(geod[-1, :, 0], geod[-1, :, 1], 'o-r')\nplt.title('Geodesic for the Square Root Velocity metric')\nplt.show()\n\n\n\n\n\n\n\n\nThe Square Root Velocity metric is reparametrization invariant in the sense that, if the two curves are reparametrized in the same way, the distance does not change.\n\ncurve_a_resampled = parametrized_curve_a(sampling_points ** 2)\ncurve_b_resampled = parametrized_curve_b(sampling_points ** 2)\n\ncurves_r2.square_root_velocity_metric.dist(curve_a_resampled, curve_b_resampled)\n\narray([9.67537491])\n\n\nThe geodesic keeps the same shape.\n\ngeod_fun_1 = curves_r2.square_root_velocity_metric.geodesic(curve_a_resampled, curve_b_resampled)\ngeod_1 = geod_fun_1(times)\n\nplt.figure(figsize=(7, 7))\nplt.plot(geod_1[0, :, 0], geod_1[0, :, 1], 'o-b')\nfor i in range(1, n_times - 1):\n    plt.plot(geod_1[i, :, 0], geod_1[i, :, 1], 'o-k')\nplt.plot(geod_1[-1, :, 0], geod_1[-1, :, 1], 'o-r')\nplt.title('Geodesic when both curves are reparametrized in the same way')\nplt.show()\n\n\n\n\n\n\n\n\nHowever, if the curves are reparametrized in different ways, the distance changes, and so does the shape of the geodesic.\n\ncurves_r2.square_root_velocity_metric.dist(curve_a, curve_b_resampled)\n\narray([9.87255687])\n\n\n\ngeod_fun_2 = curves_r2.square_root_velocity_metric.geodesic(curve_a, curve_b_resampled)\ngeod_2 = geod_fun_2(times)\n\nplt.figure(figsize=(7, 7))\nplt.plot(geod_2[0, :, 0], geod_2[0, :, 1], 'o-b')\nfor i in range(1, n_times - 1):\n    plt.plot(geod_2[i, :, 0], geod_2[i, :, 1], 'o-k')\nplt.plot(geod_2[-1, :, 0], geod_2[-1, :, 1], 'o-r')\nplt.title('Geodesic when only the red curve is reparametrized')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDistance between unparametrized curves\nIn order to completely quotient out parametrization, distances are computed in the base space of a fiber bundle where the fibers represent equivalent classes of curves with the same shape (i.e.¬†equal modulo reparametrization). Any infinitesimal deformation of a curve can be split into the sum of vertical deformation (tangent to the fiber) that simply reparametrizes the curve without changing its shape, and a horizontal deformation (orthogonal to the fiber) that changes the shape. The distance between two unparametrized curves is then computed as the length of a horizontal geodesic linking their two fibers.\nIn practice, to compute the horizontal geodesic linking the fibers of two discrete parametrized curves curve_a and curve_b, we can fix the parametrization of curve_a, and search for a reparametrization of curve_b (i.e.¬†another discrete curve with same shape as curve_b) that best ‚Äúmatches‚Äù curve_a.\nSince geodesics that start with a horizontal velocity stay horizontal, a first idea would be the following:\n\ncompute the geodesic between curve_a and curve_b\ncompute the horizontal part of its initial velocity vector\nshoot from curve_a using this horizontal vector.\n\n\ngeod_velocity = n_times * (geod[1:] - geod[:-1])\ngeod_velocity_hor, geod_velocity_ver, _ = (\n    curves_r2.quotient_square_root_velocity_metric.split_horizontal_vertical(geod_velocity, geod[:-1])\n)\n\nshooted_geod_fun = curves_r2.square_root_velocity_metric.geodesic(\n    initial_tangent_vec=geod_velocity_hor[0], \n    initial_curve=curve_a\n)\nshooted_geod = shooted_geod_fun(times)\n\nThe problem with this idea is that, while it yields a horizontal geodesic starting at curve_a, its end point does not belong to the fiber of curve_b: as we cas see below, the end curve of the horizontal geodesic is not a reparametrization of the initial red curve, it does not have the same shape.\n\nplt.figure(figsize=(7, 7))\nplt.plot(shooted_geod[0, :, 0], shooted_geod[0, :, 1], 'o-b')\nplt.plot(shooted_geod[-1, :, 0], shooted_geod[-1, :, 1], 'o-r')\nfor i in range(1, n_times - 1):\n    plt.plot(shooted_geod[i, :, 0], shooted_geod[i, :, 1], 'o-', c='k')\nplt.title('Shooting with horizontal part of initial geodesic velocity vector')\nplt.show()\n\n\n\n\n\n\n\n\nTo obtain a horizontal geodesic starting at curve_a and ending at a curve with same shape as curve_b, we use an iterative optimal matching algorithm LAB2017. This algorithm moves along the fiber of curve_b to find the best representative with respect to curve_a by iterating the following steps:\n\nstep 1: compute the geodesic between curve_a and the current representative of curve_b (initially, curve_b)\nstep 2: compute the path whose velocity is a reparametrization of the horizontal part of the geodesic velocity at all time, and set the new representative of curve_b to be the end point of this path.\n\nNote that the first step yields a geodesic that is not horizontal, while the second step yields a horizontal path that is not geodesic. By iterating these two steps, the algorithm converges to a horizontal geodesic.\n\nhgeod_fun = curves_r2.quotient_square_root_velocity_metric.horizontal_geodesic(curve_a, curve_b)\nhgeod = hgeod_fun(times)\n\nplt.figure(figsize=(7, 7))\nplt.plot(hgeod[0, :, 0], hgeod[0, :, 1], 'o-b')\nfor i in range(1, n_times - 1):\n    plt.plot(hgeod[i, :, 0], hgeod[i, :, 1], 'o-k')\nplt.plot(hgeod[-1, :, 0], hgeod[-1, :, 1], 'o-r')\nplt.title('Horizontal geodesic')\nplt.show()\n\n\n\n\n\n\n\n\nWe can check the horizontality of this geodesic by computing the norm of the vertical part of its velocity for all times.\n\ngeod_vertical_norm = curves_r2.square_root_velocity_metric.norm(geod_velocity_ver, geod[:-1])\n\nhgeod_velocity = n_times * (hgeod[1:] - hgeod[:-1])\nhgeod_velocity_hor, hgeod_velocity_ver, _ = (\n    curves_r2.quotient_square_root_velocity_metric.split_horizontal_vertical(hgeod_velocity, hgeod[:-1])\n)\nhgeod_vertical_norm = curves_r2.square_root_velocity_metric.norm(hgeod_velocity_ver, hgeod[:-1])\n\nplt.figure()\nplt.plot(times[:-1], geod_vertical_norm, 'o', label='initial geodesic')\nplt.plot(times[:-1], hgeod_vertical_norm, 'o', label='horizontal geodesic')\nplt.legend()\nplt.title('Norm of the vertical part of the geodesic velocity')\nplt.show()\n\n\n\n\n\n\n\n\nWe can also check that this horizontal geodesic does not change if we resample the end curve.\n\nhgeod_fun = curves_r2.quotient_square_root_velocity_metric.horizontal_geodesic(curve_a, curve_b_resampled)\nhgeod = hgeod_fun(times)\n\nplt.figure(figsize=(7, 7))\nplt.plot(hgeod[0, :, 0], hgeod[0, :, 1], 'o-b')\nfor i in range(1, n_times - 1):\n    plt.plot(hgeod[i, :, 0], hgeod[i, :, 1], 'o-k')\nplt.plot(hgeod[-1, :, 0], hgeod[-1, :, 1], 'o-r')\nplt.title('Horizontal geodesic when the red curve is reparametrized')\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we can check that the quotient distance remains approximately constant for any parametrizations of the curves.\n\nprint(curves_r2.quotient_square_root_velocity_metric.dist(curve_a, curve_b))\nprint(curves_r2.quotient_square_root_velocity_metric.dist(curve_a_resampled, curve_b))\nprint(curves_r2.quotient_square_root_velocity_metric.dist(curve_a_resampled, curve_b_resampled))\nprint(curves_r2.quotient_square_root_velocity_metric.dist(curve_a, curve_b_resampled))\n\n1.718817126370995\n1.71949460797375\n1.718841000681418\n1.717925122344401"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html#example-2-3d-curves",
    "href": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html#example-2-3d-curves",
    "title": "Shape analysis of curves with the Square Root Velocity metric",
    "section": "Example 2: 3D curves",
    "text": "Example 2: 3D curves\nBelow we follow similar steps for curves in \\(\\mathbb R^3\\). In this example, we can see that the horizontal geodesic ‚Äústraightens out‚Äù the original geodesic.\n\nr3 = Euclidean(dim=3)\ncurves_r3 = DiscreteCurves(ambient_manifold=r3)\n\nparametrized_curve_a = lambda x: gs.transpose(gs.stack((gs.cos(2 + 8 * x), gs.sin(2 + 8 * x), 2 + 10 * x)))\nparametrized_curve_b = lambda x: gs.transpose(gs.stack((gs.cos(4 + 8 * x), gs.sin(4 + 8 * x), 2 + 10 * x)))\n\nn_sampling_points = 100\nsampling_points = gs.linspace(0., 1., n_sampling_points)\ncurve_a = parametrized_curve_a(sampling_points)\ncurve_b = parametrized_curve_b(sampling_points)\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.plot(curve_a[:, 0], curve_a[:, 1], curve_a[:, 2], 'b')\nax.plot(curve_b[:, 0], curve_b[:, 1], curve_b[:, 2], 'r')\nax.scatter(curve_a[0, 0], curve_a[0, 1], curve_a[0, 2], 'b')\nax.scatter(curve_b[0, 0], curve_b[0, 1], curve_b[0, 2], 'r')\nplt.show()\n\n\n\n\n\n\n\n\n\ngeod_fun = curves_r3.square_root_velocity_metric.geodesic(initial_curve=curve_a, end_curve=curve_b)\n\nn_times = 20\nt = gs.linspace(0., 1., n_times)\ngeod = geod_fun(t)\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.plot(curve_a[:, 0], curve_a[:, 1], curve_a[:, 2], '-', c='b', linewidth=2)\nax.plot(curve_b[:, 0], curve_b[:, 1], curve_b[:, 2], '-', c='r', linewidth=2)\nfor i in range(1, n_times - 1):\n    ax.plot(geod[i, :, 0], geod[i, :, 1], geod[i, :, 2], '-', c='k')\nfor j in range(n_sampling_points):\n    ax.plot(geod[:, j, 0], geod[:, j, 1], geod[:, j, 2], '--', c='k')\nplt.title('SRV geodesic')\nplt.show()\n\n\n\n\n\n\n\n\n\nhgeod_fun = curves_r3.quotient_square_root_velocity_metric.horizontal_geodesic(curve_a, curve_b)\nhgeod = hgeod_fun(t)\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.plot(curve_a[:, 0], curve_a[:, 1], curve_a[:, 2], '-', c='b', linewidth=2)\nax.plot(curve_b[:, 0], curve_b[:, 1], curve_b[:, 2], '-', c='r', linewidth=2)\nfor i in range(1, n_times - 1):\n    ax.plot(hgeod[i, :, 0], hgeod[i, :, 1], hgeod[i, :, 2], '-', c='k')\nfor j in range(n_sampling_points):\n    ax.plot(hgeod[:, j, 0], hgeod[:, j, 1], hgeod[:, j, 2], '--', c='k')\nplt.title('Horizontal SRV geodesic')\nplt.show()\n\n\n\n\n\n\n\n\n\ngeod_velocity = n_times * (geod[1:] - geod[:-1])\ngeod_velocity_hor, geod_velocity_ver, _ = (\n    curves_r2.quotient_square_root_velocity_metric.split_horizontal_vertical(geod_velocity, geod[:-1])\n)\ngeod_vertical_norm = curves_r2.square_root_velocity_metric.norm(geod_velocity_ver, geod[:-1])\n\nhgeod_velocity = n_times * (hgeod[1:] - hgeod[:-1])\nhgeod_velocity_hor, hgeod_velocity_ver, _ = (\n    curves_r2.quotient_square_root_velocity_metric.split_horizontal_vertical(hgeod_velocity, hgeod[:-1])\n)\nhgeod_vertical_norm = curves_r2.square_root_velocity_metric.norm(hgeod_velocity_ver, hgeod[:-1])\n\nplt.figure()\nplt.plot(times[:-1], geod_vertical_norm, 'o', label='initial geodesic')\nplt.plot(times[:-1], hgeod_vertical_norm, 'o', label='horizontal geodesic')\nplt.legend()\nplt.title('Norm of the vertical part of the geodesic velocity')\nplt.show()"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html#references",
    "href": "wl-mechanics/src/geomstats/notebooks/08_shape_analysis.html#references",
    "title": "Shape analysis of curves with the Square Root Velocity metric",
    "section": "References",
    "text": "References\n.. [SKJJ2011] A. Srivastava, E. Klassen, S. H. Joshi and I. H. Jermyn, ‚ÄúShape Analysis of Elastic Curves in Euclidean Spaces,‚Äù in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.¬†33, no. 7, pp.¬†1415-1428, July 2011.\n.. [LAB2017] A. Le Brigant, M. Arnaudon and F. Barbaresco, ‚ÄúOptimal matching between curves in a manifold,‚Äù in International Conference on Geometric Science of Information, pp.¬†57-65, Springer, Cham, 2017."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html",
    "href": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html",
    "title": "Tutorial: Information geometry",
    "section": "",
    "text": "Disclaimer: this notebook requires the use of the numpy backend."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#introduction",
    "href": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#introduction",
    "title": "Tutorial: Information geometry",
    "section": "Introduction",
    "text": "Introduction\nInformation geometry is a branch of mathematics at the crossroads of statistics and differential geometry, focused on the study of probability distributions from a geometric point of view. One of the tools of information geometry is the Fisher information distance, which allows to compare probability distributions inside a given parametric family. In that sense, information geometry is an alternative approach to optimal transport.\nThe Fisher information metric or Fisher-Rao metric - although the latter usually denotes its non parametric counterpart - is a Riemannian metric defined on the space of parameters of a family of distributions using the Fisher information matrix. This metric is invariant under change of parameterization. Moreover it is the only Riemannian metric compatible with the notion of information contained by the model on the parameter, in the sense that it is the only metric that preserves the geometry of a parametric model after transformation by a sufficient statistic (Cencov‚Äôs theorem). For an overview, see [A2016]."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#setup",
    "title": "Tutorial: Information geometry",
    "section": "Setup",
    "text": "Setup\nBefore starting this tutorial, we set the working directory to be the root of the geomstats repository. In order to have the code working on your machine, you need to change this path to the path of your geomstats repository.\n\nimport os\nimport subprocess\n\ngeomstats_gitroot_path = subprocess.check_output(\n    ['git', 'rev-parse', '--show-toplevel'], \n    universal_newlines=True)\n\nos.chdir(geomstats_gitroot_path[:-1])\n\nprint('Working directory: ', os.getcwd())\n\nWorking directory:  /Users/alicelebrigant/ownCloud/Python/SpyderProjects/geomstats\n\n\n\nimport matplotlib\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\nimport geomstats.backend as gs\nimport geomstats.visualization as visualization\n\nINFO: Using numpy backend"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#normal-distributions",
    "href": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#normal-distributions",
    "title": "Tutorial: Information geometry",
    "section": "Normal distributions",
    "text": "Normal distributions\nThe Fisher information geometry of the family of normal distributions is arguably the most well-known. The space of parameters is the upper half-plane where the x-coordinate encodes the mean and the y-coordinate the standard deviation. Quite remarkably, the Fisher information metric induces the hyperbolic geometry of the Poincare half plane [AM1981]. To start, we need an instance of the class NormalDistributions and its Fisher information metric.\n\nfrom geomstats.information_geometry.normal import NormalDistributions\n\nnormal = NormalDistributions()\nfisher_metric = normal.metric\n\nUsing the visualization module, we can plot the geodesic between two points, each defining the parameters (mean and standard deviation) for a normal distribution. We recognise the shape of a geodesic of the Poincare half-plane, namely a half-circle orthogonal to the x-axis.\n\npoint_a = gs.array([1., 1.])\npoint_b = gs.array([3., 1.])\n\ngeodesic_ab_fisher = fisher_metric.geodesic(point_a, point_b)\n\nn_points = 20\nt = gs.linspace(0, 1, n_points)\n\n\nfig = plt.figure(figsize=(10, 5))\nax = fig.add_subplot(111)\ncc = gs.zeros((n_points, 3))\ncc[:, 2] = gs.linspace(0, 1, n_points)\n\nvisualization.plot(\n    geodesic_ab_fisher(t), ax=ax, space='H2_poincare_half_plane', label='point on geodesic', color=cc)\n\nax.set_xlim(0., 4.)\nax.set_ylim(0., 2.)\nax.set_title('Geodesic between two normal distributions for the Fisher-Rao metric')\nax.legend();\n\n\n\n\n\n\n\n\nEach point of the geodesic defines a normal distribution, and so we obtain an optimal interpolation between the distributions corresponding to point_a and point_b, which we can visualize in terms of probability density functions.\n\npdfs = normal.point_to_pdf(geodesic_ab_fisher(t))\nx = gs.linspace(-3., 7., 100)\n\nfig = plt.figure(figsize=(10, 5))\nfor i in range(n_points):\n    plt.plot(x, pdfs(x)[:, i], color=cc[i, :])\nplt.title('Corresponding interpolation between pdfs');\n\n\n\n\n\n\n\n\nAnother possibility to compare probability distributions is given by the \\(L^2\\)-Wasserstein metric, central in optimal transport. In the case of normal distributions, the \\(L^2\\)-Wasserstein metric induces the Euclidean geometry on the upper half plane [BGKL2017]. Therefore, the Wasserstein distance between two normal distributions with different means and same variance (point_a and point_b) will not change when this common variance is increased (point_c and point_d), while the corresponding Fisher information distance will decrease, as can be deduced from the shape of the geodesic. This can be interpreted as a consequence of the increasing overlap of the corresponding probability densities.\n\nfrom geomstats.geometry.euclidean import Euclidean\n\nplane = Euclidean(2)\nwasserstein_metric = plane.metric\n\npoint_c = gs.array([1., 3.])\npoint_d = gs.array([3., 3.])\n\ngeodesic_cd_fisher = fisher_metric.geodesic(point_c, point_d)\ngeodesic_ab_wasserstein = wasserstein_metric.geodesic(point_a, point_b)\ngeodesic_cd_wasserstein = wasserstein_metric.geodesic(point_c, point_d)\n\npoints = gs.stack((point_a, point_b, point_c, point_d))\npdfs = normal.point_to_pdf(points)\n\n\n%matplotlib inline\n\nfig = plt.figure(figsize=(12, 5))\nax1 = fig.add_subplot(121)\n\nvisualization.plot(\n    gs.vstack((geodesic_ab_fisher(t), geodesic_cd_fisher(t))), \n    ax=ax1, space='H2_poincare_half_plane', label='Fisher information geodesic',\n    color='black')\nvisualization.plot(\n    gs.vstack((geodesic_ab_wasserstein(t), geodesic_cd_wasserstein(t))),\n    ax=ax1, space='H2_poincare_half_plane', label='Wasserstein geodesic',\n    color='black', alpha=0.5)\nvisualization.plot(\n    gs.stack((point_a, point_b)), ax=ax1, space='H2_poincare_half_plane', \n    label='points a and b', s=100)\nvisualization.plot(\n    gs.stack((point_c, point_d)), ax=ax1, space='H2_poincare_half_plane', \n    label='points c and d', s=100)\n\nax1.set_xlim(0., 4.)\nax1.set_ylim(0., 4.)\nax1.legend();\n\nax2 = fig.add_subplot(122)\nx = gs.linspace(-3., 7., 100)\nlines = [Line2D([0], [0], color='C0'),\n         Line2D([0], [0], color='C1')]\nax2.plot(x, pdfs(x)[:, :2], c='C0')\nax2.plot(x, pdfs(x)[:, 2:], c='C1')\nax2.legend(lines, ['pdfs a and b', 'pdfs c and d']);"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#beta-distributions",
    "href": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#beta-distributions",
    "title": "Tutorial: Information geometry",
    "section": "Beta distributions",
    "text": "Beta distributions\nLet us now consider the example of beta distributions, where the space of parameters is the first quadrant. In this case, the geodesics for the Fisher-Rao metric do not have a closed form, but can be found numerically [LGRP2020]. Here we plot an example of geodesic ball.\n\nfrom geomstats.information_geometry.beta import BetaDistributions\n\nbeta = BetaDistributions()\n\n\nn_rays = 50\ncenter = gs.array([2., 2.])\ntheta = gs.linspace(-gs.pi, gs.pi, n_rays)\ndirections = gs.transpose(\n    gs.stack((gs.cos(theta), gs.sin(theta))))\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111)\nray_length = 0.25\ndirection_norms = beta.metric.squared_norm(directions, center)**(1/2)\nunit_vectors = directions/gs.expand_dims(direction_norms, 1)\ninitial_vectors = ray_length * unit_vectors\n\nn_points = 10\nt = gs.linspace(0., 1., n_points)\nfor j in range(n_rays):\n    geod = beta.metric.geodesic(\n        initial_point=center, initial_tangent_vec=initial_vectors[j, :])\n    ax.plot(*gs.transpose(gs.array([geod(k) for k in t])))\nax.set_xlim(1, 3)\nax.set_ylim(1, 3)\nax.set_title('Geodesic ball of the space of beta distributions');\n\n\n\n\n\n\n\n\nNow we consider an application to the study of the leaf inclination angle distribution of plants. The leaf angle distribution among a common plant species can be appropriately represented by a beta distribution (CPR2018). The dataset leaves (CPR2018) contains pairs of beta distribution parameters, each describing the distribution of the inclination angles of leaves inside a given plant species. These species are divided into 5 categories according to inclination angle distribution type: spherical, erectophile, uniform, planophile and plagiophile.\n\nimport geomstats.datasets.utils as data_utils\n\nbeta_param, distrib_type = data_utils.load_leaves()\n\n\nfig = plt.figure(figsize=(5, 5))\nfor distrib in set(distrib_type):\n    points = beta_param[distrib_type==distrib, :]\n    plt.plot(points[:, 0], points[:, 1], 'o', label=distrib)\nplt.title('Beta parameters of the leaf inclination angle distributions of 172 different species')\nplt.legend();\n\n\n\n\n\n\n\n\nUsing the FrechetMean learning class, we can compute the leaf inclination angle mean distribution among the species of type ‚Äòplanophile‚Äô.\n\nfrom geomstats.learning.frechet_mean import FrechetMean\n\npoints_plan = beta_param[distrib_type=='planophile', :]\n\nmean = FrechetMean(metric=beta.metric)\nmean.fit(points_plan)\n\nmean_estimate = mean.estimate_\n\n\nfig = plt.figure(figsize=(5, 5))\nplt.plot(points_plan[:, 0], points_plan[:, 1], 'o', label='planophile')\nplt.plot(*mean_estimate, 'o', markersize=10, label='mean planophile')\nplt.title('Beta parameters of the leaf inclination angle mean distribution '\n          'of species of planophile type')\nplt.legend();"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#references",
    "href": "wl-mechanics/src/geomstats/notebooks/06_information_geometry.html#references",
    "title": "Tutorial: Information geometry",
    "section": "References",
    "text": "References\n.. [A2016] S. Amari. Information geometry and its applications. Vol. 194. Springer, 2016.\n.. [AM1981] C. Atkinson and A. FS Mitchell. Rao‚Äôs distance measure. Sankhya: The Indian Journal of Statistics. Series A, pp.¬†345‚Äì365, 1981.\n.. [BGKL2017] J. Bigot, R. Gouet, T. Klein and A. L√≥pez. Geodesic PCA in the Wasserstein space by convex PCA. In Annales de l‚ÄôInstitut Henri Poincar√©, Probabilit√©s et Statistiques. Vol. 53. No.¬†1. Institut Henri Poincar√©, 2017.\n.. [CPR2018] F. Chianucci, J. Pisek, K. Raabe et al.¬†A dataset of leaf inclination angles for temperate and boreal broadleaf woody species. Annals of Forest Science Vol. 75, No.¬†50, 2018. https://doi.org/10.17632/4rmc7r8zvy.2.\n.. [LGRP2020] A. Le Brigant, N. Guigui, S. Rebbah and S. Puechmorel, Classifying histograms of medical data using information geometry of beta distributions. IFAC-PapersOnLine, Vol. 54, No.¬†9, 514-520, 2021."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "",
    "text": "From social networks to parse trees, knowledge graphs to protein interaction networks, Graph-Structured Data is endemic to a wide variety of natural and engineered systems. Often, understanding the structure and/or dynamics of these graphs yields insight into the systems under investigation. Take, for example, the problems of finding key influencers or distinct communities within social networks.\nThe goal of graph embedding is to find a way of representing the graph in a space which more readily lends itself to analysis/investigation. One approach is to identify points in a vector space with nodes of the graph in such a way that important relations between nodes are preserved via relations between their corresponding points.\nThere are a wide variety of methods which approach this problem in different ways and for different aims, say for clustering or for link prediction. Recently, the embedding of Graph Structured Data (GSD) on manifolds has received considerable attention. In particular, much work has shown that hyperbolic spaces are beneficial for a wide variety of tasks with GSD [ND2017]. This tutorial shows how to learn such embeddings using the Poincar√© Ball manifold and the well-known ‚ÄòKarate Club‚Äô social network dataset with geomstats. This data and several others can be found in the datasets.data module of the project‚Äôs github repository.\n Learning a Poincar√© disk embedding of the Karate club graph dataset"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#introduction",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#introduction",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "",
    "text": "From social networks to parse trees, knowledge graphs to protein interaction networks, Graph-Structured Data is endemic to a wide variety of natural and engineered systems. Often, understanding the structure and/or dynamics of these graphs yields insight into the systems under investigation. Take, for example, the problems of finding key influencers or distinct communities within social networks.\nThe goal of graph embedding is to find a way of representing the graph in a space which more readily lends itself to analysis/investigation. One approach is to identify points in a vector space with nodes of the graph in such a way that important relations between nodes are preserved via relations between their corresponding points.\nThere are a wide variety of methods which approach this problem in different ways and for different aims, say for clustering or for link prediction. Recently, the embedding of Graph Structured Data (GSD) on manifolds has received considerable attention. In particular, much work has shown that hyperbolic spaces are beneficial for a wide variety of tasks with GSD [ND2017]. This tutorial shows how to learn such embeddings using the Poincar√© Ball manifold and the well-known ‚ÄòKarate Club‚Äô social network dataset with geomstats. This data and several others can be found in the datasets.data module of the project‚Äôs github repository.\n Learning a Poincar√© disk embedding of the Karate club graph dataset"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#setup",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "Setup",
    "text": "Setup\nWe start by importing standard tools for logging and visualization, allowing us to draw the embedding of the GSD on the manifold. Next, we import the manifold of interest, visualization tools, and other methods from geomstats.\n\nimport os\nimport sys\nimport warnings\n\nsys.path.append(os.path.dirname(os.getcwd()))\nwarnings.filterwarnings('ignore')\n\n\nimport logging\nimport matplotlib.pyplot as plt\n\nimport geomstats.backend as gs\nimport geomstats.visualization as visualization\n\nfrom geomstats.datasets.utils import load_karate_graph\nfrom geomstats.geometry.poincare_ball import PoincareBall\n\nINFO: Using numpy backend"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#parameters-and-initialization",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#parameters-and-initialization",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "Parameters and Initialization",
    "text": "Parameters and Initialization\nWe define the following parameters needed for embedding:\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nrandom.seed\nAn initial manually set number for generating pseudorandom numbers\n\n\ndim\nDimensions of the manifold used for embedding\n\n\nmax_epochs\nNumber of iterations for learning the embedding\n\n\nlr\nLearning rate\n\n\nn_negative\nNumber of negative samples\n\n\ncontext_size\nSize of the considered context for each node of the graph\n\n\n\nLet us discuss a few things about the parameters of the above table. The number of dimensions should be high (i.e., 10+) for large datasets (i.e., where the number of nodes/edges is significantly large). In this tutorial we consider a dataset that is quite small with only 34 nodes. The Poincar√© disk of only two dimensions is therefore sufficient to capture the complexity of the graph and provide a faithful representation. Some parameters are hard to know in advance, such as max_epochs and lr. These should be tuned specifically for each dataset. Visualization can help with tuning the parameters. Also, one can perform a grid search to find values of these parameters which maximize some performance function. In learning embeddings, one can consider performance metrics such as a measure for cluster seperability or normalized mutual information (NMI) or others. Similarly, the number of negative samples and context size can also be thought of as hyperparameters and will be further discussed in the sequel. An instance of the Graph class is created and set to the Karate club dataset.\n\ngs.random.seed(1234)\ndim = 2\nmax_epochs = 100\nlr = .05\nn_negative = 2\ncontext_size = 1\nkarate_graph = load_karate_graph()\n\nThe Zachary karate club network was collected from the members of a university karate club by Wayne Zachary in 1977. Each node represents a member of the club, and each edge represents an undirected relation between two members. An often discussed problem using this dataset is to find the two groups of people into which the karate club split after an argument between two teachers.  Some information about the dataset is displayed to provide insight into its complexity.\n\nnb_vertices_by_edges =\\\n    [len(e_2) for _, e_2 in karate_graph.edges.items()]\nlogging.info('Number of vertices: %s', len(karate_graph.edges))\nlogging.info(\n    'Mean edge-vertex ratio: %s',\n    (sum(nb_vertices_by_edges, 0) / len(karate_graph.edges)))\n\nINFO: Number of vertices: 34\nINFO: Mean edge-vertex ratio: 4.588235294117647\n\n\nDenote \\(V\\) as the set of nodes and \\(E \\subset V\\times V\\) the set of edges. The goal of embedding GSD is to provide a faithful and exploitable representation of the graph structure. It is mainly achieved by preserving first-order proximity that enforces nodes sharing edges to be close to each other. It can additionally preserve second-order proximity that enforces two nodes sharing the same context (i.e., nodes that share neighbors but are not necessarily directly connected) to be close. Let \\(\\mathbb{B}^m\\) be the Poincar√© Ball of dimension \\(m\\) equipped with the distance function \\(d\\). The below figure shows geodesics between pairs of points on \\(\\mathbb{B}^2\\). Geodesics are the shortest path between two points. The distance function \\(d\\) of two points is the length of the geodesic that links them.\n\nDeclaring an instance of the PoincareBall manifold of two dimensions in geomstats is straightforward:\n\nhyperbolic_manifold = PoincareBall(2)\n\nfirst and second-order proximities can be achieved by optimising the following loss functions:"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#loss-function.",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#loss-function.",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "Loss function.",
    "text": "Loss function.\nTo preserve first and second-order proximities we adopt a loss function similar to (Nickel, 2017) and consider the negative sampling approach as in (Mikolov, 2013) :\n\\[     \\mathcal{L} = - \\sum_{v_i\\in V} \\sum_{v_j \\in C_i} \\bigg[ log(\\sigma(-d^2(\\phi_i, \\phi_j'))) + \\sum_{v_k\\sim \\mathcal{P}_n} log(\\sigma(d^2(\\phi_i, \\phi_k')))  \\bigg]\\]\nwhere \\(\\sigma(x)=\\frac{1}{1+e^{-x}}\\) is the sigmoid function and \\(\\phi_i \\in \\mathbb{B}^m\\) is the embedding of the \\(i\\)-th node of \\(V\\), \\(C_i\\) the nodes in the context of the \\(i\\)-th node, \\(\\phi_j'\\in \\mathbb{B}^m\\) the embedding of \\(v_j\\in C_i\\) and \\(\\mathcal{P}_n\\) the negative sampling distribution over \\(V\\): \\(\\mathcal{P}_n(v)=\\frac{deg(v)^{3/4}}{\\sum_{v_i\\in V}deg(v_i)^{3/4}}\\). Intuitively one can see that to minimizing \\(L\\), the distance between \\(v_i\\) and \\(v_j\\) should get smaller, while the one between \\(v_i\\) and \\(v_k\\) would get larger."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#riemannian-optimization.",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#riemannian-optimization.",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "Riemannian optimization.",
    "text": "Riemannian optimization.\nFollowing the idea of (Ganea, 2018) we use the following formula to optimize \\(L\\):\n\\[ \\phi^{t+1} = \\text{Exp}_{\\phi^t} \\left( -lr \\frac{\\partial L}{\\partial \\phi} \\right) \\]\nwhere \\(\\phi\\) is a parameter of \\(L\\), \\(t\\in\\{1,2,\\cdots\\}\\) is the epoch iteration number and \\(lr\\) is the learning rate. The formula consists of first computing the usual gradient of the loss function giving the direction in which the parameter should move. The Riemannian exponential map \\(\\text{Exp}\\) is a function that takes a base point \\(\\phi^t\\) and some direction vector \\(T\\) and returns the point \\(\\phi^{t+1}\\) such that \\(\\phi^{t+1}\\) belongs to the geodesic initiated from \\(\\phi{t}\\) in the direction of \\(T\\) and the length of the geoedesic curve between \\(\\phi^t\\) and \\(\\phi^{t+1}\\) is of 1 unit. The Riemannian exponential map is implemented as a method of the PoincareBallMetric class in the geometry module of geomstats.\nTherefore to minimize \\(L\\) we will need to compute its gradient. Several steps are required to do so, 1. Compute the gradient of the squared distance 2. Compute the gradient of the log sigmoid 3. Compute the gradient of the composision of 1. and 2.\nFor 1., we use the formula proposed by (Arnaudon, 2013) which uses the Riemannian logarithmic map to compute the gradient of the distance. This is implemented as\n\ndef grad_squared_distance(point_a, point_b):\n    \"\"\"Gradient of squared hyperbolic distance.\n\n    Gradient of the squared distance based on the\n    Ball representation according to point_a\n\n    Parameters\n    ----------\n    point_a : array-like, shape=[n_samples, dim]\n        First point in hyperbolic space.\n    point_b : array-like, shape=[n_samples, dim]\n        Second point in hyperbolic space.\n\n    Returns\n    -------\n    dist : array-like, shape=[n_samples, 1]\n        Geodesic squared distance between the two points.\n    \"\"\"\n    hyperbolic_metric = PoincareBall(2).metric\n    log_map = hyperbolic_metric.log(point_b, point_a)\n\n    return -2 * log_map\n\nFor 2. define the log_sigmoid corresponding as follows:\n\ndef log_sigmoid(vector):\n    \"\"\"Logsigmoid function.\n\n    Apply log sigmoid function\n\n    Parameters\n    ----------\n    vector : array-like, shape=[n_samples, dim]\n\n    Returns\n    -------\n    result : array-like, shape=[n_samples, dim]\n    \"\"\"\n    return gs.log((1 / (1 + gs.exp(-vector))))\n\nThe gradient of the logarithm of sigmoid function is implemented as:\n\ndef grad_log_sigmoid(vector):\n    \"\"\"Gradient of log sigmoid function.\n\n    Parameters\n    ----------\n    vector : array-like, shape=[n_samples, dim]\n\n    Returns\n    -------\n    gradient : array-like, shape=[n_samples, dim]\n    \"\"\"\n    return 1 / (1 + gs.exp(vector))\n\nFor 3., apply the composition rule to obtain the gradient of \\(L\\). The following function given \\(\\phi_i\\), \\(\\phi'_j\\) and \\(\\phi'_k\\) returns the total value of \\(L\\) and its gradient vector at \\(\\phi_i\\). For the value of \\(L\\) the loss function formula is simply applied. For the gradient, we apply the composition of grad_log_sigmoid with grad_squared_distance while paying attention to the signs.\n\ndef loss(example_embedding, context_embedding, negative_embedding,\n         manifold):\n    \"\"\"Compute loss and grad.\n\n    Compute loss and grad given embedding of the current example,\n    embedding of the context and negative sampling embedding.\n    \"\"\"\n    n_edges, dim =\\\n        negative_embedding.shape[0], example_embedding.shape[-1]\n    example_embedding = gs.expand_dims(example_embedding, 0)\n    context_embedding = gs.expand_dims(context_embedding, 0)\n    positive_distance =\\\n        manifold.metric.squared_dist(\n            example_embedding, context_embedding)\n    positive_loss =\\\n        log_sigmoid(-positive_distance)\n\n    reshaped_example_embedding =\\\n        gs.repeat(example_embedding, n_edges, axis=0)\n    negative_distance =\\\n        manifold.metric.squared_dist(\n            reshaped_example_embedding, negative_embedding)\n    negative_loss = log_sigmoid(negative_distance)\n\n    total_loss = -(positive_loss + negative_loss.sum())\n\n    positive_log_sigmoid_grad =\\\n        -grad_log_sigmoid(-positive_distance)\n\n    positive_distance_grad =\\\n        grad_squared_distance(example_embedding, context_embedding)\n\n    positive_grad =\\\n        gs.repeat(positive_log_sigmoid_grad, dim, axis=-1)\\\n        * positive_distance_grad\n\n    negative_distance_grad =\\\n        grad_squared_distance(reshaped_example_embedding, negative_embedding)\n\n    negative_distance = gs.to_ndarray(negative_distance,\n                                      to_ndim=2, axis=-1)\n    negative_log_sigmoid_grad =\\\n        grad_log_sigmoid(negative_distance)\n\n    negative_grad = negative_log_sigmoid_grad\\\n        * negative_distance_grad\n    example_grad = -(positive_grad + negative_grad.sum(axis=0))\n\n    return total_loss, example_grad"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#capturing-the-graph-structure",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#capturing-the-graph-structure",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "Capturing the graph structure",
    "text": "Capturing the graph structure\nAt this point we have the necessary bricks to compute the resulting gradient of \\(L\\). We are ready to prepare the nodes \\(v_i\\), \\(v_j\\) and \\(v_k\\) and initialise their embeddings \\(\\phi_i\\), \\(\\phi^{'}_j\\) and \\(\\phi^{'}_k\\). First, initialize an array that will hold embeddings \\(\\phi_i\\) of each node \\(v_i\\in V\\) with random points belonging to the Poincar√© disk.\n\nembeddings = gs.random.normal(size=(karate_graph.n_nodes, dim))\nembeddings = embeddings * 0.2\n\nNext, to prepare the context nodes \\(v_j\\) for each node \\(v_i\\), we compute random walks initialised from each \\(v_i\\) up to some length (5 by default). The latter is done via a special function within the Graph class. The nodes \\(v_j\\) will be later picked from the random walk of \\(v_i\\).\n\nrandom_walks = karate_graph.random_walk()\n\nNegatively sampled nodes \\(v_k\\) are chosen according to the previously defined probability distribution function \\(\\mathcal{P}_n(v_k)\\) implemented as\n\nnegative_table_parameter = 5\nnegative_sampling_table = []\n\nfor i, nb_v in enumerate(nb_vertices_by_edges):\n    negative_sampling_table +=\\\n        ([i] * int((nb_v**(3. / 4.))) * negative_table_parameter)\n\nnegative_sampling_table = gs.array(negative_sampling_table)"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#numerically-optimizing-the-loss-function",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#numerically-optimizing-the-loss-function",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "Numerically optimizing the loss function",
    "text": "Numerically optimizing the loss function\nOptimising the loss function is performed numerically over the number of epochs. At each iteration, we will compute the gradient of \\(L\\). Then the graph nodes are moved in the direction pointed by the gradient. The movement of the nodes is performed by following geodesics in the gradient direction. The key to obtain an embedding representing accurately the dataset, is to move the nodes smoothly rather than brutal movements. This is done by tuning the learning rate, such as at each epoch all the nodes made small movements.\nA first level loop iterates over the epochs, the table total_loss will record the value of \\(L\\) at each iteration and help us track the minimization of \\(L\\).\nA second level nested loop iterates over each path in the previously computed random walks. Observing these walks, notice that nodes having many edges appear more often. Such nodes can be considered as important crossroads and will therefore be subject to a greater number of embedding updates. This is one of the main reasons why random walks have proven to be effective in capturing the structure of graphs. The context of each \\(v_i\\) will be the set of nodes \\(v_j\\) belonging to the random walk from \\(v_i\\). The context_size specified earlier will limit the length of the walk to be considered. Similarly, we use the same context_size to limit the number of negative samples. We find \\(\\phi_i\\) from the embeddings array.\nA third level nested loop will iterate on each \\(v_j\\) and \\(v_k\\). From within, we find \\(\\phi'_j\\) and \\(\\phi'_k\\) then call the loss function to compute the gradient. Then the Riemannian exponential map is applied to find the new value of \\(\\phi_i\\) as we mentioned before.\n\nfor epoch in range(max_epochs):\n    total_loss = []\n    for path in random_walks:\n\n        for example_index, one_path in enumerate(path):\n            context_index = path[max(0, example_index - context_size):\n                                 min(example_index + context_size,\n                                 len(path))]\n            negative_index =\\\n                gs.random.randint(negative_sampling_table.shape[0],\n                                  size=(len(context_index),\n                                  n_negative))\n            negative_index = negative_sampling_table[negative_index]\n\n            example_embedding = embeddings[one_path]\n            for one_context_i, one_negative_i in zip(context_index,\n                                                     negative_index):\n                context_embedding = embeddings[one_context_i]\n                negative_embedding = embeddings[one_negative_i]\n                l, g_ex = loss(\n                    example_embedding,\n                    context_embedding,\n                    negative_embedding,\n                    hyperbolic_manifold)\n                total_loss.append(l)\n\n                example_to_update = embeddings[one_path]\n                embeddings[one_path] = hyperbolic_manifold.metric.exp(\n                    -lr * g_ex, example_to_update)\n    logging.info(\n        'iteration %d loss_value %f',\n        epoch, sum(total_loss, 0) / len(total_loss))\n\nINFO: iteration 0 loss_value 1.826876\nINFO: iteration 1 loss_value 1.774560\nINFO: iteration 2 loss_value 1.725700\nINFO: iteration 3 loss_value 1.663358\nINFO: iteration 4 loss_value 1.655706\nINFO: iteration 5 loss_value 1.615405\nINFO: iteration 6 loss_value 1.581097\nINFO: iteration 7 loss_value 1.526418\nINFO: iteration 8 loss_value 1.507913\nINFO: iteration 9 loss_value 1.505934\nINFO: iteration 10 loss_value 1.466526\nINFO: iteration 11 loss_value 1.453769\nINFO: iteration 12 loss_value 1.443878\nINFO: iteration 13 loss_value 1.451272\nINFO: iteration 14 loss_value 1.397864\nINFO: iteration 15 loss_value 1.396170\nINFO: iteration 16 loss_value 1.373677\nINFO: iteration 17 loss_value 1.390120\nINFO: iteration 18 loss_value 1.382397\nINFO: iteration 19 loss_value 1.404103\nINFO: iteration 20 loss_value 1.395782\nINFO: iteration 21 loss_value 1.389617\nINFO: iteration 22 loss_value 1.410152\nINFO: iteration 23 loss_value 1.390600\nINFO: iteration 24 loss_value 1.374832\nINFO: iteration 25 loss_value 1.367194\nINFO: iteration 26 loss_value 1.323190\nINFO: iteration 27 loss_value 1.389616\nINFO: iteration 28 loss_value 1.361034\nINFO: iteration 29 loss_value 1.384930\nINFO: iteration 30 loss_value 1.340814\nINFO: iteration 31 loss_value 1.349682\nINFO: iteration 32 loss_value 1.317423\nINFO: iteration 33 loss_value 1.346869\nINFO: iteration 34 loss_value 1.327198\nINFO: iteration 35 loss_value 1.363809\nINFO: iteration 36 loss_value 1.352347\nINFO: iteration 37 loss_value 1.317670\nINFO: iteration 38 loss_value 1.320039\nINFO: iteration 39 loss_value 1.323888\nINFO: iteration 40 loss_value 1.341444\nINFO: iteration 41 loss_value 1.312259\nINFO: iteration 42 loss_value 1.315983\nINFO: iteration 43 loss_value 1.305483\nINFO: iteration 44 loss_value 1.325384\nINFO: iteration 45 loss_value 1.328024\nINFO: iteration 46 loss_value 1.306958\nINFO: iteration 47 loss_value 1.303357\nINFO: iteration 48 loss_value 1.303790\nINFO: iteration 49 loss_value 1.324749\nINFO: iteration 50 loss_value 1.328376\nINFO: iteration 51 loss_value 1.313816\nINFO: iteration 52 loss_value 1.325978\nINFO: iteration 53 loss_value 1.317516\nINFO: iteration 54 loss_value 1.353495\nINFO: iteration 55 loss_value 1.331988\nINFO: iteration 56 loss_value 1.346874\nINFO: iteration 57 loss_value 1.348946\nINFO: iteration 58 loss_value 1.324719\nINFO: iteration 59 loss_value 1.330355\nINFO: iteration 60 loss_value 1.331077\nINFO: iteration 61 loss_value 1.305729\nINFO: iteration 62 loss_value 1.311746\nINFO: iteration 63 loss_value 1.347637\nINFO: iteration 64 loss_value 1.326300\nINFO: iteration 65 loss_value 1.309570\nINFO: iteration 66 loss_value 1.313999\nINFO: iteration 67 loss_value 1.346287\nINFO: iteration 68 loss_value 1.300901\nINFO: iteration 69 loss_value 1.323723\nINFO: iteration 70 loss_value 1.320784\nINFO: iteration 71 loss_value 1.313709\nINFO: iteration 72 loss_value 1.312143\nINFO: iteration 73 loss_value 1.309172\nINFO: iteration 74 loss_value 1.320642\nINFO: iteration 75 loss_value 1.308333\nINFO: iteration 76 loss_value 1.325884\nINFO: iteration 77 loss_value 1.316740\nINFO: iteration 78 loss_value 1.325933\nINFO: iteration 79 loss_value 1.316672\nINFO: iteration 80 loss_value 1.312291\nINFO: iteration 81 loss_value 1.332372\nINFO: iteration 82 loss_value 1.317499\nINFO: iteration 83 loss_value 1.329194\nINFO: iteration 84 loss_value 1.305926\nINFO: iteration 85 loss_value 1.304747\nINFO: iteration 86 loss_value 1.342343\nINFO: iteration 87 loss_value 1.331992\nINFO: iteration 88 loss_value 1.295439\nINFO: iteration 89 loss_value 1.332853\nINFO: iteration 90 loss_value 1.332004\nINFO: iteration 91 loss_value 1.357248\nINFO: iteration 92 loss_value 1.342234\nINFO: iteration 93 loss_value 1.329379\nINFO: iteration 94 loss_value 1.313617\nINFO: iteration 95 loss_value 1.310320\nINFO: iteration 96 loss_value 1.320590\nINFO: iteration 97 loss_value 1.315822\nINFO: iteration 98 loss_value 1.328819\nINFO: iteration 99 loss_value 1.339718"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#plotting-results",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#plotting-results",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "Plotting results",
    "text": "Plotting results\nOnce the max_epochs iterations of epochs is achieved, we can plot the resulting embeddings array and the true labels shown as two colors. At 100 epochs we can see that the two group of nodes with different labels are moving away from each other on the manifold. If one increases the max_epochs, then further separability is achieved.\n\nimport matplotlib.patches as mpatches\n\ncolors = {1: 'b', 2: 'r'}\ngroup_1 = mpatches.Patch(color=colors[1], label='Group 1')\ngroup_2 = mpatches.Patch(color=colors[2], label='Group 2')\n\ncircle = visualization.PoincareDisk(point_type='ball')\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.axes.xaxis.set_visible(False)\nax.axes.yaxis.set_visible(False)\ncircle.set_ax(ax)\ncircle.draw(ax=ax)\nfor i_embedding, embedding in enumerate(embeddings):\n    x = embedding[0]\n    y = embedding[1]\n    pt_id = i_embedding\n    plt.scatter(\n        x, y,\n        c=colors[karate_graph.labels[pt_id][0]],\n        s = 150\n        )\n    ax.annotate(pt_id, (x,y))\n\nplt.tick_params(\nwhich='both')\nplt.title('Poincare Ball Embedding of the Karate Club Network')\nplt.legend(handles=[group_1, group_2])\nplt.show()\n\n\n\n\n\n\n\n\nIn geomstats, several unsupervized clustering algorithms on manifolds are implemented such as \\(K\\)-means and Expectation-Maximization.\nLet us apply \\(K\\)-means to learn the node belonging of the two groups and see how well we predicted the true labels. Lets first import \\(K\\)-means\n\nfrom geomstats.learning.kmeans import RiemannianKMeans\n\nSet the number of groups to 2.\n\nn_clusters = 2\n\nInitialize an instance of \\(K\\)-means.\n\nkmeans = RiemannianKMeans(metric= hyperbolic_manifold.metric,\n                          n_clusters=n_clusters,\n                          init='random',\n                          mean_method='batch'\n                              )\n\nFit the embedded nodes\n\ncentroids = kmeans.fit(X=embeddings)\nlabels = kmeans.predict(X=embeddings)\n\nAnd plot the resulting labels provided by \\(K\\)-means\n\ncolors = ['g', 'c', 'm']\ncircle = visualization.PoincareDisk(point_type='ball')\nfig2, ax2 = plt.subplots(figsize=(8, 8))\ncircle.set_ax(ax2)\ncircle.draw(ax=ax2)\nax2.axes.xaxis.set_visible(False)\nax2.axes.yaxis.set_visible(False)\ngroup_1_predicted = mpatches.Patch(color=colors[0], label='Predicted Group 1')\ngroup_2_predicted = mpatches.Patch(color=colors[1], label='Predicted Group 2')\ngroup_centroids = mpatches.Patch(color=colors[2], label='Cluster centroids')\n\nfor i in range(n_clusters):\n    for i_embedding, embedding in enumerate(embeddings):\n        x = embedding[0]\n        y = embedding[1]\n        pt_id = i_embedding\n        if labels[i_embedding] == 0:\n            color = colors[0]\n        else:\n            color = colors[1]\n        plt.scatter(\n            x, y,\n            c=color,\n            s = 150\n            )\n        ax2.annotate(pt_id, (x,y))\n\nfor i_centroid, centroid in enumerate(centroids): \n    x = centroid[0]\n    y = centroid[1]\n    plt.scatter(\n        x, y,\n        c=colors[2],\n        marker='*',\n        s = 150,\n        )\n\nplt.title('K-means applied to Karate club embedding')\nplt.legend(handles = [group_1_predicted, group_2_predicted, group_centroids])\nplt.show()\n\n\n\n\n\n\n\n\nBy comparing the \\(K\\)-means labels and the true labels, notice how \\(K\\)-means accurately finds the two groups of nodes (not perfectly, e.g., nodes 2 and 8). We therefore achieved good performances in predicting the belonging of each member of the Karate club to one of the two groups."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#references",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_graph_embedding_and_clustering_in_hyperbolic_space.html#references",
    "title": "Tutorial: Hyperbolic Embedding of Graphs and Clustering",
    "section": "References",
    "text": "References\n.. [ABY2013] Arnaudon, Marc, Fr√©d√©ric Barbaresco, and Le Yang. ‚ÄúRiemannian medians and means with applications to radar signal processing.‚Äù IEEE Journal of Selected Topics in Signal Processing 7.4 (2013): 595-604.\n.. [GBH2018] Ganea, Octavian, Gary B√©cigneul, and Thomas Hofmann. ‚ÄúHyperbolic neural networks.‚Äù Advances in neural information processing systems. 2018.\n.. [M2013] Mikolov, Tomas, et al.¬†‚ÄúDistributed representations of words and phrases and their compositionality.‚Äù Advances in neural information processing systems. 2013.\n.. [ND2017] Nickel, Maximillian, and Douwe Kiela. ‚ÄúPoincar√© embeddings for learning hierarchical representations.‚Äù Advances in neural information processing systems. 2017."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_optic_nerve_heads_analysis_in_kendall_shape_space.html",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_optic_nerve_heads_analysis_in_kendall_shape_space.html",
    "title": "Tutorial: Computing with shapes of landmarks in Kendall shape spaces",
    "section": "",
    "text": "In this tutorial, we show how to use geomstats to perform a shape data analysis. Specifically, we aim to study the difference between two groups of data: - optical nerve heads that correspond to normal eyes, - optical nerve heads that correspond to glaucoma eyes.\nWe wish to investigate if there is a difference in these two groups, and if this difference is a difference in sizes of the optical nerve heads, or a difference in shapes (where the size has been quotiented out)."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_optic_nerve_heads_analysis_in_kendall_shape_space.html#set-up",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_optic_nerve_heads_analysis_in_kendall_shape_space.html#set-up",
    "title": "Tutorial: Computing with shapes of landmarks in Kendall shape spaces",
    "section": "Set up",
    "text": "Set up\n\nimport os\nimport sys\nimport warnings\n\nsys.path.append(os.path.dirname(os.getcwd()))\nwarnings.filterwarnings('ignore')\n\n\n%matplotlib inline\nimport matplotlib.colors as colors\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\nimport geomstats.backend as gs\nimport geomstats.datasets.utils as data_utils\nfrom geomstats.geometry.pre_shape import PreShapeSpace, KendallShapeMetric\n\nINFO: Using numpy backend\n\n\nWe import the dataset of the optical nerve heads from 22 images of Rhesus monkeys‚Äô eyes (11 monkeys), available in [PE2015].\nFor each monkey, an experimental glaucoma was introduced in one eye, while the second eye was kept as control. One seeks to observe differences between the glaucoma and the control eyes. On each image, 5 anatomical landmarks were recorded: - 1st landmark: superior aspect of the retina, - 2nd landmark: side of the retina closest to the temporal bone of the skull, - 3rd landmark: nose side of the retina, - 4th landmark: inferior point, - 5th landmark: optical nerve head deepest point.\nLabel 0 refers to a normal eye, and Label 1 to an eye with glaucoma.\n\nnerves, labels, monkeys = data_utils.load_optical_nerves()\nprint(nerves.shape)\nprint(labels)\nprint(monkeys)\n\n(22, 5, 3)\n[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n[ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10]\n\n\nWe extract the landmarks‚Äô sets corresponding to the two eyes‚Äô nerves of the first monkey, with their corresponding labels.\n\ntwo_nerves = nerves[monkeys==0]\nprint(two_nerves.shape)\n\ntwo_labels = labels[monkeys==0]\nprint(two_labels)\n\n(2, 5, 3)\n[0 1]\n\n\n\nlabel_to_str = {0: 'Normal nerve', 1: 'Glaucoma nerve'}\nlabel_to_color = {0: (102/255, 178/255, 255/255, 1.), 1: (255/255, 178/255, 102/255, 1.)}\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.set_xlim((2000, 4000))\nax.set_ylim((1000, 5000))\nax.set_zlim((-600, 200))\n\nfor nerve, label in zip(two_nerves, two_labels):\n    x = nerve[:, 0]\n    y = nerve[:, 1]\n    z = nerve[:, 2]\n\n    verts = [list(zip(x,y,z))]\n    \n    poly = Poly3DCollection(verts, alpha=0.5)\n    color =  label_to_color[int(label)]\n    poly.set_color(colors.rgb2hex(color))\n    poly.set_edgecolor('k')\n    ax.add_collection3d(poly)\n\npatch_0 = mpatches.Patch(color=label_to_color[0], label=label_to_str[0], alpha=0.5)\npatch_1 = mpatches.Patch(color=label_to_color[1], label=label_to_str[1], alpha=0.5)\nplt.legend(handles=[patch_0, patch_1], prop={'size': 14})\nplt.show()\n\n\n\n\n\n\n\n\nWe first try to detect if there are two groups of optical nerve heads, based on the 3D coordinates of the landmarks sets.\n\nfrom geomstats.geometry.euclidean import EuclideanMetric\n\nnerves_vec = nerves.reshape(22, -1)\n\neucl_metric = EuclideanMetric(nerves_vec.shape[-1])\n\neucl_dist = eucl_metric.dist_pairwise(nerves_vec)\n\nplt.figure()\nplt.imshow(eucl_dist);\n\n\n\n\n\n\n\n\nWe do not see any two clear clusters.\nWe want to investigate if there is a difference between these two groups of shapes - normal nerve versus glaucoma nerve - or if the main difference is merely relative to the global size of the landmarks‚Äô sets.\n\nm_ambient = 3\nk_landmarks = 5\n\npreshape = PreShapeSpace(m_ambient=m_ambient, k_landmarks=k_landmarks)\nmatrices_metric = preshape.embedding_metric\n\nsizes = matrices_metric.norm(preshape.center(nerves))\n\nplt.figure(figsize=(6, 4))\nfor label, col in label_to_color.items():\n    label_sizes = sizes[labels==label]\n    plt.hist(label_sizes, color=col, label=label_to_str[label], alpha=0.5, bins=10)\n    plt.axvline(gs.mean(label_sizes),  color=col)\nplt.legend(fontsize=14)\nplt.title('Sizes of optical nerves', fontsize=14);\n\n\n\n\n\n\n\n\nThe vertical lines represent the sample mean of each group (normal/glaucoma).\n\nplt.figure(figsize=(6, 4))\nplt.hist(sizes[labels==1] - sizes[labels==0], alpha=0.5)\nplt.axvline(0, color='black')\nplt.title('Difference in size of optical nerve between glaucoma and normal eyes', fontsize=14);\n\n\n\n\n\n\n\n\nWe perform a hypothesis test, testing if the two samples of sizes have the same average. We use the t-test for related samples, since the sample elements are paired: two eyes for each monkey.\n\nfrom scipy import stats\n\nsignif_level = 0.05\n\ntstat, pvalue = stats.ttest_rel(sizes[labels==0], sizes[labels==1])\nprint(pvalue &lt; signif_level)\n\nTrue\n\n\nThere is a significative difference, in optical nerve eyes‚Äô sizes, between the glaucoma and normal eye.\nWe want to investigate if there is a difference in shapes, where the size component has been quotiented out.\nWe project the data to the Kendall pre-shape space, which: - centers the nerve landmark sets so that they share the same barycenter, - normalizes the sizes of the landmarks‚Äô sets to 1.\n\nnerves_preshape = preshape.projection(nerves)\nprint(nerves_preshape.shape)\nprint(preshape.belongs(nerves_preshape))\nprint(gs.isclose(matrices_metric.norm(nerves_preshape), 1.))\n\n(22, 5, 3)\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True]\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True]\n\n\nIn order to quotient out the 3D orientation component, we align the landmark sets in the preshape space.\n\nbase_point = nerves_preshape[0]\n\nnerves_shape = preshape.align(point=nerves_preshape, base_point=base_point)\n\nThe Kendall metric is a Riemannian metric that takes this alignment into account. It corresponds to the metric of the Kendall shape space, which is the manifold defined as the preshape space quotient by the action of the rotation in m_ambient dimensions, here in 3 dimensions.\n\nkendall_metric = KendallShapeMetric(m_ambient=m_ambient, k_landmarks=k_landmarks)\n\nWe can use it to perform a tangent PCA in the Kendall shape space, and determine if we see a difference in the shapes of the optical nerves.\n\nfrom geomstats.learning.pca import TangentPCA\n\ntpca = TangentPCA(kendall_metric)\ntpca.fit(nerves_shape)\n\nplt.plot(\n    tpca.explained_variance_ratio_)\nplt.xlabel(\"Number of principal tangent components\", size=14)\nplt.ylabel(\"Fraction of explained variance\", size=14);\n\n\n\n\n\n\n\n\nTwo principal components already describe around 60% of the variance. We plot the data projected in the tangent space defined by these two principal components.\n\nX = tpca.transform(nerves_shape)\n\nplt.figure(figsize=(12, 12))\n\nfor label, col in label_to_color.items():\n    mask = labels == label\n    plt.scatter(X[mask, 0], X[mask, 1], color=col, s=100, label=label_to_str[label]);\nplt.legend(fontsize=14);\n    \nfor label, x, y in zip(monkeys, X[:, 0], X[:, 1]):\n    plt.annotate(\n        label,\n        xy=(x, y), xytext=(-20, 20),\n        textcoords='offset points', ha='right', va='bottom',\n        bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.5),\n        arrowprops=dict(arrowstyle = '-&gt;', connectionstyle='arc3,rad=0'))\n\nplt.show()\n\n\n\n\n\n\n\n\nThe indices represent the monkeys‚Äô indices.\nIn contrast to the above study focusing on the optical nerves‚Äô sizes, visual inspection does not reveal any clusters between the glaucoma and normal optical nerves‚Äô shapes. We also do not see any obvious pattern between the two optical nerves of the same monkey.\nThis shows that the difference between the optical nerve heads mainly resides in the over sizes of the optical nerves.\n\ndist_pairwise = kendall_metric.dist_pairwise(nerves_shape)\nprint(dist_pairwise .shape)\n\n(22, 22)\n\n\n\nplt.figure()\nplt.imshow(dist_pairwise);\n\n\n\n\n\n\n\n\nWe try a agglomerative hierarchical clustering to investigate if we can cluster in the Kendall shape space.\n\nfrom geomstats.learning.agglomerative_hierarchical_clustering import AgglomerativeHierarchicalClustering\n\nclustering = AgglomerativeHierarchicalClustering(distance='precomputed', n_clusters=2)\nclustering.fit(dist_pairwise)\npredicted_labels = clustering.labels_\n\nprint('True labels:', labels)\nprint('Predicted labels:', predicted_labels)\n\naccuracy = gs.sum(labels==predicted_labels) / len(labels)\nprint(f'Accuracy: {accuracy:.2f}')\n\nTrue labels: [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\nPredicted labels: [0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1]\nAccuracy: 0.55\n\n\nThe accuracy is barely above the accuracy of a random classifier, that would assign 0 or 1 with probably 0.5 to each of the shapes. This confirms that the difference that exists between the two groups is mostly due to the landmarks‚Äô set size and not their shapes."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_optic_nerve_heads_analysis_in_kendall_shape_space.html#references",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_optic_nerve_heads_analysis_in_kendall_shape_space.html#references",
    "title": "Tutorial: Computing with shapes of landmarks in Kendall shape spaces",
    "section": "References",
    "text": "References\n.. [PE2015] Patrangenaru and L. Ellingson. Nonparametric Statistics on Manifolds and Their Applications to Object Data, 2015. https://doi.org/10.1201/b18969"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html",
    "title": "Tutorial: Hand gesture classification with EMG data using Riemannian metrics",
    "section": "",
    "text": "In this notebook we are using EMG time series collected by 8 electrodes placed on the arm skin. We are going to show how to:"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#context",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#context",
    "title": "Tutorial: Hand gesture classification with EMG data using Riemannian metrics",
    "section": "Context",
    "text": "Context\nThe data are acquired from somOS-interface: an sEMG armband that allows you to interact via bluetooth with an Android smartphone (you can contact Marius Guerard (marius.guerard@gmail.com) or Renaud Renault (renaud.armand.renault@gmail.com) for more info on how to make this kind of armband yourself).\nAn example of application is to record static signs that are linked with different actions (moving a cursor and clicking, sign recognition for command based personal assistants, ‚Ä¶). In these experiments, we want to evaluate the difference in performance (measured as the accuracy of sign recognition) between three different real life situations where we change the conditions of training (when user record signs or ‚Äúcalibrate‚Äù the device) and testing (when the app guess what sign the user is doing):\n\n\nWhat is the accuracy when doing sign recognition right after training?\n\n\nWhat is the accuracy when calibrating, removing and replacing the armband at the same position and then testing?\n\n\nWhat is the accuracy when calibrating, removing the armband and giving it to someone else that is testing it without calibration?\n\n\nTo simulate these situations, we record data from two different users (rr and mg) and in two different sessions (s1 or s2). The user put the bracelet before every session and remove it after every session.\nQuick description of the data:\n\nEach row corresponds to one acquisition, there is an acquisition every ~4 ms for 8 electrodes which correspond to a 250Hz acquisition rate.\nThe time column is in ms.\nThe columns c0 to c7 correspond to the electrical value recorded at each of the 8 electrodes (arbitrary unit).\nThe label correspond to the sign being recorded by the user at this time point (‚Äòrest‚Äô, ‚Äòrock‚Äô, ‚Äòpaper‚Äô, ‚Äòscissors‚Äô, or ‚Äòok). ‚Äôrest‚Äô correspond to a rested arm.\nthe exp identify the user (rr and mg) and the session (s1 or s2)\n\nNote: Another interesting use case, not explored in this notebook, would be to test what is the accruacy when calibrating, removing the armband and giving it to someone else that is calibrating it on its own arm before testing it. The idea being that transfer learning might help getting better results (or faster calibration) than calibrating on one user."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#setup",
    "title": "Tutorial: Hand gesture classification with EMG data using Riemannian metrics",
    "section": "Setup",
    "text": "Setup\nBefore starting this tutorial, we set the working directory to be the root of the geomstats repository. In order to have the code working on your machine, you need to change this path to the path of your geomstats repository.\n\nimport os\nimport subprocess\nimport matplotlib\nmatplotlib.interactive(True)\nimport matplotlib.pyplot as plt\n\ngeomstats_gitroot_path = subprocess.check_output(\n    ['git', 'rev-parse', '--show-toplevel'], \n    universal_newlines=True)\n\nos.chdir(geomstats_gitroot_path[:-1])\n\nprint('Working directory: ', os.getcwd())\n\nimport geomstats.backend as gs\n\ngs.random.seed(2021)\n\nWorking directory:  /home/marius/proj/geomstats\n\n\nINFO: Using numpy backend"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#parameters",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#parameters",
    "title": "Tutorial: Hand gesture classification with EMG data using Riemannian metrics",
    "section": "Parameters",
    "text": "Parameters\n\nN_ELECTRODES = 8\nN_SIGNS = 4"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#the-data",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#the-data",
    "title": "Tutorial: Hand gesture classification with EMG data using Riemannian metrics",
    "section": "The Data",
    "text": "The Data\n\nimport geomstats.datasets.utils as data_utils\n\ndata = data_utils.load_emg()\n\n\ndata.head()\n\n\n\n\n\n\n\n\ntime\nc0\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nlabel\nexp\n\n\n\n\n0\n23\n127\n123\n128\n134\n125\n128\n130\n124\nrest\nmg_s1\n\n\n1\n28\n126\n130\n128\n119\n129\n128\n126\n133\nrest\nmg_s1\n\n\n2\n32\n129\n130\n127\n125\n129\n129\n127\n130\nrest\nmg_s1\n\n\n3\n36\n127\n128\n126\n123\n128\n127\n125\n131\nrest\nmg_s1\n\n\n4\n40\n127\n128\n129\n124\n127\n129\n127\n128\nrest\nmg_s1\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(N_SIGNS, figsize=(20, 20))\nlabel_list = ['rock', 'scissors', 'paper', 'ok']\nfor i, label_i in enumerate(label_list):\n    sign_df = data[data.label==label_i].iloc[:100]\n    for electrode in range(N_ELECTRODES):\n        ax[i].plot(sign_df.iloc[:, 1 + electrode])\n        ax[i].title.set_text(label_i)\n\n\n\n\n\n\n\n\nWe are removing the sign ‚Äòrest‚Äô for the rest of the analysis.\n\ndata = data[data.label != 'rest']\n\n\nPreprocessing into covariance matrices\n\nimport numpy as np\nimport pandas as pd\n\n### Parameters.\nN_STEPS = 100\nLABEL_MAP = {'rock': 0, 'scissors': 1, 'paper': 2, 'ok': 3}\nMARGIN = 1000\n\nUnpacking data into arrays for batching\n\ndata_dict = {\n    'time': gs.array(data.time),\n    'raw_data': gs.array(data[['c{}'.format(i) for i in range(N_ELECTRODES)]]),\n    'label': gs.array(data.label),\n    'exp': gs.array(data.exp)}\n\n\nfrom geomstats.datasets.prepare_emg_data import TimeSeriesCovariance\n\ncov_data = TimeSeriesCovariance(data_dict, N_STEPS, N_ELECTRODES, LABEL_MAP, MARGIN)\ncov_data.transform()\n\nWe check that these matrics belong to the space of SPD matrices.\n\nimport geomstats.geometry.spd_matrices as spd\n\nmanifold = spd.SPDMatrices(N_ELECTRODES)\n\n\ngs.all(manifold.belongs(cov_data.covs))\n\nTrue\n\n\n\nCovariances plot of the euclidean average\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 10))\nfor label_i, i in cov_data.label_map.items():\n    label_ids = np.where(cov_data.labels==i)[0]\n    sign_cov_mat = cov_data.covs[label_ids]\n    mean_cov = np.mean(sign_cov_mat, axis=0)\n    ax[i // 2, i % 2].matshow(mean_cov)\n    ax[i // 2, i % 2].title.set_text(label_i)\n\n\n\n\n\n\n\n\nLooking at the euclidean average of the spd matrices for each sign, does not show a striking difference between 3 of our signs (scissors, paper, and ok). Minimum Distance to Mean (MDM) algorithm will probably performed poorly if using euclidean mean here.\n\n\nCovariances plot of the Frechet Mean of the affine invariant metric\n\nfrom geomstats.learning.frechet_mean import FrechetMean\nfrom geomstats.geometry.spd_matrices import SPDMetricAffine\n\n\nmetric_affine = SPDMetricAffine(N_ELECTRODES)\nmean_affine = FrechetMean(metric=metric_affine, point_type='matrix')\n\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 10))\nfor label_i, i in cov_data.label_map.items():\n    label_ids = np.where(cov_data.labels==i)[0]\n    sign_cov_mat = cov_data.covs[label_ids]\n    mean_affine.fit(X=sign_cov_mat)\n    mean_cov = mean_affine.estimate_\n    ax[i // 2, i % 2].matshow(mean_cov)\n    ax[i // 2, i % 2].title.set_text(label_i)\n\n\n\n\n\n\n\n\nWe see that the average matrices computed using the affine invariant metric are now more differenciated from each other and can potentially give better results, when using MDM to predict the sign linked to a matrix sample."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#sign-classification",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_emg_sign_classification_in_spd_manifold.html#sign-classification",
    "title": "Tutorial: Hand gesture classification with EMG data using Riemannian metrics",
    "section": "Sign Classification",
    "text": "Sign Classification\nWe are now going to train some classifiers on those matrices to see how we can accurately discriminate these 4 hand positions. The baseline accuracy is defined as the accuracy we get by randomly guessing the signs. In our case, the baseline accuracy is 25%.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Hiding the numerous sklearn warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n!pip install tensorflow\n\n\nRequirement already satisfied: keras in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (2.3.1)\n\nRequirement already satisfied: numpy&gt;=1.9.1 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from keras) (1.19.5)\n\nRequirement already satisfied: keras-applications&gt;=1.0.6 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from keras) (1.0.8)\n\nRequirement already satisfied: pyyaml in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from keras) (5.3)\n\nRequirement already satisfied: keras-preprocessing&gt;=1.0.5 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from keras) (1.1.2)\n\nRequirement already satisfied: scipy&gt;=0.14 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from keras) (1.4.1)\n\nRequirement already satisfied: h5py in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from keras) (2.10.0)\n\nRequirement already satisfied: six&gt;=1.9.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from keras) (1.12.0)\n\nRequirement already satisfied: tensorflow in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (2.4.1)\n\nRequirement already satisfied: typing-extensions~=3.7.4 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (3.7.4.3)\n\nRequirement already satisfied: google-pasta~=0.2 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n\nRequirement already satisfied: flatbuffers~=1.12.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (1.12)\n\nRequirement already satisfied: wrapt~=1.12.1 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n\nRequirement already satisfied: tensorboard~=2.4 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (2.4.1)\n\nCollecting six~=1.15.0 (from tensorflow)\n\n  Using cached https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n\nRequirement already satisfied: absl-py~=0.10 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (0.11.0)\n\nRequirement already satisfied: tensorflow-estimator&lt;2.5.0,&gt;=2.4.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n\nRequirement already satisfied: grpcio~=1.32.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (1.32.0)\n\nRequirement already satisfied: opt-einsum~=3.3.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n\nRequirement already satisfied: numpy~=1.19.2 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (1.19.5)\n\nRequirement already satisfied: wheel~=0.35 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n\nRequirement already satisfied: astunparse~=1.6.3 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n\nRequirement already satisfied: h5py~=2.10.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n\nRequirement already satisfied: gast==0.3.3 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n\nRequirement already satisfied: protobuf&gt;=3.9.2 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (3.14.0)\n\nRequirement already satisfied: termcolor~=1.1.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorboard~=2.4-&gt;tensorflow) (2.22.0)\n\nRequirement already satisfied: werkzeug&gt;=0.11.15 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorboard~=2.4-&gt;tensorflow) (0.15.4)\n\nRequirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorboard~=2.4-&gt;tensorflow) (0.4.1)\n\nRequirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorboard~=2.4-&gt;tensorflow) (1.7.1)\n\nCollecting setuptools&gt;=41.0.0 (from tensorboard~=2.4-&gt;tensorflow)\n\n  Downloading https://files.pythonhosted.org/packages/ae/4d/153a2cfab2ea03d4f4aee45d9badb52426db9e2275edfb4b825c5dc55a10/setuptools-54.1.0-py3-none-any.whl (784kB)\n\n    100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 788kB 6.1MB/s eta 0:00:01   35% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 276kB 4.1MB/s eta 0:00:01\n\nRequirement already satisfied: markdown&gt;=2.6.8 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorboard~=2.4-&gt;tensorflow) (3.1.1)\n\nRequirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from tensorboard~=2.4-&gt;tensorflow) (1.8.0)\n\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.4-&gt;tensorflow) (1.25.7)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.4-&gt;tensorflow) (2020.6.20)\n\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.4-&gt;tensorflow) (2.8)\n\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard~=2.4-&gt;tensorflow) (3.0.4)\n\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.4-&gt;tensorflow) (1.3.0)\n\nRequirement already satisfied: cachetools&lt;3.2,&gt;=2.0.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.4-&gt;tensorflow) (3.1.1)\n\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.4-&gt;tensorflow) (0.2.7)\n\nRequirement already satisfied: rsa&lt;4.1,&gt;=3.1.4 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.4-&gt;tensorflow) (4.0)\n\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard~=2.4-&gt;tensorflow) (3.1.0)\n\nRequirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /home/marius/miniconda3/envs/stats/lib/python3.7/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard~=2.4-&gt;tensorflow) (0.4.8)\n\nastroid 2.2.5 requires typed-ast&gt;=1.3.0; implementation_name == \"cpython\", which is not installed.\n\nInstalling collected packages: six, setuptools\n\n  Found existing installation: six 1.12.0\n\n    Uninstalling six-1.12.0:\n\n      Successfully uninstalled six-1.12.0\n\n  Found existing installation: setuptools 40.8.0\n\n    Uninstalling setuptools-40.8.0:\n\n      Successfully uninstalled setuptools-40.8.0\n\nSuccessfully installed setuptools-54.1.0 six-1.15.0\n\n\n\n\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nimport tensorflow as tf\n\nN_EPOCHS is the number of epochs on which to train the MLP. Recommended is ~100\n\nN_EPOCHS = 10\nN_FEATURES = int(N_ELECTRODES * (N_ELECTRODES + 1) / 2)\n\n\nA. Test on the same session and user as Training/Calibration\nIn this first part we are training our model on the same session that we are testing it on. In real life, it corresponds to a user calibrating his armband right before using it. To do this, we are splitting every session in k-folds, training on \\((k-1)\\) fold to test on the \\(k^{th}\\) last fold.\n\nclass ExpResults:\n    \"\"\"Class handling the score collection and plotting among the different experiments.\n    \"\"\"\n    \n    def __init__(self, exps):\n        self.exps = exps\n        self.results = {}\n        self.exp_ids = {}\n        # Compute the index corresponding to each session only once at initialization.\n        for exp in set(self.exps):\n            self.exp_ids[exp] = np.where(self.exps==exp)[0]\n    \n    def add_result(self, model_name, model, X, y):\n        \"\"\"Add the results from the cross validated pipeline.\n        \n        For the model 'pipeline', it will add the cross validated results of every session in the model_name\n        entry of self.results.\n        \n        Parameters\n        ---------- \n        model_name : str\n            Name of the pipeline/model that we are adding results from.\n        model : sklearn.pipeline.Pipeline\n            sklearn pipeline that we are evaluating.\n        X : array\n            data that we are ingesting in the pipeline.\n        y : array\n            labels corresponding to the data.\n        \"\"\"\n        self.results[model_name] = {'fit_time': [], 'score_time': [], 'test_score': [], 'train_score': []}\n        for exp in self.exp_ids.keys():\n            ids = self.exp_ids[exp]\n            exp_result = cross_validate(pipeline, X[ids], y[ids], return_train_score=True)\n            for key in exp_result.keys():\n                self.results[model_name][key] += list(exp_result[key])\n        print('Average training score: {:.4f}, Average test score: {:.4f}'.format(\n            np.mean(self.results[model_name]['train_score']), \n            np.mean(self.results[model_name]['test_score'])))\n        \n    def plot_results(self, title, variables, err_bar=None, save_name=None, xlabel='Model', ylabel='Acc'):\n        \"\"\"Plot bar plot comparing the different pipelines' results.\n        \n        Compare the results added previously using the 'add_result' method with bar plots.\n        \n        Parameters\n        ---------- \n        title : str\n            Title of the plot.\n        variables : list of array\n            List of the variables to plot (e.g. train_score, test_score,...)\n        err_bar : list of float\n            list of error to use for plotting error bars. If None, std is used by default.\n        save_name : str\n            path to save the plot. If None, plot is not saved.\n        xlabel : str\n            Label of the x-axis.\n        ylabel : str\n            Label of the y-axis.\n        \"\"\"\n        ### Some defaults parameters.\n        w = 0.5                                                                                                                                                                        \n        colors = ['b', 'r', 'gray']\n        \n        ### Reshaping the results for plotting.\n        x_labels = self.results.keys()\n        list_vec = []\n        for variable in variables:\n            list_vec.append(np.array([self.results[model][variable] for model in x_labels]).transpose())\n        rand_m1 = lambda size: np.random.random(size) * 2 - 1\n        \n        ### Plots parameters.                                                                                                                                                                                   \n        label_loc = np.arange(len(x_labels))                                                                                                                                            \n        center_bar = [w * (i - 0.5) for i in range(len(list_vec))]\n        \n        ### Plots values.                                                                                                                                                                                       \n        avg_vec = [np.nanmean(vec, axis=0) for vec in list_vec]\n        if err_bar is None:\n            err_bar = [np.nanstd(vec, axis=0) for vec in list_vec]\n        \n        ### Plotting the data.                                                                                                                                                                                  \n        fig, ax = plt.subplots(figsize=(20, 15))\n        for i, vec in enumerate(list_vec):\n            label_i = variable[i] + ' (n = {})'.format(len(vec))\n            rects = ax.bar(label_loc + center_bar[i], avg_vec[i], w, label=label_i,\n                           yerr=err_bar[i], color=colors[i], alpha=0.6)\n            for j, x in enumerate(label_loc):\n                ax.scatter((x + center_bar[i]) + rand_m1(vec[:, j].size) * w/4,\n                           vec[:, j], color=colors[i], edgecolor='k')\n\n        # Add some text for labels, title and custom x-axis tick labels, etc.                                                                                                                                   \n        ax.set_xlabel(xlabel)\n        ax.set_ylabel(ylabel)\n        ax.set_title(title)\n        ax.set_xticks(label_loc)\n        ax.set_xticklabels(x_labels)\n        ax.legend()\n        plt.legend()\n        \n        ### Saving the figure with a timestamp as a name.                                                                                                                                                       \n        if save_name is not None:\n            plt.savefig(save_name)\n\n\nexp_arr = data.exp.iloc[cov_data.batches]\nintra_sessions_results = ExpResults(exp_arr)\n\n\nA.0. Using Logistic Regression on the vectorized Matrix (Euclidean Method)\n\npipeline = Pipeline(\n    steps=[('standardize', StandardScaler()),\n           ('logreg', LogisticRegression(solver='lbfgs', multi_class='multinomial'))])\n\nintra_sessions_results.add_result(model_name='logreg_eucl', model=pipeline, X=cov_data.covecs, y=cov_data.labels)\n\nAverage training score: 0.9937, Average test score: 0.9165\n\n\n\n\nA.1. Using MLP on the vectorized Matrix (Euclidean Method)\n\ndef create_model(weights='initial_weights.hd5', n_features=N_FEATURES, n_signs=N_SIGNS):\n    \"\"\"Function to create model, required for using KerasClassifier and wrapp a Keras model inside a \n    scikitlearn form.\n    We added a weight saving/loading to remove the randomness of the weight initialization (for better comparison).\n    \"\"\"\n    model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(n_features, activation='relu', input_shape=(n_features,)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(17, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(n_signs, activation='softmax'),\n])\n    \n    model.compile(loss = 'sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    if weights is None:\n        model.save_weights('initial_weights.hd5')\n    else:\n        model.load_weights(weights)\n    return model\n\ndef create_model_covariance(weights='initial_weights.hd5'):\n    return create_model(weights=weights, n_features=N_FEATURES)\n\nUse the line below to generate the ‚Äòinitial_weights.hd5‚Äô file\n\ngenerate_weights = create_model(weights=None)\n\n\npipeline = Pipeline(\n    steps=[('standardize', StandardScaler()),\n           ('mlp', KerasClassifier(build_fn=create_model, epochs=N_EPOCHS, verbose=0))])\n\nintra_sessions_results.add_result(model_name='mlp_eucl', model=pipeline, X=cov_data.covecs, y=cov_data.labels)\n\nAverage training score: 0.9445, Average test score: 0.8083\n\n\n\n\nA.2. Using Tangent space projection + Logistic Regression\n\nfrom geomstats.learning.preprocessing import ToTangentSpace\n\npipeline = Pipeline(\n    steps=[('feature_ext', ToTangentSpace(geometry=metric_affine)),\n           ('standardize', StandardScaler()),\n           ('logreg', LogisticRegression(solver='lbfgs', multi_class='multinomial'))])\n\nintra_sessions_results.add_result(model_name='logreg_affinvariant_tangent', model=pipeline, X=cov_data.covs, y=cov_data.labels)\n\nAverage training score: 0.9959, Average test score: 0.9200\n\n\n\n\nA.3. Using Tangent space projection + MLP\n\npipeline = Pipeline(\n    steps=[('feature_ext', ToTangentSpace(geometry=metric_affine)),\n           ('standardize', StandardScaler()),\n           ('mlp', KerasClassifier(build_fn=create_model_covariance, epochs=N_EPOCHS, verbose=0))])\n\nintra_sessions_results.add_result(model_name='mlp_affinvariant_tangent', model=pipeline, X=cov_data.covs, y=cov_data.labels)\n\nAverage training score: 0.9601, Average test score: 0.8358\n\n\n\n\nA.4. Using Euclidean MDM\n\nfrom geomstats.learning.mdm import RiemannianMinimumDistanceToMeanClassifier\nfrom geomstats.geometry.spd_matrices import SPDMetricEuclidean\n\npipeline = Pipeline(\n    steps=[('clf', RiemannianMinimumDistanceToMeanClassifier(\n        riemannian_metric=SPDMetricEuclidean(n=N_ELECTRODES),\n        n_classes=N_SIGNS))])\n\nintra_sessions_results.add_result(model_name='mdm_eucl', model=pipeline, X=cov_data.covs, y=cov_data.labels)\n\nAverage training score: 0.8552, Average test score: 0.7710\n\n\n\n\nA.5. Using Riemannian MDM\n\npipeline = Pipeline(\n    steps=[('clf', RiemannianMinimumDistanceToMeanClassifier(\n        riemannian_metric=SPDMetricAffine(n=N_ELECTRODES),\n        n_classes=N_SIGNS))])\n\nintra_sessions_results.add_result(model_name='mdm_affinvariant', model=pipeline, X=cov_data.covs, y=cov_data.labels)\n\nAverage training score: 0.9342, Average test score: 0.8353\n\n\n\n\nSummary plots\n\nintra_sessions_results.plot_results('intra_sess', ['test_score'])"
  },
  {
    "objectID": "wl-mechanics/notebooks/data.html",
    "href": "wl-mechanics/notebooks/data.html",
    "title": "M Dims blog",
    "section": "",
    "text": "%pylab inline\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nimport sys\nsys.path.append('wl-mechanics')\nimport datasets\nfrom models import utils as mutils\nfrom models import mlp\nimport train_utils as tutils\nfrom flax.training import checkpoints\n\n2023-11-16 22:57:46.633812: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /pkgs/cuda-11.8/targets/x86_64-linux/lib:/pkgs/cudnn-8.8/lib:/pkgs/cuda-11.3/lib64:/pkgs/nccl_2.9.9-1+cuda11.3_x86_64:/pkgs/nccl_2.8.3-1+cuda11.0_x86_64/lib:/pkgs/cuda-11.8/targets/x86_64-linux/lib:/pkgs/cudnn-8.8/lib\n2023-11-16 22:57:46.634637: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /pkgs/cuda-11.8/targets/x86_64-linux/lib:/pkgs/cudnn-8.8/lib:/pkgs/cuda-11.3/lib64:/pkgs/nccl_2.9.9-1+cuda11.3_x86_64:/pkgs/nccl_2.8.3-1+cuda11.0_x86_64/lib:/pkgs/cuda-11.8/targets/x86_64-linux/lib:/pkgs/cudnn-8.8/lib\n2023-11-16 22:57:46.634662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n/ssd003/home/kirill/venvs/jax-env/lib/python3.9/site-packages/flax/core/frozen_dict.py:169: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n  jax.tree_util.register_keypaths(\n\n\n\n!ls wl-mechanics/assets\n\nebdata_v3.h5ad             test_4i.npz\nop_cite_inputs_0.h5ad          test_embrio.npz\nop_train_multi_targets_0.h5ad  train_4i.npz\not.pdf                 train_embrio.npz\not.png                 ubot.pdf\nphot.pdf               ubot.png\nphot.png               v2_full-4i_normalized.csv\nsb.pdf                 v2_full-4i_train_test_split_idxs.npz\nsb.png                 val_embrio.npz\n\n\n\nwith np.load('wl-mechanics/assets/v2_full-4i_train_test_split_idxs.npz') as data:\n  print(data.files)\n  print(data['train_idxs'], len(data['train_idxs']))\n  print(data['test_idxs'], len(data['test_idxs']))\n  train_ids = data['train_idxs']\n  test_ids = data['test_idxs']\n\n['train_idxs', 'test_idxs']\n[165900 122300  32952 ...   9404 177057  63966] 254148\n[ 64517 177348  56344 ... 194509 316355 238922] 63537\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv('wl-mechanics/assets/v2_full-4i_normalized.csv')\ndf.head()\n\n\n\n\n\n\n\n\ntime\nNuclei_Morphology_Area\nNuclei_Morphology_Circularity\nNuclei_Morphology_Convexity\nNuclei_Morphology_Eccentricity\nNuclei_Morphology_Elongation\nNuclei_Morphology_Equivalent_Diameter\nNuclei_Morphology_Extent\nNuclei_Morphology_Major_Axis_Length\nNuclei_Morphology_Mean_Radius\n...\nIntensity_mean_pAKT\nIntensity_mean_pEGFR\nIntensity_mean_aTUB\nIntensity_mean_pERK\nIntensity_mean_Ki67\nIntensity_mean_pMET\nIntensity_mean_CD45\nIntensity_mean_ClCasp3\nIntensity_mean_TotProtein\nCondition\n\n\n\n\n0\n8\n-0.274112\n-1.161924\n-1.024294\n-2.117290\n-1.385679\n-0.282769\n-0.084229\n-0.867892\n-0.119449\n...\n2.740725\n2.050947\n1.256621\n1.159009\n1.124681\n3.885301\n1.724430\n4.769124\n0.986931\nixazomib\n\n\n1\n8\n-1.140102\n-0.974085\n-2.296770\n-1.142132\n-0.906933\n-1.328908\n-1.082079\n-1.446597\n-1.284489\n...\n0.591499\n-0.120799\n-0.599442\n0.443334\n-0.119510\n1.289209\n0.819025\n2.304385\n0.167991\nixazomib\n\n\n2\n8\n0.232487\n-2.525538\n-3.063312\n-2.752453\n-1.588989\n0.227103\n-0.612237\n-0.467192\n-0.390752\n...\n1.763594\n2.121434\n0.368434\n0.297882\n1.091764\n3.158411\n3.025618\n4.459429\n1.128970\nixazomib\n\n\n3\n8\n1.428426\n-2.658096\n-2.244442\n-0.976498\n-0.802136\n1.259917\n-1.410106\n0.608182\n0.698723\n...\n0.704826\n1.046160\n-0.100396\n0.284098\n0.472126\n1.877242\n1.778915\n2.808882\n0.744667\nixazomib\n\n\n4\n8\n2.323858\n-2.901381\n-3.469861\n-1.247588\n-0.969757\n1.928539\n-1.712069\n1.127107\n1.269271\n...\n0.843195\n0.548065\n-0.587789\n0.705410\n0.499171\n2.220354\n2.300006\n3.848469\n0.693036\nixazomib\n\n\n\n\n5 rows √ó 52 columns\n\n\n\n\nnp.unique(df['time'])\n\narray([ 8, 24, 48])\n\n\n\ndrugs = 'trametinib_erlotinib'\n# df = df[df['Condition'] == drugs]\n# df\n\n\ntrain_df = df.iloc[train_ids]\ntrain_df = train_df[train_df['Condition'] == drugs]\nval_df = train_df.iloc[np.random.choice(len(train_df), 1000, replace=False)]\ntest_df = df.iloc[test_ids]\ntest_df = test_df[test_df['Condition'] == drugs]\nfeatures = list(filter(lambda s: s != 'time' and s != 'Condition', df.columns))\n\n\ndef save_numpy(df, name):\n  X = df[features].to_numpy()\n  t = df['time'].to_numpy()\n  np.savez(f'wl-mechanics/assets/{name}_4i.npz', X=X, ts=t)\n\n\nsave_numpy(train_df, 'train')\nsave_numpy(val_df, 'val')\nsave_numpy(test_df, 'test')\n\n\nval_df\n\n\n\n\n\n\n\n\ntime\nNuclei_Morphology_Area\nNuclei_Morphology_Circularity\nNuclei_Morphology_Convexity\nNuclei_Morphology_Eccentricity\nNuclei_Morphology_Elongation\nNuclei_Morphology_Equivalent_Diameter\nNuclei_Morphology_Extent\nNuclei_Morphology_Major_Axis_Length\nNuclei_Morphology_Mean_Radius\n...\nIntensity_mean_pAKT\nIntensity_mean_pEGFR\nIntensity_mean_aTUB\nIntensity_mean_pERK\nIntensity_mean_Ki67\nIntensity_mean_pMET\nIntensity_mean_CD45\nIntensity_mean_ClCasp3\nIntensity_mean_TotProtein\nCondition\n\n\n\n\n178715\n24\n0.106599\n-1.084040\n-0.528961\n-0.256009\n-0.245753\n0.105466\n-0.770606\n-0.102526\n0.088115\n...\n3.288893\n0.013928\n0.318092\n-0.188659\n0.045889\n-0.017199\n-0.009561\n-0.218826\n-0.173393\ntrametinib_erlotinib\n\n\n294139\n48\n2.407107\n-1.198517\n-1.735799\n0.572473\n0.680211\n1.987301\n-1.127095\n2.419034\n0.890756\n...\n-0.270605\n-0.659487\n-0.416262\n-0.204056\n-0.587652\n-0.939749\n-0.467970\n-0.661947\n-0.646881\ntrametinib_erlotinib\n\n\n112143\n24\n-1.228426\n0.755796\n0.486999\n0.168205\n0.178844\n-1.454557\n0.674454\n-1.313883\n-1.074927\n...\n-0.310512\n-0.500434\n1.248626\n-0.211290\n0.182418\n-0.393597\n-0.497840\n-0.553563\n0.184783\ntrametinib_erlotinib\n\n\n112089\n24\n-0.201015\n-0.659578\n-1.369740\n0.058553\n0.060555\n-0.205621\n-0.618095\n-0.204020\n-0.243422\n...\n-0.417168\n-0.418095\n-0.608661\n-0.211274\n-0.482743\n-0.675267\n-0.382926\n-0.523685\n-0.603285\ntrametinib_erlotinib\n\n\n179118\n24\n0.769543\n0.566387\n0.384051\n-0.560598\n-0.502799\n0.715370\n0.342940\n0.288114\n0.931191\n...\n1.927283\n0.275076\n-0.163957\n-0.207075\n0.252699\n-0.077697\n-0.180494\n-0.323031\n-0.190461\ntrametinib_erlotinib\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n294092\n48\n1.043655\n-0.489972\n-0.620366\n0.204768\n0.219781\n0.948306\n-0.689368\n0.911565\n0.748560\n...\n-0.444985\n-0.573890\n-0.101664\n-0.203490\n-0.494840\n-0.752913\n-0.384293\n-0.616366\n-0.532592\ntrametinib_erlotinib\n\n\n161614\n24\n0.582741\n0.913484\n0.818840\n0.054915\n0.056744\n0.550699\n0.951304\n0.425165\n0.805580\n...\n-0.340261\n0.155849\n0.889945\n-0.217325\n0.286034\n0.017111\n-0.055508\n-0.180895\n0.444789\ntrametinib_erlotinib\n\n\n76580\n8\n0.087310\n-0.356306\n0.085397\n-0.049801\n-0.050187\n0.086554\n0.606696\n-0.036671\n0.244805\n...\n-0.152657\n-0.223259\n0.047698\n-0.173601\n-0.259046\n-0.516987\n-0.193116\n-0.403409\n-0.457272\ntrametinib_erlotinib\n\n\n178780\n24\n-0.084264\n-0.213894\n-1.080568\n0.483532\n0.559793\n-0.085077\n-0.261275\n0.275390\n-0.384250\n...\n2.670602\n-0.228067\n0.101521\n-0.207039\n-0.149082\n-0.285899\n-0.373466\n-0.479985\n-0.503311\ntrametinib_erlotinib\n\n\n178353\n24\n-0.519797\n0.590674\n0.381401\n-1.092734\n-0.876475\n-0.552603\n1.070267\n-0.908265\n-0.197425\n...\n3.523514\n1.649901\n2.133733\n-0.199916\n1.377278\n2.157987\n1.166302\n1.350943\n0.251910\ntrametinib_erlotinib\n\n\n\n\n1000 rows √ó 52 columns\n\n\n\n\ntest_df\n\n\n\n\n\n\n\n\ntime\nNuclei_Morphology_Area\nNuclei_Morphology_Circularity\nNuclei_Morphology_Convexity\nNuclei_Morphology_Eccentricity\nNuclei_Morphology_Elongation\nNuclei_Morphology_Equivalent_Diameter\nNuclei_Morphology_Extent\nNuclei_Morphology_Major_Axis_Length\nNuclei_Morphology_Mean_Radius\n...\nIntensity_mean_pAKT\nIntensity_mean_pEGFR\nIntensity_mean_aTUB\nIntensity_mean_pERK\nIntensity_mean_Ki67\nIntensity_mean_pMET\nIntensity_mean_CD45\nIntensity_mean_ClCasp3\nIntensity_mean_TotProtein\nCondition\n\n\n\n\n275798\n48\n0.216244\n-0.701630\n-0.405460\n0.116111\n0.121832\n0.211578\n-0.145978\n0.172908\n0.254796\n...\n-0.467107\n-0.493395\n-0.297149\n-0.215123\n-0.440421\n-0.617635\n-0.527239\n-0.649984\n-0.455227\ntrametinib_erlotinib\n\n\n76402\n8\n-0.659898\n-0.400738\n-0.096410\n0.851261\n1.106170\n-0.714665\n-0.114373\n-0.110372\n-0.797019\n...\n0.186040\n-0.091303\n0.014798\n-0.169418\n-0.262193\n0.149671\n0.020990\n0.252864\n-0.274722\ntrametinib_erlotinib\n\n\n293827\n48\n-0.705584\n-0.964579\n-0.284755\n0.914122\n1.214748\n-0.768959\n-1.525292\n-0.083902\n-0.958794\n...\n0.036849\n0.237000\n1.580126\n-0.204042\n0.253853\n-0.124293\n0.216668\n-0.190139\n0.432071\ntrametinib_erlotinib\n\n\n161648\n24\n-0.146193\n-1.630370\n-4.041432\n0.064740\n0.067054\n-0.148619\n-1.819371\n0.051651\n-1.126005\n...\n-0.135134\n-0.727941\n-0.232038\n-0.217327\n-0.482402\n-0.663027\n-0.562821\n-0.698900\n-0.370735\ntrametinib_erlotinib\n\n\n76034\n8\n1.630457\n-2.880856\n-5.862187\n0.610025\n0.733058\n1.417172\n-3.393056\n2.451823\n0.496502\n...\n0.093129\n1.408854\n0.222077\n-0.098458\n1.092290\n0.787201\n2.582427\n1.176022\n1.621623\ntrametinib_erlotinib\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n162359\n24\n-0.376650\n0.919171\n0.350688\n-0.174516\n-0.170722\n-0.393325\n0.512350\n-0.517515\n-0.071613\n...\n-0.420381\n-0.660219\n-0.145264\n-0.217305\n-0.249169\n-0.510716\n-0.518984\n-0.641574\n-0.135489\ntrametinib_erlotinib\n\n\n112665\n24\n-0.049746\n0.319783\n-0.055655\n-0.407746\n-0.378215\n-0.050037\n0.368632\n-0.301800\n0.215695\n...\n-0.299189\n0.532660\n1.038945\n-0.211279\n0.394505\n0.132946\n0.341788\n-0.041571\n0.214764\ntrametinib_erlotinib\n\n\n218702\n48\n0.573604\n-0.277104\n0.187579\n0.252714\n0.274651\n0.542512\n-0.344015\n0.555084\n0.566953\n...\n-0.461702\n-0.729653\n0.546645\n-0.211210\n-0.540501\n-0.525967\n-0.599270\n-0.642962\n-0.371642\ntrametinib_erlotinib\n\n\n178750\n24\n-0.676142\n0.511200\n0.575459\n0.251695\n0.273470\n-0.733885\n-0.044382\n-0.611794\n-0.501223\n...\n2.597780\n-0.302810\n0.683340\n-0.202567\n-0.065888\n0.251196\n-0.325902\n-0.307387\n-0.514849\ntrametinib_erlotinib\n\n\n275962\n48\n0.325888\n-0.509756\n-0.082835\n0.606594\n0.728176\n0.315441\n-0.731544\n0.656564\n0.189633\n...\n-0.476878\n-0.775455\n-0.230436\n-0.215122\n-0.611806\n-0.119238\n-0.643984\n-0.363280\n-0.619941\ntrametinib_erlotinib\n\n\n\n\n1490 rows √ó 52 columns\n\n\n\n\ntrain_df\n\n\n\n\n\n\n\n\ntime\nNuclei_Morphology_Area\nNuclei_Morphology_Circularity\nNuclei_Morphology_Convexity\nNuclei_Morphology_Eccentricity\nNuclei_Morphology_Elongation\nNuclei_Morphology_Equivalent_Diameter\nNuclei_Morphology_Extent\nNuclei_Morphology_Major_Axis_Length\nNuclei_Morphology_Mean_Radius\n...\nIntensity_mean_pAKT\nIntensity_mean_pEGFR\nIntensity_mean_aTUB\nIntensity_mean_pERK\nIntensity_mean_Ki67\nIntensity_mean_pMET\nIntensity_mean_CD45\nIntensity_mean_ClCasp3\nIntensity_mean_TotProtein\nCondition\n\n\n\n\n275633\n48\n0.062944\n-0.658394\n-0.369858\n-0.787760\n-0.673049\n0.062558\n0.097627\n-0.347400\n0.279222\n...\n-0.435127\n-0.538165\n-0.524588\n-0.215130\n-0.346497\n-0.784790\n-0.461487\n-0.673533\n-0.288049\ntrametinib_erlotinib\n\n\n112498\n24\n-1.303553\n-0.741629\n-0.536323\n1.198309\n1.790715\n-1.565404\n1.054265\n-0.517865\n-1.761100\n...\n-0.443802\n-0.301336\n0.523970\n-0.211285\n-0.381492\n0.012531\n-0.441819\n-0.167296\n0.240434\ntrametinib_erlotinib\n\n\n112839\n24\n-1.763452\n0.546494\n-0.288348\n-0.615744\n-0.545707\n-2.356001\n-0.922979\n-2.279599\n-1.988576\n...\n-0.387436\n-0.437237\n0.001319\n-0.211234\n0.096048\n-0.477706\n-0.223582\n-0.472777\n-0.335657\ntrametinib_erlotinib\n\n\n161371\n24\n0.264975\n0.474076\n0.560441\n-0.609166\n-0.540640\n0.258009\n0.445212\n-0.124112\n0.559540\n...\n-0.321071\n0.285090\n0.759071\n-0.217101\n0.360318\n0.352539\n0.096049\n0.076231\n0.364122\ntrametinib_erlotinib\n\n\n275788\n48\n-1.563452\n-0.418477\n-0.485998\n-0.000907\n-0.000937\n-1.983663\n0.343712\n-1.784992\n-1.735494\n...\n0.094690\n0.321196\n-0.551626\n-0.215132\n-0.038196\n-0.504033\n-0.151441\n-0.408550\n0.730899\ntrametinib_erlotinib\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n112556\n24\n-0.101523\n-0.595075\n-1.902382\n0.108508\n0.113640\n-0.102697\n-1.793155\n0.035421\n-0.623868\n...\n-0.333507\n-0.000038\n-0.201443\n-0.211270\n0.457797\n0.135082\n-0.278018\n-0.227338\n1.080551\ntrametinib_erlotinib\n\n\n293964\n48\n0.535025\n-0.524075\n-0.193027\n0.929755\n1.242617\n0.507801\n-0.487692\n1.312271\n0.125912\n...\n-0.430162\n0.159233\n-0.158983\n-0.204060\n-0.524252\n-0.473789\n-0.494141\n-0.550030\n-0.262814\ntrametinib_erlotinib\n\n\n61693\n8\n0.269036\n-0.247288\n-0.044789\n0.606381\n0.727874\n0.261858\n-1.307007\n0.629483\n0.122064\n...\n-0.337067\n-0.181845\n-0.388283\n-0.195236\n-0.112785\n-0.546234\n0.069175\n-0.437667\n-0.165593\ntrametinib_erlotinib\n\n\n61430\n8\n0.035533\n-0.279688\n-0.169410\n0.113996\n0.119551\n0.035417\n0.118866\n0.013880\n0.155712\n...\n-0.086957\n0.094996\n0.883797\n-0.166792\n0.202249\n-0.511002\n0.151831\n-0.330463\n-0.001980\ntrametinib_erlotinib\n\n\n76511\n8\n0.192893\n-0.961609\n-0.676374\n-2.461138\n-1.505388\n0.189173\n0.516995\n-0.542896\n0.265397\n...\n0.104459\n0.943797\n1.759813\n-0.127999\n0.749921\n0.297842\n-0.014309\n-0.001605\n0.614597\ntrametinib_erlotinib\n\n\n\n\n6125 rows √ó 52 columns\n\n\n\n\nX[:2], t[:2]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 X[:2], t[:2]\n\nNameError: name 'X' is not defined\n\n\n\n\nwith np.load('wl-mechanics/assets/test_rna.npz') as data:\n  print(data.files)\n\n['X', 'ts', 'v', 'idx']\n\n\n\nwith np.load('wl-mechanics/assets/train_rna.npz') as data:\n  X = [data['X'][data['ts'] == t] for t in np.unique(data['ts'])]\nfor i in range(len(X)):\n  print(X[i].shape)\n  \nfor i in range(len(X)):\n  print(350/(X[i].shape[0] + 350))\n\n(1831, 100)\n(3613, 100)\n(2728, 100)\n(3115, 100)\n(2782, 100)\n0.16047684548372307\n0.08831693161746151\n0.11371020142949967\n0.10101010101010101\n0.11174968071519796"
  },
  {
    "objectID": "wl-mechanics/notebooks/visualization.html",
    "href": "wl-mechanics/notebooks/visualization.html",
    "title": "M Dims blog",
    "section": "",
    "text": "%pylab inline\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nimport sys\nsys.path.append('wl-mechanics')\nsys.path.append('..')\nimport datasets\nfrom models import utils as mutils\nfrom models import mlp\nimport train_utils as tutils\nfrom flax.training import checkpoints\n\nWARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n\n\n\nfrom configs import toy_ubot as config\nconfig = config.get_config()\nkey = random.PRNGKey(config.seed)\nkey, init_key = random.split(key)\nX, _, _, inv_scaler, _ = datasets.get_data(config, init_key)\n\n2024-03-22 20:30:02.880777: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.4.99). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n\n\n\nplt.scatter(X[0][:,0], X[0][:,1])\nplt.scatter(X[1][:,0], X[1][:,1])\nplt.xticks(range(-2,3), [])\nplt.yticks(range(-2,3), [])\nplt.grid()\n\n\n\n\n\n\n\n\n\n# chkpt_id = 11021717 # ot\n# chkpt_id = 11031078 # sb\n# chkpt_id = 11031137 # phot\n# chkpt_id = 11031225 # ubot\n\n\ncheckpoint_dir = f'wl-mechanics/checkpoint/{chkpt_id}/checkpoints'\n\nkey, *init_key = random.split(key, 3)\nmodel_s, initial_params = mutils.init_model_s(init_key[0], config.model_s)\noptimizer_s = tutils.get_optimizer(config.optimizer_s)\nopt_state_s = optimizer_s.init(initial_params)\ntime_sampler, init_sampler_state = tutils.get_time_sampler(config)\n\nstate_s = mutils.State(step=1, opt_state=opt_state_s,\n                      model_params=initial_params,\n                      ema_rate=config.model_s.ema_rate,\n                      params_ema=initial_params,\n                      sampler_state=init_sampler_state,\n                      key=key, wandbid=np.random.randint(int(1e7),int(1e8)))\nstate_s = checkpoints.restore_checkpoint(checkpoint_dir, state_s, prefix='chkpt_s_')\n\n\n@jax.jit\ndef grad_vf(t,y,state):\n  s = mutils.get_model_fn(model_s, \n                          state.params_ema if config.eval.use_ema else state.model_params, \n                          train=False)\n  dsdx = jax.grad(lambda _t, _x: s(_t*jnp.ones((_x.shape[0],1)), _x).sum(), argnums=1)\n  return dsdx(t,y)\n\n@jax.jit\ndef grad_vf_weighted(t,y,state):\n      y, w = y\n      s = mutils.get_model_fn(model_s, \n                              state.params_ema if config.eval.use_ema else state.model_params, \n                              train=False)\n      dsdx = jax.grad(lambda _t, _x: s(_t, _x).sum(), argnums=1)\n      return (dsdx(t*jnp.ones((y.shape[0],1)),y), s(t*jnp.ones((y.shape[0],1)), y))\n\n\ndt = 1e-2\nn = int(1/dt)\nsolution = np.zeros((X[0].shape[0],n+1,2))\nsolution[:,0,:] = X[0]\nt = np.zeros((n+1,1))\nkey, loop_key = random.split(key)\nfor i in range(n):\n  solution[:,i+1,:] = solution[:,i,:] + dt*grad_vf(t[i], solution[:,i,:], state_s)\n  t[i+1] = t[i] + dt\n\n\ndt = 1e-2\nn = int(1/dt)\nN = 200\nsolution = np.zeros((N,n+1,2))\nsolution[:,0,:] = X[0][:N]\nlogw = np.zeros((N,n+1,1))\nt = np.zeros((n+1,1))\nkey, loop_key = random.split(key)\nfor i in range(100):\n  eps = random.normal(random.fold_in(loop_key, i), shape=(N, 2))\n  dxdt, dlogwdt = grad_vf_weighted(t[i], (solution[:,i,:], logw[:,i,:]), state_s)\n  solution[:,i+1,:] = solution[:,i,:] + dt*dxdt\n  logw[:,i+1,:] = logw[:,i,:] + dt*dlogwdt\n  t[i+1] = t[i] + dt\nw = jnp.exp(logw[:,:,0] - jax.scipy.special.logsumexp(logw[:,:,0], axis=0, keepdims=True))\n\n\ndt = 1e-2\nn = int(1/dt)\nsolution = np.zeros((X[0].shape[0],n+1,2))\nsolution[:,0,:] = X[0]\nt = np.zeros((n+1,1))\nkey, loop_key = random.split(key)\nfor i in range(n):\n  eps = random.normal(random.fold_in(loop_key, i), shape=(X[0].shape[0], 2))\n  solution[:,i+1,:] = solution[:,i,:] + dt*grad_vf(t[i], solution[:,i,:], state_s) + jnp.sqrt(dt)*config.sigma*eps\n  t[i+1] = t[i] + dt\n\n\nfrom matplotlib.collections import LineCollection\n\nn = N\nw_normed = w/w.max()\nfigsize(6,6)\nplt.scatter(solution[:n,0,0], solution[:n,0,1], alpha=1.0)\nplt.scatter(solution[:n,-1,0], solution[:n,-1,1], alpha=w_normed[:,-1])\n# for i in range(n):\n#   points = solution[i,::4,:].reshape(-1,1,2)\n#   segments = np.concatenate([points[:-1], points[1:]], axis=1)\n#   lc = LineCollection(segments, cmap=plt.get_cmap('Blues'), norm=plt.Normalize(0, 0.6), alpha=0.3)\n#   lc.set_array(w_normed[i,::4])\n#   plt.gca().add_collection(lc)\nplt.xticks(range(-2,3), [])\nplt.yticks(range(-2,3), [])\nplt.grid()\n# plt.savefig('wl-mechanics/assets/ubot.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nw_normed.shape\n\n(200, 101)\n\n\n\nn = 100\nw_normed = w/w.max()\nfigsize(6,6)\nplt.scatter(solution[:n,0,0], solution[:n,0,1], alpha=w_normed[:,0])\nplt.scatter(solution[:n,-1,0], solution[:n,-1,1], alpha=w_normed[:,-1])\nplt.scatter(solution[:n,50,0], solution[:n,50,1], alpha=w_normed[:,50])\nplt.xticks([], [])\nplt.yticks([], [])\nplt.xlim(-2,2)\nplt.ylim(-2,2)\nplt.grid()\n\n\n\n\n\n\n\n\n\nn = 200\nfigsize(6,6)\nplt.scatter(solution[:n,0,0], solution[:n,0,1])\nplt.scatter(solution[:n,-1,0], solution[:n,-1,1])\nfor i in range(n):\n  plt.plot(solution[i,:,0], solution[i,:,1], c='blue', alpha=0.2)\n# plt.xticks(range(-2,3), [])\n# plt.yticks(range(-2,3), [])\nplt.xlim(-2,2)\nplt.ylim(-2,2)\nplt.grid()\n# plt.savefig('wl-mechanics/assets/phot.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nsolution.shape\n\n(2000, 101, 2)\n\n\n\n!mkdir wl-mechanics/assets/gifs/ubot\n\n\ndef generate_frames(solution, dir, n=200):\n  for frame_id in range(solution.shape[1]):\n    figsize(6,6)\n    plt.scatter(solution[:n,0,0], solution[:n,0,1])\n    plt.scatter(solution[:n,-1,0], solution[:n,-1,1])\n    for i in range(n):\n      plt.plot(solution[i,:frame_id,0], solution[i,:frame_id,1], c='blue', alpha=0.2)\n    plt.xlim(-2,2)\n    plt.ylim(-2,2)\n    plt.xticks([], [])\n    plt.yticks([], [])\n    # plt.grid()\n    plt.box(False)\n    plt.savefig(f'{dir}/frame_{frame_id:04}.png', bbox_inches='tight')\n    plt.clf()\n    \ndef generate_frames(solution, dir):\n  for frame_id in range(solution.shape[1]):\n    figsize(6,6)\n    plt.scatter(solution[:,frame_id,0], solution[:,frame_id,1], alpha=w_normed[:,frame_id])\n    plt.scatter(solution[:,-1,0], solution[:,-1,1], alpha=w_normed[:,-1])\n    plt.xlim(-2,2)\n    plt.ylim(-2,2)\n    plt.xticks([], [])\n    plt.yticks([], [])\n    # plt.grid()\n    plt.box(False)\n    plt.savefig(f'{dir}/frame_{frame_id:04}.png', bbox_inches='tight')\n    plt.clf()\n\n\ngenerate_frames(solution, 'wl-mechanics/assets/gifs/ubot')\n\n&lt;Figure size 600x600 with 0 Axes&gt;\n\n\n\n!convert -delay 5 -loop 0 -dispose previous wl-mechanics/assets/gifs/ubot/*.png wl-mechanics/assets/gifs/ubot.gif\n\n\nfrom IPython.display import Image\n\n\nImage(open('wl-mechanics/assets/gifs/ubot.gif','rb').read())"
  },
  {
    "objectID": "wl-mechanics/notebooks/amot.html",
    "href": "wl-mechanics/notebooks/amot.html",
    "title": "M Dims blog",
    "section": "",
    "text": "%pylab inline\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\nimport jax\n\nimport jax.numpy as jnp\nfrom jax import grad, jit, vmap\nfrom jax import random\n\ntry:\n  import flax\nexcept ModuleNotFoundError:\n  !pip install --quiet flax\n  import flax\n\ntry:\n  import optax\nexcept ModuleNotFoundError:\n  !pip install --quiet optax\n  import optax\n\ntry:\n  import diffrax\nexcept ModuleNotFoundError:\n  !pip install --quiet diffrax\n  import diffrax\n\nfrom flax import linen as nn\nfrom flax.training import train_state\n\nfrom tqdm import trange\nfrom functools import partial\n\n/ssd003/home/kirill/venvs/jax-env/lib/python3.9/site-packages/flax/core/frozen_dict.py:169: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n  jax.tree_util.register_keypaths(\n\n\n\nfrom typing import NamedTuple, Any\n\nclass TimedX(NamedTuple):\n  t: Any\n  x: jnp.ndarray\n\ndef gaussians(jseed, t):\n  jseeds = random.split(jseed, 3)\n  x_1 = random.randint(jseeds[0], minval=0, maxval=2, shape=(t.shape[0], 2))\n  x_1 = x_1.astype(float)-0.5\n  x_1 += 1e-1*random.normal(jseeds[1], shape=(t.shape[0],2))\n  x_0 = 1e-1*random.normal(jseeds[2], shape=(t.shape[0],2))\n  x_t = jnp.sqrt(1-t)*x_0 + jnp.sqrt(t)*x_1\n  return x_0, x_1, x_t\n\ndef diamonds(jseed, t):\n  jseeds = random.split(jseed, 4)\n  x_1 = random.randint(jseeds[0], minval=0, maxval=2, shape=(t.shape[0], 2))\n  x_1 = x_1.astype(float)-0.5\n  x_1 = x_1.at[:,0].set(0.5)\n\n  x_0 = random.randint(jseeds[1], minval=0, maxval=2, shape=(t.shape[0], 2))\n  x_0 = x_0.astype(float)-0.5\n  x_0 = x_0.at[:,0].set(-0.5)\n\n  R = jnp.array([[1/math.sqrt(2),-1/math.sqrt(2)],\n                 [1/math.sqrt(2),1/math.sqrt(2)]])\n  x_1 += 5e-1*(random.uniform(jseeds[2], shape=(t.shape[0],2))-0.5)@R\n  x_0 += 5e-1*(random.uniform(jseeds[3], shape=(t.shape[0],2))-0.5)@R\n  x_t = (1-t)*x_0 + t*x_1\n  \n  return x_0, x_1, x_t\n\n# DEFINE THE DYNAMICS TO USE HERE\nq_t = diamonds\n\n\nseed = 0\nnp.random.seed(seed)\nkey = random.PRNGKey(seed)\nDS = 2048\nkey, loc_key = random.split(key)\nDATA_0, DATA_1, _ = q_t(loc_key, jnp.ones([DS,1]))\n\n\nfigsize(23,7)\nplt.scatter(DATA_0[:,0], DATA_0[:,1], alpha=0.7)\nplt.scatter(DATA_1[:,0], DATA_1[:,1], alpha=0.7)\nplt.xlim(-1.5,1.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nclass Smodel(nn.Module):\n  num_hid : int\n  num_out : int\n\n  @nn.compact\n  def __call__(self, t, x):\n    h = jnp.hstack([t,x])\n    h = nn.Dense(self.num_hid)(h)\n    h = nn.relu(h)\n    h = nn.Dense(self.num_hid)(h)\n    h = nn.swish(h)\n    h = nn.Dense(self.num_hid)(h)\n    h = nn.swish(h)\n    h = nn.Dense(self.num_out)(h)\n    return h\n\nclass Qmodel(nn.Module):\n  num_hid : int\n  num_out : int\n\n  @nn.compact\n  def __call__(self, t, x_0, x_1, u):\n    h = jnp.hstack([t, x_0, x_1, t &lt; 0.5, u])\n    # h = jnp.hstack([t, x_0, x_1]) jnp.linspace(0.0, 1.0, 10).reshape((1,-1))\n    h = nn.Dense(self.num_hid)(h)\n    h = nn.relu(h)\n    h = nn.Dense(self.num_hid)(h)\n    h = nn.swish(h)\n    h = nn.Dense(self.num_hid)(h)\n    h = nn.swish(h)\n    h = nn.Dense(self.num_out)(h)\n    out = (1-t)*x_0 + t*x_1 + jnp.sqrt(2*t*(1-t))*h\n    return out\n\n\ndef sample_t(u0, n, t0=0.0, t1=1.0):\n  u = (u0 + math.sqrt(2)*jnp.arange(n + 1)) % 1\n  u = u.reshape([-1,1])\n  return u[:-1]*(t1-t0) + t0, u[-1]\n\n\ns = Smodel(num_hid=512, num_out=1)\nq = Qmodel(num_hid=128, num_out=2)\n\nBS = 512\n\nkey, *init_keys = random.split(key, 3)\nstate_s = train_state.TrainState.create(apply_fn=s.apply,\n                                        params=s.init(init_keys[0], np.ones([BS,1]), DATA_0[:BS]),\n                                        tx=optax.adam(learning_rate=2e-4))\n\nstate_q = train_state.TrainState.create(apply_fn=q.apply,\n                                        params=q.init(init_keys[1], np.ones([BS,1]), DATA_0[:BS], DATA_1[:BS], np.ones([BS,10])),\n                                        tx=optax.adam(learning_rate=2e-4))\n\n# derivatives of the model that we need to define the loss\ndef V(t, x, p):\n  dsdtdx_fn = jax.grad(lambda t, x, p: state_s.apply_fn(p,t,x).sum(), argnums=[0,1])\n  dsdt, dsdx = dsdtdx_fn(t, x, p)\n  return dsdt + 0.5*(dsdx**2).sum(1, keepdims=True)\ndVdx_fn = jax.grad(lambda t, x, p: V(t,x,p).sum(), argnums=1)\n# and to apply the model\ndsdx_fn = jax.jit(jax.grad(lambda t, x, p: state_s.apply_fn(p,t,x).sum(), argnums=1))\n\n\ndef loss_fn(state_s, state_q, params_s, params_q, u0, key):\n  keys = random.split(key, 3)\n  t_0, t_1 = jnp.zeros([BS, 1]), jnp.ones([BS, 1])\n  t, u0 = sample_t(u0, BS)\n  x_0, x_1 = random.choice(keys[0], DATA_0, (BS,)), random.choice(keys[1], DATA_1, (BS,))\n  u = random.uniform(keys[2], shape=(BS,10))\n  x_t = state_q.apply_fn(params_q, t, x_0, x_1, u)\n  update = jax.lax.stop_gradient(dVdx_fn(t, x_t, params_s))\n  x_t += 1e-1*t*(1-t)*update\n  x_t = jax.lax.stop_gradient(x_t)\n\n  loss = state_s.apply_fn(params_s, t_0, x_0) - state_s.apply_fn(params_s, t_1, x_1)\n  loss += V(t, x_t, params_s)\n  loss -= V(t, state_q.apply_fn(params_q, t, x_0, x_1, u), jax.lax.stop_gradient(params_s))\n  return loss.mean(), (jnp.sqrt((update**2).sum(1)).mean(), u0)\n\n@jax.jit\ndef train_step(state_s, state_q, u0, key):\n  grad_fn = jax.value_and_grad(loss_fn, argnums=[2,3], has_aux=True)\n  (loss, (gradV, u0)), grads = grad_fn(state_s, state_q, state_s.params, state_q.params, u0, key)\n  state_s = state_s.apply_gradients(grads=grads[0])\n  state_q = state_q.apply_gradients(grads=grads[1])\n  return state_s, state_q, loss, gradV, u0\n\nkey, loc_key = random.split(key)\n_, _, _, _, _ = train_step(state_s, state_q, 0.5, loc_key)\n\n\nnum_iterations = 10_000\nu0 = 0.5\n\nloss_plot = np.zeros(num_iterations)\ngradV_plot = np.zeros(num_iterations)\nfor iter in trange(num_iterations):\n  key, loc_key = random.split(key)\n  state_s, state_q, loss, gradV, u0 = train_step(state_s, state_q, u0, loc_key)\n  loss_plot[iter] = loss\n  gradV_plot[iter] = gradV\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:48&lt;00:00, 204.82it/s]\n\n\n\nfigsize(12,4)\nplt.subplot(121)\nplt.plot(loss_plot)\nplt.grid()\nplt.subplot(122)\nplt.plot(gradV_plot)\n# plt.ylim(0.0, 5.0)\nplt.grid()\n\n\n\n\n\n\n\n\n\ndt = 1e-2\nt = 0.0\nn = int(1/dt)\nx = np.zeros((DS,n+1,2))\nx[:,0,:] = DATA_0\nx_t = np.zeros((DS,n+1,2))\nx_t[:,0,:] = DATA_0\nt = np.zeros((DS,n+1,1))\nkey, iter_key = random.split(key)\nfor i in trange(n):\n  x[:,i+1,:] = x[:,i,:] + dt*dsdx_fn(t[:,i,:], x[:,i,:], state_s.params)\n  t[:,i+1,:] = t[:,i,:] + dt\n  x_t[:,i+1,:] = state_q.apply_fn(state_q.params, t[:,i,:], DATA_0, DATA_1, random.uniform(random.fold_in(iter_key, i), shape=(DS,10)))\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04&lt;00:00, 23.60it/s]\n\n\n\nfigsize(23,7)\nplt.scatter(DATA_0[:,0], DATA_0[:,1], alpha=0.7)\nplt.scatter(x[:,-1,0], x[:,-1,1], alpha=0.7)\nfor i in range(BS):\n  plt.plot(x[i,:,0], x[i,:,1], c='b', alpha=0.3)\nplt.xlim(-1.5,1.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfigsize(23,7)\nplt.scatter(DATA_0[:,0], DATA_0[:,1], alpha=0.7)\nplt.scatter(x[:,-1,0], x[:,-1,1], alpha=0.7)\nfor i in range(BS):\n  plt.scatter(x_t[i,:,0], x_t[i,:,1], c='b', alpha=0.3)\nplt.xlim(-1.5,1.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfigsize(23,7)\nplt.scatter(DATA_0[:,0], DATA_0[:,1], alpha=0.7)\nplt.scatter(x[:,-1,0], x[:,-1,1], alpha=0.7)\nfor i in range(BS):\n  plt.plot(x_t[i,:,0], x_t[i,:,1], c='b', alpha=0.3)\nplt.xlim(-1.5,1.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfigsize(12,4)\nplt.subplot(121)\nplt.plot(loss_plot)\nplt.grid()\nplt.subplot(122)\nplt.plot(gradV_plot)\n# plt.ylim(0.0, 5.0)\nplt.grid()\n\n\n\n\n\n\n\n\n\nfigsize(23,7)\nplt.scatter(DATA_0[:,0], DATA_0[:,1], alpha=0.7)\nplt.scatter(x[:,-1,0], x[:,-1,1], alpha=0.7)\nfor i in range(BS):\n  plt.plot(x[i,:,0], x[i,:,1], c='b', alpha=0.3)\nplt.xlim(-1.5,1.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfigsize(23,7)\nplt.scatter(DATA_0[:,0], DATA_0[:,1], alpha=0.7)\nplt.scatter(x[:,-1,0], x[:,-1,1], alpha=0.7)\nfor i in range(BS):\n  plt.scatter(x_t[i,:,0], x_t[i,:,1], c='b', alpha=0.3)\nplt.xlim(-1.5,1.5)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nfigsize(23,7)\nplt.scatter(DATA_0[:,0], DATA_0[:,1], alpha=0.7)\nplt.scatter(x[:,-1,0], x[:,-1,1], alpha=0.7)\nfor i in range(BS):\n  plt.plot(x_t[i,:,0], x_t[i,:,1], c='b', alpha=0.3)\nplt.xlim(-1.5,1.5)\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/LICENSE.html",
    "href": "wl-mechanics/src/geomstats/LICENSE.html",
    "title": "M Dims blog",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 Nina Miolane\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/05_riemannian_kmeans.html",
    "href": "wl-mechanics/src/geomstats/notebooks/05_riemannian_kmeans.html",
    "title": "Tutorial: K-Means clustering on a Riemannian Manifold",
    "section": "",
    "text": "In this notebook, we demonstrate how run a K-mean clustering algorithm on a Riemannian Manifold. Generate data on a sphere from a Von Mises Fisher distribution and apply a rotation sampled randomly from SO3."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/05_riemannian_kmeans.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/05_riemannian_kmeans.html#setup",
    "title": "Tutorial: K-Means clustering on a Riemannian Manifold",
    "section": "Setup",
    "text": "Setup\n\nimport os\nimport subprocess\n\ngeomstats_gitroot_path = subprocess.check_output(\n    ['git', 'rev-parse', '--show-toplevel'], \n    universal_newlines=True)\n\nos.chdir(geomstats_gitroot_path[:-1])\n\nprint('Working directory: ', os.getcwd())\n\nWorking directory:  /Users/nicolasguigui/gits/geomstats\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport geomstats.backend as gs\nimport geomstats.visualization as visualization\nfrom geomstats.geometry.hypersphere import Hypersphere\nfrom geomstats.geometry.special_orthogonal import SpecialOrthogonal\nfrom geomstats.learning.frechet_mean import FrechetMean\nfrom geomstats.learning.pca import TangentPCA\n\nnp.random.seed(1)\ngs.random.seed(1000)"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/05_riemannian_kmeans.html#data-init-generate-clusters-randomly-on-the-sphere",
    "href": "wl-mechanics/src/geomstats/notebooks/05_riemannian_kmeans.html#data-init-generate-clusters-randomly-on-the-sphere",
    "title": "Tutorial: K-Means clustering on a Riemannian Manifold",
    "section": "Data init: Generate clusters randomly on the sphere",
    "text": "Data init: Generate clusters randomly on the sphere\n\nsphere = Hypersphere(dim=2)\ncluster = sphere.random_von_mises_fisher(kappa=20, n_samples=140)\n\nSO3 = SpecialOrthogonal(3)\nrotation1 = SO3.random_uniform()\nrotation2 = SO3.random_uniform()\n\ncluster_1 =  cluster @ rotation1\ncluster_2 =  cluster @ rotation2\n\nfig = plt.figure(figsize=(15, 15))\nax = visualization.plot(cluster_1, space='S2', color='red', alpha=0.7, label='Data points 1 ')\nax = visualization.plot(cluster_2, space='S2', ax = ax, color='blue', alpha=0.7, label='Data points 2')\nax.auto_scale_xyz([-1, 1], [-1, 1], [-1, 1])\nax.legend();"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/05_riemannian_kmeans.html#operate-riemannian-k-means-clustering",
    "href": "wl-mechanics/src/geomstats/notebooks/05_riemannian_kmeans.html#operate-riemannian-k-means-clustering",
    "title": "Tutorial: K-Means clustering on a Riemannian Manifold",
    "section": "Operate Riemannian K-Means clustering",
    "text": "Operate Riemannian K-Means clustering\n\nfrom geomstats.learning.kmeans import RiemannianKMeans\nfrom geomstats.geometry.hypersphere import Hypersphere\n\nmanifold = Hypersphere(dim=2)\nmetric = manifold.metric\n\ndata = gs.concatenate((cluster_1, cluster_2), axis=0)\n\nkmeans = RiemannianKMeans(metric, 2, tol=1e-3, lr=1.)\nkmeans.fit(data)\nlabels = kmeans.predict(data)\ncentroids = kmeans.centroids\n\nPlot the results:\n\nfig = plt.figure(figsize=(15, 15))\ncolors = ['red', 'blue']\n\nax = visualization.plot(\n    data,\n    space='S2',\n    marker='.',\n    color='grey')\n\nfor i in range(2):\n    ax = visualization.plot(\n        points=data[labels == i],\n        ax=ax,\n        space='S2',\n        marker='.',\n        color=colors[i])\n\nfor i, c in enumerate(centroids):\n    ax = visualization.plot(\n        c,\n        ax=ax,\n        space='S2',\n        marker='*',\n        s=2000,\n        color=colors[i])\n\nax.set_title('Kmeans on Hypersphere Manifold');\nax.auto_scale_xyz([-1, 1], [-1, 1], [-1, 1])"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_hand_poses_analysis_in_kendall_shape_space.html",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_hand_poses_analysis_in_kendall_shape_space.html",
    "title": "Tutorial: Classifying hands poses with Kendall shape spaces",
    "section": "",
    "text": "In this tutorial, we show how to use geomstats to perform a shape data analysis. Specifically, we aim to study the difference between two groups of data: - hand poses that correspond to the action ‚ÄúGrab‚Äù, - hand poses heads that correspond to the action ‚ÄúExpand‚Äù.\nWe wish to investigate if there is a difference in these two groups.\nThe hand poses are represented as the coordinates of 22 joints in 3D:"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_hand_poses_analysis_in_kendall_shape_space.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_hand_poses_analysis_in_kendall_shape_space.html#setup",
    "title": "Tutorial: Classifying hands poses with Kendall shape spaces",
    "section": "Setup",
    "text": "Setup\n\nimport os\nimport sys\nimport warnings\n\nsys.path.append(os.path.dirname(os.getcwd()))\nwarnings.filterwarnings('ignore')\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport geomstats.visualization as visualization\nimport geomstats.backend as gs\nimport geomstats.datasets.utils as data_utils\nfrom geomstats.geometry.pre_shape import PreShapeSpace, KendallShapeMetric\n\nvisualization.tutorial_matplotlib()\n\nINFO: Using numpy backend\n\n\n\nHands shapes\nLoad the dataset of hand poses, where a hand is represented as a set of 22 landmarks - the hands joints - in 3D.\nThe hand poses represent two different hand poses: - Label 0: hand is in the position ‚ÄúGrab‚Äù - Label 1: hand is in the position ‚ÄúExpand‚Äù\nThis is a subset of the SHREC 2017 dataset [SWVGLF2017].\nWe load the dataset of landmarks‚Äô sets and corresponding labels.\n\nhands, labels, bone_list = data_utils.load_hands()\n\n\nprint(hands.shape)\nprint(labels)\n\n(52, 22, 3)\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n\n\nWe extract two hands, one corresponding to the ‚ÄúGrab‚Äù pose, and the other to the ‚ÄúExpand‚Äù pose.\n\nlabel_to_str = {0: 'Grab', 1: 'Expand'}\nlabel_to_color = {0: (102/255, 178/255, 255/255, 1.), 1: (255/255, 178/255, 102/255, 1.)}\nfirst_grab_hand = hands[labels==0][0]\nfirst_expand_hand = hands[labels==1][0]\n\nWe implement a function to plot one hand in 3D.\n\ndef plot_hand(hand, bone_list):\n    fig = plt.figure()\n    ax = plt.axes(projection=\"3d\")\n\n    x = hand[:, 0]\n    y = hand[:, 1]\n    z = hand[:, 2]\n\n    sc = ax.scatter(x, y, z, s=40)\n    for bone in bone_list:\n        start_bone_idx = bone[0]\n        end_bone_idx = bone[1]\n        ax.plot(\n            xs=[x[start_bone_idx], x[end_bone_idx]],\n            ys=[y[start_bone_idx], y[end_bone_idx]],\n            zs=[z[start_bone_idx], z[end_bone_idx]],\n        )\n\nWe plot two examples of hands.\n\n%matplotlib notebook\n\nplot_hand(first_grab_hand, bone_list)\nplt.title(f\"Hand: {label_to_str[0]}\");\n\n\n\n\n\n\n\n\nplot_hand(first_expand_hand, bone_list)\nplt.title(f\"Hand: {label_to_str[1]}\");\n\n\n\n\n\n\n\nWe want to investigate if there is a difference between these two groups of shapes - grab versus expand - or if the main difference is merely relative to the global size of the landmarks‚Äô sets.\n\nm_ambient = 3\nk_landmarks = 22\n\npreshape = PreShapeSpace(m_ambient=m_ambient, k_landmarks=k_landmarks)\nmatrices_metric = preshape.embedding_metric\n\nsizes = matrices_metric.norm(preshape.center(hands))\n\nplt.figure(figsize=(6, 4))\nfor label, col in label_to_color.items():\n    label_sizes = sizes[labels==label]\n    plt.hist(label_sizes, color=col, label=label_to_str[label], alpha=0.5, bins=10)\n    plt.axvline(gs.mean(label_sizes),  color=col)\nplt.legend(fontsize=14)\nplt.title('Hands sizes', fontsize=14);\n\n\n\n\n\n\n\nWe perform a hypothesis test, testing if the two samples of sizes have the same average.\n\nfrom scipy import stats\n\nsignif_level = 0.05\n\ntstat, pvalue = stats.ttest_ind(sizes[labels==0], sizes[labels==1])\nprint(pvalue &lt; signif_level)\n\nTrue\n\n\nThe size could be a characteristic allowing to distinguish between these two specific shapes.\nWe want to investigate if there is a difference in shapes, where the size component has been quotiented out.\nWe project the data to the Kendall pre-shape space, which: - centers the hand landmark sets so that they share the same barycenter, - normalizes the sizes of the landmarks‚Äô sets to 1.\nDirectly looking at the hands by looking at the coordinates of their landmarks in 3D does not show clear clusters\n\nfrom geomstats.geometry.euclidean import EuclideanMetric\n\neucl_metric = EuclideanMetric(3*22)\nhands_vec = hands.reshape(52, -1)\neucl_pair_dist = eucl_metric.dist_pairwise(hands_vec)\n\n\nplt.figure(figsize=(4, 4))\nplt.imshow(eucl_pair_dist);\n\n\n\n\n\n\n\nWe project the hands in a Kendall shape space.\n\nhands_preshape = preshape.projection(hands)\nprint(hands_preshape.shape)\nprint(preshape.belongs(hands_preshape))\nprint(gs.isclose(matrices_metric.norm(hands_preshape), 1.))\n\n(52, 22, 3)\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True]\n\n\nIn order to quotient out the 3D orientation component, we align the landmark sets in the preshape space.\n\nbase_point = hands_preshape[0]\n\nhands_shape = preshape.align(point=hands_preshape, base_point=base_point)\nprint(hands_shape.shape)\n\n(52, 22, 3)\n\n\nThe Kendall metric is a Riemannian metric that takes this alignment into account. It corresponds to the metric of the Kendall shape space, which is the manifold defined as the preshape space quotient by the action of the rotation in m_ambient dimensions, here in 3 dimensions.\n\nkendall_metric = KendallShapeMetric(m_ambient=m_ambient, k_landmarks=k_landmarks)\n\nWe can use it to perform a tangent PCA in the Kendall shape space, and determine if we see a difference in the hand shapes.\n\nfrom geomstats.learning.pca import TangentPCA\n\ntpca = TangentPCA(kendall_metric)\ntpca.fit(hands_shape)\n\nplt.figure()\nplt.plot(\n    tpca.explained_variance_ratio_)\nplt.xlabel(\"Number of principal tangent components\", size=14)\nplt.ylabel(\"Fraction of explained variance\", size=14);\n\n\n\n\n\n\n\nThe first 2 principal components capture around 60% of the variance.\n\n%matplotlib inline\nX = tpca.transform(hands_shape)\n\nplt.figure(figsize=(6, 6))\n\nfor label, col in label_to_color.items():\n    mask = labels == label\n    plt.scatter(X[mask, 0], X[mask, 1], color=col, s=100, label=label_to_str[label]);\nplt.legend(fontsize=14)\nplt.title(\n    'Projection on the 2 first principal components'\n    '\\nof tangent PCA in Kendall shape space');\n\n\n\n\n\n\n\n\n\ndist_pairwise = kendall_metric.dist_pairwise(hands_shape)\nprint(dist_pairwise.shape)\n\n(52, 52)\n\n\n\nplt.figure(figsize=(4, 4))\nplt.imshow(dist_pairwise);\n\n\n\n\n\n\n\n\nThis distance matrix can now be used to perform clustering on the hands shapes."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_hand_poses_analysis_in_kendall_shape_space.html#references",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_hand_poses_analysis_in_kendall_shape_space.html#references",
    "title": "Tutorial: Classifying hands poses with Kendall shape spaces",
    "section": "References",
    "text": "References\n.. [SWVGLF2017] Q. De Smedt, H. Wannous, J.P. Vandeborre, J. Guerry, B. Le Saux, D. Filliat, SHREC‚Äô17 Track: 3D Hand Gesture Recognition Using a Depth and Skeletal Dataset, 10th Eurographics Workshop on 3D Object Retrieval, 2017. https://doi.org/10.2312/3dor.20171049"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/01_data_on_manifolds.html",
    "href": "wl-mechanics/src/geomstats/notebooks/01_data_on_manifolds.html",
    "title": "Tutorial: Data on Manifolds",
    "section": "",
    "text": "import os\nimport sys\nimport warnings\n\nsys.path.append(os.path.dirname(os.getcwd()))\nwarnings.filterwarnings('ignore')\n\nWe import the backend that will be used for geomstats computations and set a seed for reproducibility of the results.\n\nimport geomstats.backend as gs\n\ngs.random.seed(2020)\n\nINFO: Using numpy backend\n\n\nFinally, we import the visualization module.\n\nimport matplotlib\nimport matplotlib.colors as colors\nimport matplotlib.image as mpimg\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n\nimport geomstats.visualization as visualization\n\nvisualization.tutorial_matplotlib()"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/01_data_on_manifolds.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/01_data_on_manifolds.html#setup",
    "title": "Tutorial: Data on Manifolds",
    "section": "",
    "text": "import os\nimport sys\nimport warnings\n\nsys.path.append(os.path.dirname(os.getcwd()))\nwarnings.filterwarnings('ignore')\n\nWe import the backend that will be used for geomstats computations and set a seed for reproducibility of the results.\n\nimport geomstats.backend as gs\n\ngs.random.seed(2020)\n\nINFO: Using numpy backend\n\n\nFinally, we import the visualization module.\n\nimport matplotlib\nimport matplotlib.colors as colors\nimport matplotlib.image as mpimg\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n\nimport geomstats.visualization as visualization\n\nvisualization.tutorial_matplotlib()"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/01_data_on_manifolds.html#from-data-on-linear-spaces-to-data-on-manifolds",
    "href": "wl-mechanics/src/geomstats/notebooks/01_data_on_manifolds.html#from-data-on-linear-spaces-to-data-on-manifolds",
    "title": "Tutorial: Data on Manifolds",
    "section": "From data on linear spaces to data on manifolds",
    "text": "From data on linear spaces to data on manifolds\nThe science of Statistics is defined as the collection of data, their analysis and interpretation. Statistical theory is usually defined for data belonging to vector spaces, which are linear spaces. For example, we know how to compute the mean of a data set of numbers, like the mean of students‚Äô weights in a classroom, or of multidimensional arrays, like the average 3D velocity vector of blood cells in a vessel.\nHere is an example of the computation of the mean of two arrays of dimension 2.\n\nfrom geomstats.geometry.euclidean import Euclidean\n\ndim = 2\nn_samples = 2\n\neuclidean = Euclidean(dim=dim)\npoints_in_linear_space = euclidean.random_point(n_samples=n_samples)\nprint('Points in linear space:\\n', points_in_linear_space)\n\nlinear_mean = gs.sum(points_in_linear_space, axis=0) / n_samples\nprint('Mean of points:\\n', linear_mean)\n\nPoints in linear space:\n [[ 0.97255366  0.74678389]\n [ 0.01949105 -0.45632857]]\nMean of points:\n [0.49602235 0.14522766]\n\n\nWe plot the points and their mean on the 2D Euclidean space, which is a linear space: a plane.\n\n%matplotlib inline\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111)\n\nax.scatter(points_in_linear_space[:, 0], points_in_linear_space[:, 1], label='Points')\nax.plot(points_in_linear_space[:, 0], points_in_linear_space[:, 1], linestyle='dashed')\n\nax.scatter(\n    gs.to_numpy(linear_mean[0]), \n    gs.to_numpy(linear_mean[1]), label='Mean', s=80, alpha=0.5)\n\nax.set_title('Mean of points in a linear space')\nax.legend();\n\n\n\n\n\n\n\n\nWhat happens to the usual statistical theory when the data doesn‚Äôt naturally belong to a linear space. For example, if we want to perform statistics on the coordinates of world cities, which lie on the earth: a sphere?\nThe non-linear spaces we consider are called manifolds. A manifold \\(M\\) of dimension \\(m\\) is a space that is allowed to be curved but that looks like an \\(m\\)-dimensional vector space in the neighborhood of every point.\nA sphere, like the earth, is a good example of a manifold. We know that the earth is curved, but at our scale we do not see its curvature. Can we still use linear statistics when data are defined on these manifolds, or shall we?\nLet‚Äôs try.\n\nfrom geomstats.geometry.hypersphere import Hypersphere\n\nsphere = Hypersphere(dim=dim)\npoints_in_manifold = sphere.random_uniform(n_samples=n_samples)\nprint('Points in manifold:\\n', points_in_manifold)\n\nlinear_mean = gs.sum(points_in_manifold, axis=0) / n_samples\nprint('Mean of points:\\n', linear_mean)\n\nPoints in manifold:\n [[-0.71502435 -0.41197257 -0.56481748]\n [-0.997575   -0.04788171  0.05051201]]\nMean of points:\n [-0.85629967 -0.22992714 -0.25715273]\n\n\nWe plot the points and their mean computed with the linear formula.\n\n%matplotlib inline\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\n\nvisualization.plot(\n    points_in_manifold, ax=ax, space='S2', label='Point', s=80)\n\nax.plot(\n    points_in_manifold[:, 0], \n    points_in_manifold[:, 1], \n    points_in_manifold[:, 2], \n    linestyle='dashed', alpha=0.5)\n\nax.scatter(\n    linear_mean[0], linear_mean[1], linear_mean[2], \n    label='Mean', s=80, alpha=0.5)\n\nax.set_title('Mean of points on a manifold')\nax.legend();\n\n\n\n\n\n\n\n\nWhat happened? The mean of two points on a manifold (the sphere) is not on the manifold. In our example, the mean city is not on the earth. This leads to errors in statistical computations.\n\nprint(sphere.belongs(linear_mean))\n\nFalse\n\n\nFor this reason, researchers aim to build a theory of statistics that is by construction compatible with any structure we equip the manifold with. This theory is called Geometric Statistics.\nGeometric Statistics is a theory of statistics on manifolds, that takes into account their geometric structures. Geometric Statistics is therefore the child of two major pillars of Mathematics: Geometry and Statistics."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/01_data_on_manifolds.html#examples-of-data-on-manifolds",
    "href": "wl-mechanics/src/geomstats/notebooks/01_data_on_manifolds.html#examples-of-data-on-manifolds",
    "title": "Tutorial: Data on Manifolds",
    "section": "Examples of data on manifolds",
    "text": "Examples of data on manifolds\nWhy should we bother to build a whole new theory of statistics? Do we really have data that belong to spaces like the sphere illustrated in introduction?\nLet‚Äôs see some examples of data spaces that are naturally manifolds. By doing so, we will introduce the datasets and visualization modules of geomstats.\nWe first import the datasets.utils module that allows loading datasets.\n\nimport geomstats.datasets.utils as data_utils\n\n\nWorld cities: data on the sphere\nWe load the dataset cities, that contains the coordinates of world cities in spherical coordinates.\n\ndata, names = data_utils.load_cities()\nprint(names[:5])\nprint(data[:5])\n\n['Tokyo', 'New York', 'Mexico City', 'Mumbai', 'S√£o Paulo']\n[[ 0.61993792 -0.52479018  0.58332859]\n [-0.20994315  0.7285533   0.65202298]\n [ 0.14964311  0.93102728  0.33285904]\n [-0.27867026 -0.9034188   0.32584868]\n [-0.62952884  0.6662902  -0.3996884 ]]\n\n\nWe convert the spherical coordinates to X, Y, Z coordinates and verify that they belong to the sphere.\n\ngs.all(sphere.belongs(data))\n\nTrue\n\n\nNow, we plot the cities on the sphere. We choose only a subset of the cities that have a nice rendering in the 2D plot of the 3D earth. This plot is nicer shown in an interactive 3D figure.\n\ndata, names = data_utils.load_cities()\n\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\nvisualization.plot(data[15:20], ax=ax, space='S2', label=names[15:20], s=80, alpha=0.5)\n\nax.set_title('Cities on the earth.');\n\n\n\n\n\n\n\n\n\n\nPose of objects in pictures: data on the Lie group of 3D rotations\nWe consider the dataset poses, that contains the 3D poses of objects in images. Specifically, we consider poses of beds in images, i.e.¬†the 3D orientation of each bed within a given 2D image.\nThe orientation corresponds to a 3D rotation. A 3D rotation \\(R\\) is visually represented as the result of \\(R\\) applied to the coordinate frame \\((e_x, e_y, e_z)\\).\nWe first load the dataset.\n\ndata, img_paths = data_utils.load_poses()\n\nimg_path1, img_path2 = img_paths[0], img_paths[1]\nimg_path1 = os.path.join(data_utils.DATA_PATH, 'poses', img_path1)\nimg_path2 = os.path.join(data_utils.DATA_PATH, 'poses', img_path2)\n\nimg1 = mpimg.imread(img_path1)\nimg2 = mpimg.imread(img_path2)\n\nfig = plt.figure(figsize=(16, 8))\n\nax = fig.add_subplot(121)\nimgplot = ax.imshow(img1)\nax.axis('off')\nax = fig.add_subplot(122)\nimgplot = ax.imshow(img2)\nax.axis('off')\n\nplt.show()\n\n\n\n\n\n\n\n\nWe import the manifold of 3D rotations: the Special Orthogonal group in 3D, \\(SO(3)\\). We choose to represent the 3D rotations as rotation vectors, hence: point_type='vector'.\n\nfrom geomstats.geometry.special_orthogonal import SpecialOrthogonal\n\nso3 = SpecialOrthogonal(n=3, point_type='vector')\n\nWe verify that the poses belong to the space of 3D rotations.\n\ngs.all(so3.belongs(data))\n\nTrue\n\n\nWe plot the orientations of the first 2 beds.\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\n\nvisualization.plot(data[:2], ax=ax, space='SO3_GROUP')\n\nax.set_title('3D orientations of the beds.');\n\n\n\n\n\n\n\n\nThese orientations are very close, as expected from the corresponding images.\n\n\nSocial networks: data on the hyperbolic space\nWe consider the dataset karate_graph, that contains the network of the social interactions of the 34 members of a karate club, for a period of three years from 1970 to 1972. During the study a conflict arose between the administrator and instructor, which led to the split of the club into two. This dataset has been widely used in the machine learning literature as a case-study of clustering on graphs.\nWe load the dataset.\n\nkarate_graph = data_utils.load_karate_graph()\n\nNext, we embed the karate graph into the hyperbolic space.\n\nfrom geomstats.datasets.prepare_graph_data import HyperbolicEmbedding\n\nhyperbolic_embedding = HyperbolicEmbedding(max_epochs=20)\nembeddings = hyperbolic_embedding.embed(karate_graph)\n\nINFO: Number of edges: 34\nINFO: Mean vertices by edges: 4.588235294117647\nINFO: iteration 0 loss_value 1.747328\nINFO: iteration 1 loss_value 1.717289\nINFO: iteration 2 loss_value 1.642318\nINFO: iteration 3 loss_value 1.577687\nINFO: iteration 4 loss_value 1.520208\nINFO: iteration 5 loss_value 1.490956\nINFO: iteration 6 loss_value 1.484013\nINFO: iteration 7 loss_value 1.469828\nINFO: iteration 8 loss_value 1.458344\nINFO: iteration 9 loss_value 1.412500\nINFO: iteration 10 loss_value 1.369023\nINFO: iteration 11 loss_value 1.393613\nINFO: iteration 12 loss_value 1.347368\nINFO: iteration 13 loss_value 1.351978\nINFO: iteration 14 loss_value 1.345865\nINFO: iteration 15 loss_value 1.329180\nINFO: iteration 16 loss_value 1.340976\nINFO: iteration 17 loss_value 1.362401\nINFO: iteration 18 loss_value 1.332718\nINFO: iteration 19 loss_value 1.307022\n\n\n\ndisk = visualization.PoincareDisk(point_type='ball')\nfig, ax = plt.subplots(figsize=(8, 8))\ndisk.set_ax(ax)\ndisk.draw(ax=ax)\nax.scatter(embeddings[:, 0], embeddings[:, 1]);\n\n\n\n\n\n\n\n\nIn this plot, each dot represents a member of the Karate club. The nodes of the social networks have been embedded in the hyperbolic plane. We refer to the notebook embedding_graph_structured_data_h2 for details on the embedding of networks in the hyperbolic space and the advantages of such procedure.\n\n\nBrain connectomes: data on the manifold of Symmetric Positive Definite (SPD) matrices\nWe consider the dataset connectomes that contains brain functional connectomes from the MSLP 2014 Schizophrenia Challenge. The dataset correponds to the Functional Connectivity Networks (FCN) extracted from resting-state fMRIs of 86 patients at 28 Regions Of Interest (ROIs). Roughly, an FCN corresponds to a correlation matrix and can be seen as a point on the manifold of Symmetric Positive-Definite (SPD) matrices.\nWe can load the dataset.\n\ndata, patient_ids, labels = data_utils.load_connectomes()\n\nWe plot the first two connectomes from the MSLP dataset with their corresponding labels.\n\nlabels_str = ['Healthy', 'Schizophrenic']\n\nfig = plt.figure(figsize=(8, 4))\n\nax = fig.add_subplot(121)\nimgplot = ax.imshow(data[0])\nax.set_title(labels_str[labels[0]])\n\nax = fig.add_subplot(122)\nimgplot = ax.imshow(data[1])\nax.set_title(labels_str[labels[1]])\n\nplt.show()\n\n\n\n\n\n\n\n\nWe check that the connectomes belong to the space of SPD matrices.\n\nimport geomstats.geometry.spd_matrices as spd\n\nmanifold = spd.SPDMatrices(28)\n\n\ngs.all(manifold.belongs(data))\n\nTrue\n\n\n\n\nMonkey‚Äôs optical nerve heads: Data as landmarks in 3D\nWe consider 22 images of Rhesus monkeys‚Äô eyes (11 monkeys), acquired with a Heidelberg Retina Tomograph and available in (PE2015). For each monkey, an experimental glaucoma was introduced in one eye, while the second eye was kept as control. One seeks a significant difference between the glaucoma and the control eyes. On each image, 5 anatomical landmarks were recorded: - 1st landmark: superior aspect of the retina, - 2nd landmark: side of the retina closest to the temporal bone of the skull, - 3rd landmark: nose side of the retina, - 4th landmark: inferior point, - 5th landmark: optical nerve head deepest point.\nLabel 0 refers to a normal eye, and Label 1 to an eye with glaucoma.\n\nReference:\n(PE2015) Patrangenaru and L. Ellingson. Nonparametric Statistics on Manifolds and Their Applications to Object Data, 2015. https://doi.org/10.1201/b18969\nWe load the dataset of landmarks‚Äô sets and corresponding labels.\n\nnerves, labels, monkeys = data_utils.load_optical_nerves()\nprint(nerves.shape)\nprint(labels)\nprint(monkeys)\n\n(22, 5, 3)\n[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n[ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10]\n\n\nWe extract the landmarks‚Äô sets corresponding to the two eyes‚Äô nerves of the first monkey, with their corresponding labels.\n\ntwo_nerves = nerves[monkeys==0]\nprint(two_nerves.shape)\n\ntwo_labels = labels[monkeys==0]\nprint(two_labels)\n\n(2, 5, 3)\n[0 1]\n\n\n\nlabel_to_str = {0: 'Normal nerve', 1: 'Glaucoma nerve'}\nlabel_to_color = {0: (102/255, 178/255, 255/255, 1.), 1: (255/255, 178/255, 102/255, 1.)}\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.set_xlim((2000, 4000))\nax.set_ylim((1000, 5000))\nax.set_zlim((-600, 200))\n\nfor nerve, label in zip(two_nerves, two_labels):\n    x = nerve[:, 0]\n    y = nerve[:, 1]\n    z = nerve[:, 2]\n\n    verts = [list(zip(x,y,z))]\n    \n    poly = Poly3DCollection(verts, alpha=0.5)\n    color =  label_to_color[int(label)]\n    poly.set_color(colors.rgb2hex(color))\n    poly.set_edgecolor('k')\n    ax.add_collection3d(poly)\n\npatch_0 = mpatches.Patch(color=label_to_color[0], label=label_to_str[0], alpha=0.5)\npatch_1 = mpatches.Patch(color=label_to_color[1], label=label_to_str[1], alpha=0.5)\nplt.legend(handles=[patch_0, patch_1], prop={'size':20})\nplt.show()\n\n\n\n\n\n\n\n\nWe observe that the shape of the optical nerve head on the eye with glaucoma is different from the shape of the optical nerve head on the normal eye.\n\n\n\nCell shapes: Data as curves in 2D\nWe load the dataset of cells.\nThe cells have been treated with one of three treatments: - control (no treatment), - jasp (jasplakinolide), or - cytd (cytochalasin D).\nThese treatments are drugs which perturb the cytoskelet of the cells, and thus the shapes of their boundaries.\n\ncells, cell_lines, treatments = data_utils.load_cells()\n\nWe check that the dataset of cells belongs to the manifold of discrete curves.\n\nfrom geomstats.geometry.discrete_curves import DiscreteCurves\nfrom geomstats.geometry.euclidean import Euclidean\n\nr2 = Euclidean(2)\n\nplanar_curves_space = DiscreteCurves(r2)\n\nresult = planar_curves_space.belongs(cells)\ngs.all(result)\n\nTrue\n\n\nWe show one of the cells.\n\ncell_id = 500\ncell = cells[cell_id]\n\nplt.plot(cell[:, 0], cell[:, 1])\nplt.axis('equal')\nplt.title(f\"Cell treated with drug: {treatments[cell_id]}\")\nplt.axis('off');"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/04_riemannian_frechet_mean_and_tangent_pca.html",
    "href": "wl-mechanics/src/geomstats/notebooks/04_riemannian_frechet_mean_and_tangent_pca.html",
    "title": "Tutorial: Fr√©chet Mean and Tangent PCA",
    "section": "",
    "text": "This notebook shows how to compute the Fr√©chet mean of a data set. Then it performs tangent PCA at the mean."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/04_riemannian_frechet_mean_and_tangent_pca.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/04_riemannian_frechet_mean_and_tangent_pca.html#setup",
    "title": "Tutorial: Fr√©chet Mean and Tangent PCA",
    "section": "Setup",
    "text": "Setup\n\nimport os\nimport sys\nimport warnings\n\nsys.path.append(os.path.dirname(os.getcwd()))\nwarnings.filterwarnings('ignore')\n\n\nimport matplotlib.pyplot as plt\n\nimport geomstats.backend as gs\nimport geomstats.visualization as visualization\n\nfrom geomstats.learning.frechet_mean import FrechetMean\nfrom geomstats.learning.pca import TangentPCA\n\nINFO: Using numpy backend"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/04_riemannian_frechet_mean_and_tangent_pca.html#on-the-sphere",
    "href": "wl-mechanics/src/geomstats/notebooks/04_riemannian_frechet_mean_and_tangent_pca.html#on-the-sphere",
    "title": "Tutorial: Fr√©chet Mean and Tangent PCA",
    "section": "On the sphere",
    "text": "On the sphere\n\nGenerate data on the sphere\n\nfrom geomstats.geometry.hypersphere import Hypersphere\n\nsphere = Hypersphere(dim=2)\ndata = sphere.random_von_mises_fisher(kappa=15, n_samples=140)\n\n\nfig = plt.figure(figsize=(8, 8))\nax = visualization.plot(data, space='S2', color='black', alpha=0.7, label='Data points')\nax.set_box_aspect([1, 1, 1])\nax.legend();\n\n\n\n\n\n\n\n\n\n\nFr√©chet mean\nWe compute the Fr√©chet mean of the simulated data points.\n\nmean = FrechetMean(metric=sphere.metric)\nmean.fit(data)\n\nmean_estimate = mean.estimate_\n\n\nfig = plt.figure(figsize=(8, 8))\nax = visualization.plot(data, space='S2', color='black', alpha=0.2, label='Data points')\nax = visualization.plot(mean_estimate, space='S2', color='red', ax=ax, s=200, label='Fr√©chet mean')\nax.set_box_aspect([1, 1, 1])\nax.legend();\n\n\n\n\n\n\n\n\n\n\nTangent PCA (at the Fr√©chet mean)\nWe perform tangent PCA at the Fr√©chet mean, with two principal components.\n\ntpca = TangentPCA(metric=sphere.metric, n_components=2)\ntpca = tpca.fit(data, base_point=mean_estimate)\ntangent_projected_data = tpca.transform(data)\n\nWe compute the geodesics on the sphere corresponding to the two principal components.\n\ngeodesic_0 = sphere.metric.geodesic(\n        initial_point=mean_estimate,\n        initial_tangent_vec=tpca.components_[0])\ngeodesic_1 = sphere.metric.geodesic(\n        initial_point=mean_estimate,\n        initial_tangent_vec=tpca.components_[1])\n\nn_steps = 100\nt = gs.linspace(-1., 1., n_steps)\ngeodesic_points_0 = geodesic_0(t)\ngeodesic_points_1 = geodesic_1(t) \n\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111)\nxticks = gs.arange(1, 2+1, 1)\nax.xaxis.set_ticks(xticks)\nax.set_title('Explained variance')\nax.set_xlabel('Number of Principal Components')\nax.set_ylim((0, 1))\nax.bar(xticks, tpca.explained_variance_ratio_);\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection=\"3d\")\n\nax = visualization.plot(\n    geodesic_points_0, ax, space='S2', linewidth=2, label='First component')\nax = visualization.plot(\n    geodesic_points_1, ax, space='S2', linewidth=2, label='Second component')\nax = visualization.plot(\n    data, ax, space='S2', color='black', alpha=0.2, label='Data points')\nax = visualization.plot(\n    mean_estimate, ax, space='S2', color='red', s=200, label='Fr√©chet mean')\nax.legend()\nax.set_box_aspect([1, 1, 1])\nplt.show()"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/04_riemannian_frechet_mean_and_tangent_pca.html#in-the-hyperbolic-plane",
    "href": "wl-mechanics/src/geomstats/notebooks/04_riemannian_frechet_mean_and_tangent_pca.html#in-the-hyperbolic-plane",
    "title": "Tutorial: Fr√©chet Mean and Tangent PCA",
    "section": "In the Hyperbolic plane",
    "text": "In the Hyperbolic plane\n\nGenerate data on the hyperbolic plane\n\nfrom geomstats.geometry.hyperboloid import Hyperboloid\n\nhyperbolic_plane = Hyperboloid(dim=2)\n\ndata = hyperbolic_plane.random_point(n_samples=140)\n\n\nfig = plt.figure(figsize=(8, 8))\nax = visualization.plot(data, space='H2_poincare_disk', color='black', alpha=0.7, label='Data points')\nax.legend();\n\n\n\n\n\n\n\n\n\n\nFr√©chet mean\nWe compute the Fr√©chet mean of the data points.\n\nmean = FrechetMean(metric=hyperbolic_plane.metric)\nmean.fit(data)\n\nmean_estimate = mean.estimate_\n\n\nfig = plt.figure(figsize=(8, 8))\nax = visualization.plot(data, space='H2_poincare_disk', color='black', alpha=0.2, label='Data points')\nax = visualization.plot(mean_estimate, space='H2_poincare_disk', color='red', ax=ax, s=200, label='Fr√©chet mean')\nax.legend();\n\n\n\n\n\n\n\n\n\n\nTangent PCA (at the Fr√©chet mean)\nWe perform tangent PCA at the Fr√©chet mean.\n\ntpca = TangentPCA(metric=hyperbolic_plane.metric, n_components=2)\ntpca = tpca.fit(data, base_point=mean_estimate)\ntangent_projected_data = tpca.transform(data)\n\nWe compute the geodesics corresponding to the first components of the tangent PCA.\n\ngeodesic_0 = hyperbolic_plane.metric.geodesic(\n        initial_point=mean_estimate,\n        initial_tangent_vec=tpca.components_[0])\ngeodesic_1 = hyperbolic_plane.metric.geodesic(\n        initial_point=mean_estimate,\n        initial_tangent_vec=tpca.components_[1])\n\nn_steps = 100\nt = gs.linspace(-1., 1., n_steps)\ngeodesic_points_0 = geodesic_0(t)\ngeodesic_points_1 = geodesic_1(t) \n\n\nfig = plt.figure(figsize=(6, 6))\nax = fig.add_subplot(111)\nxticks = gs.arange(1, 2+1, 1)\nax.xaxis.set_ticks(xticks)\nax.set_title('Explained variance')\nax.set_xlabel('Number of Principal Components')\nax.set_ylim((0, 1))\nax.bar(xticks, tpca.explained_variance_ratio_);\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111)\n\nax = visualization.plot(\n    geodesic_points_0, ax, space='H2_poincare_disk', linewidth=2, label='First component')\nax = visualization.plot(\n    geodesic_points_1, ax, space='H2_poincare_disk', linewidth=2, label='Second component')\nax = visualization.plot(\n    data, ax, space='H2_poincare_disk', color='black', alpha=0.2, label='Data points')\nax = visualization.plot(\n    mean_estimate, ax, space='H2_poincare_disk', color='red', s=200, label='Fr√©chet mean')\nax.legend()\nplt.show()"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/07_implement_your_own_riemannian_geometry.html",
    "href": "wl-mechanics/src/geomstats/notebooks/07_implement_your_own_riemannian_geometry.html",
    "title": "Tutorial: Implement your own Riemannian Geometry",
    "section": "",
    "text": "Geomstats provides several Riemannian manifolds in its geometry folder. Yet, the manifold that you are interested in might not be available there.\nThis notebook shows how to use Riemannian geometry on any manifold defined by an immersion into a Euclidean space, such as high-dimensional surfaces immersed in a Euclidean space.\nSpecifically, we focus on the case of an embedded manifold \\(M\\) that can be defined by a map f: \\(f: M \\rightarrow \\mathbb{R}^n\\) called the immersion, whose differential \\(df_x\\) is injective for all \\(x \\in M\\).\nThis immersion allows to define the pull-back metric \\(g\\) on \\(M\\), as: \\[g : T_xM \\times T_x M \\rightarrow \\mathbb{R}\\\\\nu, v \\rightarrow &lt;df_x.u, df_x.v&gt;\\] where \\(&lt;,&gt;\\) represents the Euclidean inner-product of the embedding space.\nThe pull-back metric gives a structure of Riemannian manifold to \\(M\\). In particular, we can compute the Riemannian exp and log maps, the Riemannian distance, the Riemannian parallel transport, etc.\nThis notion illustrates the computation of the pull-back metric, using the class PullbackMetric from geomstats, on two embedded manifolds: - the 2-sphere \\(S^2\\) embedded in \\(\\mathbb{R}^3\\), - a surface defined by: \\(x, y \\rightarrow z = x^2 + y^2\\) embedded in \\(\\mathbb{R}^3\\)."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/07_implement_your_own_riemannian_geometry.html#setup",
    "href": "wl-mechanics/src/geomstats/notebooks/07_implement_your_own_riemannian_geometry.html#setup",
    "title": "Tutorial: Implement your own Riemannian Geometry",
    "section": "Setup",
    "text": "Setup\n\nimport os\nimport sys\nimport time\nimport warnings\n\nsys.path.append(os.path.dirname(os.getcwd()))\nwarnings.filterwarnings('ignore')\n\n\nimport matplotlib.pyplot as plt\n\nimport geomstats.visualization as viz\n\nINFO: Using pytorch backend\n\n\n\nimport geomstats.backend as gs\n\nWe import the main structure used in this notebook: the PullbackMetric:\n\nfrom geomstats.geometry.pullback_metric import PullbackMetric"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/07_implement_your_own_riemannian_geometry.html#immersed-manifolds-the-example-of-the-2-sphere",
    "href": "wl-mechanics/src/geomstats/notebooks/07_implement_your_own_riemannian_geometry.html#immersed-manifolds-the-example-of-the-2-sphere",
    "title": "Tutorial: Implement your own Riemannian Geometry",
    "section": "Immersed manifolds: the example of the 2-sphere",
    "text": "Immersed manifolds: the example of the 2-sphere\nWe first consider the simple example of the 2-sphere. We define the immersion of the 2-sphere \\(S^2\\) into the Euclidean space \\(\\mathbb{R}^3\\) as follows: \\[f : S^2 \\rightarrow \\mathbb{R}^3\\\\\n(\\theta, \\phi) \\rightarrow (\\cos\\phi.\\sin\\theta, \\sin\\phi.\\sin\\theta, \\cos\\theta)\\]\n\ndef sphere_immersion(spherical_coords):\n    theta = spherical_coords[..., 0]\n    phi = spherical_coords[..., 1]\n    return gs.array([\n        gs.cos(phi) * gs.sin(theta),\n        gs.sin(phi) * gs.sin(theta),\n        gs.cos(theta)])\n\nFor the purpose of visualizing the results in the embedding space \\(\\mathbb{R}^3\\), we will need the jacobian of the immersion, which we compute here:\n\njac_sphere_immersion = gs.autodiff.jacobian(sphere_immersion)\n\nWe use the PullbackMetric structure to define the Riemannian metric on \\(S^2\\) from the immersion.\nNote that the Riemannian metric on the sphere is already implemented in Geomstats using closed forms with the class Hypersphere. However, this notebook showcases how we can recover the computations of Riemanian geometry by only relying on the immersion.\n\nsphere_metric = PullbackMetric(dim=2, embedding_dim=3, immersion=sphere_immersion)\n\nNow, we can access the methods from any Riemannian metric, i.e.¬†the Riemannian exp and log maps, the parallel transport, etc.\nWe first show the computation of the Riemannian exp map of a tangent vector at a point.\n\npoint_a = gs.array([gs.pi / 2. , - gs.pi / 2.])\ntangent_vec = gs.array([0., gs.pi / 3.])\n\nend_point = sphere_metric.exp(\n    tangent_vec=tangent_vec, base_point=point_a)\nprint(end_point)\n\ntensor([ 1.5708, -0.5236])\n\n\nAnd visualize the result of the Riemannian exp map in the embedding space \\(\\mathbb{R}^3\\):\n\n%matplotlib notebook\n\n# We immerse the points and the tangent vector in R^3\n\nimmersed_point_a = sphere_immersion(point_a)\nimmersed_tangent_vec = gs.matmul(jac_sphere_immersion(point_a), tangent_vec)\nimmersed_end_point = sphere_immersion(end_point)\n\n# We plot our results\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\n\nviz.plot(\n    immersed_point_a, ax=ax, space='S2', label='Initial point', s=80)\n\narrow = viz.Arrow3D(immersed_point_a, vector=immersed_tangent_vec)\narrow.draw(ax, color='black', label=\"Tangent vector\")\n\nviz.plot(\n    immersed_end_point, ax=ax, space='S2', label='End point', s=80)\n\nax.set_title('Riemannian Exp map on the sphere')\nax.legend()\nax.grid(False)\nplt.axis('off');\n\n\n\n\n\n\n\nNext, we show the computation of the parallel transport on the sphere. Note that step, n_steps, tol and alpha are integration parameters that control the efficiency-accuracy tradeoff of the computation.\nNote: The accuracy of the computation varies for the different backends. We recommend using autograd, and we discourage the use of tensorflow.\n\npoint_a = gs.array([gs.pi / 2. , - gs.pi / 2.])\ntangent_vec = gs.array([0., gs.pi / 3.])\ntangent_vec_to_transport = gs.array([gs.pi / 4., gs.pi / 3.])\n\ntime_start = time.perf_counter()\nparallel_transport = sphere_metric.ladder_parallel_transport(\n    tangent_vec_a=tangent_vec_to_transport, \n    tangent_vec_b=tangent_vec,\n    base_point=point_a,\n    step='euler', n_steps=1, tol=1e-6, alpha=1)  \n\ntime_elapsed = time.perf_counter() - time_start\nprint(f\"Computing time for parallel transport: {time_elapsed:5.2f} secs\")\n\ndisplay(parallel_transport)\n\ntransported_tangent_vec = parallel_transport[\"transported_tangent_vec\"]\nend_point = parallel_transport[\"end_point\"]\n\nComputing time for parallel transport:  0.19 secs\n\n\n{'transported_tangent_vec': tensor([0.7854, 1.0472]),\n 'end_point': tensor([ 1.5708, -0.5236]),\n 'trajectory': [[]]}\n\n\nWe visualize the result of the parallel transport in the embedding space \\(\\mathbb{R}^3\\):\n\n%matplotlib notebook\n\n\n# We first immerse the points and tangent vectors into the embedding space R^3\n\nimmersed_point_a = sphere_immersion(point_a)\nimmersed_end_point = sphere_immersion(end_point)\n\nimmersed_tangent_vec = gs.matmul(\n    jac_sphere_immersion(point_a), tangent_vec)\nimmersed_tangent_vec_to_transport = gs.matmul(\n    jac_sphere_immersion(point_a), tangent_vec_to_transport)\nimmersed_transported_tangent_vec = gs.matmul(\n    jac_sphere_immersion(end_point), transported_tangent_vec)\n\n# We verify manually that the immersed tangent vector is actually tangent to the sphere\n# as the plot can be sometimes misleading. We use the method of the Hypersphere class.\n\nfrom geomstats.geometry.hypersphere import Hypersphere\nsphere = Hypersphere(dim=2)\nis_tangent = sphere.is_tangent(\n    immersed_transported_tangent_vec, base_point=immersed_end_point)\nprint(\"is_tangent = \", is_tangent)\n\n# We plot the results\n\nfig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\n\nviz.plot(\n    immersed_point_a, ax=ax, space='S2', label='Initial point', s=80)\n\narrow = viz.Arrow3D(immersed_point_a, vector=immersed_tangent_vec)\narrow.draw(ax, color='black', label=\"Tangent vector\")\n\narrow = viz.Arrow3D(immersed_point_a, vector=immersed_tangent_vec_to_transport)\narrow.draw(ax, color='red', label=\"Tangent vector to transport\")\n\nviz.plot(\n    immersed_end_point, ax=ax, space='S2', label='End point', s=80)\n\narrow = viz.Arrow3D(immersed_end_point, vector=immersed_transported_tangent_vec)\narrow.draw(ax, color='orange', label=\"Transported tangent vector\")\n\nax.set_title('Riemannian parallel transport on the sphere')\nax.legend()\nax.grid(False)\nplt.axis('off');\n\nis_tangent =  tensor(True)"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/07_implement_your_own_riemannian_geometry.html#immersed-manifolds-the-example-of-a-surface-defined-by-its-graph",
    "href": "wl-mechanics/src/geomstats/notebooks/07_implement_your_own_riemannian_geometry.html#immersed-manifolds-the-example-of-a-surface-defined-by-its-graph",
    "title": "Tutorial: Implement your own Riemannian Geometry",
    "section": "Immersed manifolds: the example of a surface defined by its graph",
    "text": "Immersed manifolds: the example of a surface defined by its graph\nWe consider the example of a 2D surface immersed in \\(\\mathbb{R}^3\\). The surface is defined by its graph: \\[ x, y \\rightarrow z = x^2 + y^2\\] which leads to the following immersion into \\(\\mathbb{R}^3\\): \\[f : S^2 \\rightarrow \\mathbb{R}^3\\\\\n(x, y) \\rightarrow (x, y, x^2 + y^2)\\]\nWe first implement the graph and the immersion:\n\ndef surface_graph(x, y):\n    return x ** 2 + y ** 2\n\ndef surface_immersion(intrinsic_coords):\n    x = intrinsic_coords[..., 0]\n    y = intrinsic_coords[..., 1]\n    return gs.transpose(gs.array([x, y, surface_graph(x, y)]))\n\nFor the purpose of visualizing the results in the embedding space ‚Ñù3 , we will need the jacobian of the immersion, which we compute here:\n\njac_surface_immersion = gs.autodiff.jacobian(surface_immersion)\n\nWe also add a utility function to visualization the surface in 3D:\n\n%matplotlib notebook\n\ndef plot_surface(alpha=1., ax=None):\n    if ax is None:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n    \n    x = y = gs.arange(-3.0, 3.0, 0.1)\n    X, Y =gs.meshgrid(x, y)\n    zs = gs.array(surface_graph(gs.flatten(X), gs.flatten(Y)))\n    Z = gs.reshape(zs, X.shape)\n    \n    ax.plot_surface(\n        gs.to_numpy(X), \n        gs.to_numpy(Y), \n        gs.to_numpy(Z), \n        alpha=alpha)\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Zm')\n    plt.show()\n    return ax\n\nax = plot_surface()\nax.grid(False)\nplt.axis('off');\n\n\n\n\n\n\n\nWe use the PullbackMetric structure to define the Riemannian metric on the surface from the immersion.\n\nsurface_metric = PullbackMetric(dim=2, embedding_dim=3, immersion=surface_immersion)\n\nNow, we can access the methods from any Riemannian metric, i.e.¬†the Riemannian exp and log maps, the parallel transport, etc.\nWe show the computation of the Riemannian exp map:\n\npoint_a = gs.array([-2. , - 2.])\ntangent_vec = gs.array([0., 1.])\n\nend_point = surface_metric.exp(\n    tangent_vec=tangent_vec, base_point=point_a)\nprint(end_point)\n\ntensor([-2.0000, -1.0000])\n\n\nAnd visualize the result:\n\n%matplotlib notebook\n\n# We first immerse the points and tangent vector into the embedding space R^3\n\nimmersed_point_a = surface_immersion(point_a)\nimmersed_tangent_vec = gs.matmul(jac_surface_immersion(point_a), tangent_vec)\nimmersed_end_point = surface_immersion(end_point)\n\ntwo_points = gs.vstack([immersed_point_a, immersed_end_point])\n\n# We plot the results\n\nax = plot_surface(alpha=0.3)\n\nax.plot(\n    immersed_point_a[0], \n    immersed_point_a[1], \n    immersed_point_a[2], \n    label='Initial point', marker=\"o\", linestyle = 'None')\n\narrow = viz.Arrow3D(immersed_point_a, vector=immersed_tangent_vec)\narrow.draw(ax, color='black', label=\"Tangent vector\")\n\nax.plot(\n    immersed_end_point[0], \n    immersed_end_point [1], \n    immersed_end_point [2], \n    label='End point', marker=\"o\", linestyle = 'None')\n\nax.set_title('Riemannian exponential map on a surface')\nax.legend()\nax.grid(False)\nplt.axis('off');\n\n\n\n\n\n\n\nNext, we show the computation of the parallel transport on the surface. Again, note that step, n_steps, tol and alpha are integration parameters that control the efficiency-accuracy tradeoff of the computation.\n\npoint_a = gs.array([-2. , - 2.])\ntangent_vec = gs.array([0., 1.])\ntangent_vec_to_transport = gs.array([-.6, .6])\n\ntime_start = time.perf_counter()\nparallel_transport = surface_metric.ladder_parallel_transport(\n    tangent_vec_a=tangent_vec_to_transport, \n    tangent_vec_b=tangent_vec,\n    base_point=point_a,\n    step=\"rk4\", n_steps=1, tol=1e-14, alpha=2)\ntime_elapsed = time.perf_counter() - time_start\nprint(f\"Computing time for parallel transport: {time_elapsed:5.2f} secs\")\n\ndisplay(parallel_transport)\n\ntransported_tangent_vec = parallel_transport[\"transported_tangent_vec\"]\nend_point = parallel_transport[\"end_point\"]\n\nComputing time for parallel transport:  0.22 secs\n\n\n{'transported_tangent_vec': tensor([-0.6000,  0.6000]),\n 'end_point': tensor([-2.0000, -1.0000]),\n 'trajectory': [[]]}\n\n\nWe visualize the result of the parallel transport.\n\n%matplotlib notebook\n\n# We first immerse the points and tangent vectors into the embedding space R^3\n\nimmersed_point_a = surface_immersion(point_a)\nimmersed_tangent_vec = gs.matmul(\n    jac_surface_immersion(point_a), tangent_vec)\nimmersed_tangent_vec_to_transport = gs.matmul(\n    jac_surface_immersion(point_a), tangent_vec_to_transport)\nimmersed_end_point = surface_immersion(end_point)\nimmersed_transported_tangent_vec = gs.matmul(\n    jac_surface_immersion(end_point), transported_tangent_vec)\n\n# We plot the results\n\nax = plot_surface(alpha=0.3)\n\nax.plot(\n    immersed_point_a[0], \n    immersed_point_a[1], \n    immersed_point_a[2], \n    label='Initial point', marker=\"o\", color=\"orange\")\n\narrow = viz.Arrow3D(\n    immersed_point_a, \n    vector=immersed_tangent_vec_to_transport)\narrow.draw(ax, color=\"orange\", label=\"Tangent vector to transport\")\n\narrow = viz.Arrow3D(\n    immersed_point_a, \n    vector=immersed_tangent_vec)\narrow.draw(ax, color='black', label=\"Tangent vector\")\n\nax.plot(\n    immersed_end_point[0], \n    immersed_end_point[1], \n    immersed_end_point[2], \n    label='End point', marker=\"o\", color=\"green\")\n\n\narrow = viz.Arrow3D(\n    immersed_end_point, \n    vector=immersed_transported_tangent_vec)\narrow.draw(ax, color='green', label=\"Transported tangent vector\")\n\nax.set_title('Riemannian parallel transport on a surface')\nax.legend()\nax.grid(False)\nplt.axis('off');"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html",
    "title": "Tutorial : Computing with triangular shapes in Kendall framework",
    "section": "",
    "text": "David G. Kendall first took interest into triangular shapes while studying random alignments of points in archeology and related fields [KK1980]. Following this statistical study, he developped the more general theory of shapes. This theory echoes in many fields like biology, medicine, or image analysis, where the shape of an object is particularly relevant when it comes to its analysis. Since the deformation of a shape is not necessarely linear, we need to use geometric statistics in order to proceed with such data.\nIn Kendall theory, the word shape is used in the common sense, refering to the appearance of an object. Since the shape of an object should not depend on the way it is looked at or computed, it is natural to remove location, rotationnal and eventually scale effects. A practical way of describing an object‚Äôs shape is to locate a finite number of relevant points on it, which we call landmarks, which should summarize the geometrical and/or scientifical information [DM2016].\nThus we define Kendall shape space of order \\((k,m)\\) (where \\(k\\geq 2\\)) as the quotient of \\(k\\)-tuples of non-aligned points in \\(\\mathbb{R}^m\\) by the group of rotations, translations and dilatations of \\(\\mathbb{R}^m\\) :\n\\[\\big(\\Sigma^k_m\\triangleq\\mathbb{R}^{km}\\setminus \\Delta \\big)/\\big(\\mathbb{R_+^*}\\times SO(m)\\ltimes \\mathbb{R}^m\\big)\\]\nwhere \\(\\Delta =\\{\\ (a)_{1\\leq i\\leq k}\\ |\\ a\\in\\mathbb{R}^m\\}\\) correspond to the degenerate case where all landmarks collapse into a unique point.\nIt is useful to see that considering first the action of translations and dilatations before applying rotation group \\(\\text{SO}\\) allows to write :\n\\[\\Sigma^k_m=\\mathcal{S}^k_m/ \\text{SO}(m)\\]\nwhere \\(\\mathcal{S}^k_m\\), called the pre-shape space of order \\((k,m)\\) denotes the space of centered and normalized \\(k\\)-tuples of in \\(\\mathbb{R}^m\\). It corresponds to the unit sphere of dimension \\(k(m-1)\\) [K1984].\nThe Kendall shape space of order \\((k,m)\\) is a differential manifold which acquires singularities as soon as \\(m\\geq 3\\). Apart from these singularities, it is a smooth manifold which can be equipped with a Riemannian metric inherited from the Frobenius metric on the pre-shape space [LK1993].\nIn this tutorial, we take a deeper look at two specific cases of Kendall shape spaces : the space \\(\\Sigma^3_2\\) of 2D triangles and the space \\(\\Sigma^3_3\\) of 3D triangles. These two spaces are particularly interesting to look at because there exists a projection such that we can directly visualize them.\nThe tutorial is organized as follows :"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#uniform-distribution",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#uniform-distribution",
    "title": "Tutorial : Computing with triangular shapes in Kendall framework",
    "section": "Uniform distribution",
    "text": "Uniform distribution\nSince the projection is isometric, the uniform distribution in \\(\\Sigma^3_2\\) is exactly the uniform distribution on the sphere.\n\nfig = plt.figure(figsize=(15, 15))\nS.draw()\n\npoints = preshape_triangle_2d.random_uniform(1000)\n\nS.clear_points()\nS.add_points(points)\nS.draw_points(color='k', s=2)"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#geodesic",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#geodesic",
    "title": "Tutorial : Computing with triangular shapes in Kendall framework",
    "section": "Geodesic",
    "text": "Geodesic\nThe geodesics in \\(\\Sigma^3_2\\) corresponds to great circles on the sphere. The distance between two triangles of \\(\\Sigma^3_2\\) is given exactly by the arc-length of the arc joining the two projected points on the sphere. The diameter of the space \\(\\Sigma^3_2\\), defined as the distance between the two most distant points, is then equal to \\(\\pi/2\\).\n\nfig = plt.figure(figsize=(15, 15))\nS.draw()\n\ntimes = gs.linspace(0., 1., 1000)\nvelocities = gs.array([-t * .5 * S.ub for t in times])\npoints = metric_triangle_2d.exp(velocities, S.pole)\n\nS.clear_points()\nS.add_points(points)\nS.draw_curve(color='b', lw=3)\nS.clear_points()\nS.add_points(points[[0, -1]])\nS.draw_points(color='k', s=70)"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#parallel-transport",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#parallel-transport",
    "title": "Tutorial : Computing with triangular shapes in Kendall framework",
    "section": "Parallel transport",
    "text": "Parallel transport\nGiven the geodesics and the logarithm map, one can easily compute the parallel transport of a vector \\(w\\) along the geodesic \\(\\exp_\\text{b.p.}(tv)\\) using the Pole Ladder algorithm implemented in geomstats.\n\nbase_point, v, w = S.pole, -.5 * S.ub, S.ua\nn_rungs=10\nladder = metric_triangle_2d.ladder_parallel_transport(\n    w, v, base_point, n_rungs, return_geodesics=True)\n\ntransported = ladder['transported_tangent_vec']\nend_point = ladder['end_point']\ntrajectory = ladder['trajectory']\n\nn_points = 10\nt = gs.linspace(0., 1., n_points)\nt_main = gs.linspace(0., 1., n_points * 4)\nmain = []\ndiag = []\nfor points in trajectory:\n    main_geodesic, diagonal, final_geodesic = points\n    main.append(main_geodesic(t_main))\n    diag.append(diagonal(-t))\n    diag.append(diagonal(t))\n\ndiag = gs.stack(diag).reshape(-1, k_landmarks, m_ambient)\nmain = gs.stack(main).reshape(-1, k_landmarks, m_ambient)\n\ntangent_vectors = [v / n_rungs, w / n_rungs, transported / n_rungs]\nbase_points = [base_point, base_point, main[-1]]\n\n\nfig = plt.figure(figsize=(15, 15))\nS.draw()\n\nS.clear_points()\nS.add_points(main)\nS.draw_curve(color='b', lw=3)\nS.clear_points()\nS.add_points(diag)\nS.draw_points(color='r', s=2)\nfor bp, tv in zip(base_points, tangent_vectors):\n    S.draw_vector(tv, bp, color=(0,1,0), lw=3)\n\n\n\n\n\n\n\n\nThe diagonal geodesics used by the Pole Ladder method to compute the parallel transport are plotted in red.\nIn the specific case of Kendall shape spaces, one can use a second method, based on an integration scheme. This method follows the result of [KDLS2021] giving conditions on a vector field in the top space \\(\\mathcal S^k_m\\) for its projection to be the parallel transport in the shape space \\(\\Sigma^k_m\\). The implementation of the method can be found in the KendallShapeMetric and is described along with its comparison to the Pole Ladder method in [GMTP21]."
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#uniform-distribution-1",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#uniform-distribution-1",
    "title": "Tutorial : Computing with triangular shapes in Kendall framework",
    "section": "Uniform distribution",
    "text": "Uniform distribution\nAs in the previous example, the uniform distribution on \\(\\Sigma^3_3\\) is given exactly by the uniform distribution on a disk.\n\nfig = plt.figure(figsize=(15, 15))\nD.draw()\n\npoints = preshape_triangle_3d.random_uniform(1000)\n\nD.clear_points()\nD.add_points(points)\nD.draw_points(color='k', s=2)"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#geodesics",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#geodesics",
    "title": "Tutorial : Computing with triangular shapes in Kendall framework",
    "section": "Geodesics",
    "text": "Geodesics\nWe show a first example where the geodesic is radial. It is the projection of the geodesic we previously shot in \\(\\Sigma^3_2\\).\n\nfig = plt.figure(figsize=(15, 15))\nD.draw()\n\ntimes = gs.linspace(0., 1., 1000)\nub_3d = gs.transpose(gs.stack((D.ub[:, 0], D.ub[:, 1], gs.zeros(3))))\nspeeds = gs.array([-t * .5 * ub_3d for t in times])\npoints = metric_triangle_3d.exp(speeds, D.centre)\n\nD.clear_points()\nD.add_points(points)\nD.draw_curve(color='b', lw=3)\nD.clear_points()\nD.add_points(points[[0, -1]])\nD.draw_points(color='k', s=70)\n\n\n\n\n\n\n\n\nWe show a second example where the initial point is no more the equilateral triangle.\n\nfig = plt.figure(figsize=(15, 15))\nD.draw()\n\ntimes = gs.linspace(0., 1., 1000)\nbase_point = preshape_triangle_3d.projection(D.centre + .35 * ub_3d)\ntangent_vec = preshape_triangle_3d.to_tangent(preshape_triangle_3d.random_point(), base_point)\ntangent_vec = .5 * tangent_vec / metric_triangle_3d.norm(tangent_vec, base_point)\nspeeds = gs.array([t * tangent_vec for t in times])\npoints = metric_triangle_3d.exp(speeds, base_point)\n\nD.clear_points()\nD.add_points(points)\nD.draw_curve(color='b', lw=3)\nD.clear_points()\nD.add_points(points[[0,-1]])\nD.draw_points(color='k', s=70)"
  },
  {
    "objectID": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#parallel-transport-1",
    "href": "wl-mechanics/src/geomstats/notebooks/usecase_visualizations_in_kendall_shape_spaces.html#parallel-transport-1",
    "title": "Tutorial : Computing with triangular shapes in Kendall framework",
    "section": "Parallel transport",
    "text": "Parallel transport\nWe compute the parallel transport of an orthogonal vector \\(w\\) along the previous geodesic using the Pole Ladder method.\n\nbase_point = base_point\nv = tangent_vec\nw = preshape_triangle_3d.to_tangent(preshape_triangle_3d.random_point(), base_point)\nw = w - metric_triangle_3d.inner_product(w, 2. * tangent_vec, base_point) * 2. * tangent_vec\nw = w / metric_triangle_3d.norm(w, base_point)\nn_rungs = 10\nladder = metric_triangle_3d.ladder_parallel_transport(\n    w, v, base_point, n_rungs, return_geodesics=True)\n\ntransported = ladder['transported_tangent_vec']\nend_point = ladder['end_point']\ntrajectory = ladder['trajectory']\n\nn_points = 10\nt = gs.linspace(0., 1, n_points)\nt_main = gs.linspace(0., 1., n_points * 4)\nmain = []\ndiag = []\nfor points in trajectory:\n    main_geodesic, diagonal, final_geodesic = points\n    main.append(main_geodesic(t_main))\n    diag.append(diagonal(-t))\n    diag.append(diagonal(t))\n\ndiag = gs.stack(diag).reshape(-1, k_landmarks, m_ambient)\nmain = gs.stack(main).reshape(-1, k_landmarks, m_ambient)\n\ntangent_vectors = [v / n_rungs, w / n_rungs, transported / n_rungs]\nbase_points = [base_point, base_point, main[-1]]\n\n\nfig = plt.figure(figsize=(15, 15))\nD.draw()\n\nD.clear_points()\nD.add_points(main)\nD.draw_curve(color='b', lw=3)\nD.clear_points()\nD.add_points(diag)\nD.draw_points(color='r', s=1)\nfor bp, vec in zip(base_points, tangent_vectors):\n    D.draw_vector(vec, bp, color=(0, 1, 0), width=.003)"
  },
  {
    "objectID": "posts/Grad_flows.html",
    "href": "posts/Grad_flows.html",
    "title": "Wasserstein gradient flows",
    "section": "",
    "text": "Given some energy functional \\(\\mathcal{E}(\\rho)\\) in some probability space \\(\\mathcal{P}(\\Omega)\\) with some metric \\(\\mathcal{G}(\\rho))\\), \\((\\mathcal{P}(\\Omega), \\mathcal{G}(\\rho))\\), a gradient flow is defined as the inverse metric times the differential of the energy function \\[\\begin{equation}\n    \\partial_t \\rho_t = -\\mathcal{G}(\\rho_t)^{-1} \\frac{\\delta \\mathcal{E}(\\rho_t)}{\\delta \\rho_t}.\n\\end{equation}\\] Here, \\(\\rho_t\\) is a distribution at time \\(t\\).\nIntuitively, this means that the considered system of equations follows the trajectory of steepest descend on the energy functional \\(\\mathcal{E}(\\rho)\\). To define this steepest descend we need to define the notion of the gradient. The gradient, in turn, depends on the selected geometry of the space and is computed according to the selected metric.\nIf we consider for energy functional the Kullback Leibler divergence \\(D_{KL}\\), and for (information) metric the Wasserstein metric \\(\\mathcal{W}\\), the considered gradient flow, known as Wasserstein gradient flow, forms the Fokker-Planck equation.\nIn this case the metric inverse is \\(\\nabla \\cdot \\rho_t \\nabla\\), and we can derive the Fokker‚ÄìPlanck equation as follows:\n\\[\\begin{align}\n\\partial_t \\rho_t &= - \\text{grad}^{\\mathcal{W}} D_{KL}(\\rho_t ||\\rho_{ss})\\\\\n&= \\nabla \\cdot \\left(  \\rho_t \\nabla \\left( f + \\log \\rho_t +1 \\right)  \\right)\\\\\n&= \\nabla \\cdot \\left( \\rho_t \\nabla f\\right) + \\nabla \\cdot \\nabla \\rho_t\\\\\n&= \\nabla \\cdot \\left( \\rho_t \\nabla f\\right) + \\Delta \\rho_t.\n\\end{align}\\]\nIn the above equation we have considered that the stationary density is given by \\(\\rho_{ss} \\propto e^{-f}\\), and that the differential of \\(\\frac{\\delta \\mathcal{E}(\\rho_t)}{\\delta \\rho_t} = \\log \\rho + f\\).\nBy considering the Benamou-Brenier formulation (Benamou and Brenier 2000),(Ambrosio et al. 2003) of the Fokker-Planck dynamics we can obtain a better understanding on how the selected geometry (and metric) of the probability space influences the gradient flow dynamics. According to the Benamou-Brenier formalism the gradient flow dynamics for the Fokker-Planck equation has the following optimal transport interpretation: It describes a search over all possible vector fields \\(v_t\\) that will transport probability mass from \\(\\rho_0\\) to \\(\\rho_1\\), with the Wasserstein distance capturing the minimum possible cost of this transfer. Given two probability distributions \\(\\rho_0\\) and \\(\\rho_1\\), we define this distance to be the minimum of the integral of the norm of the vector field \\(v_t\\) \\[\\begin{equation}\nd^2_{OT} (\\rho_0, \\rho_1) = \\inf \\limits_{\\rho_t, v_t} \\int_0^1 \\| v_t \\|^2_{L^2(\\rho_t)} dt = \\mathcal{W}^2_2 (\\rho_0,\\rho_1),\n\\end{equation}\\] under the constraint that the transient probability distribution \\(\\rho_t\\) fulfils the continuity equation \\[\\begin{equation}\n    \\partial_t \\rho_t + \\nabla \\cdot (\\rho_t v_t) = 0,\n\\end{equation}\\] with \\(\\rho_0 = \\rho^0\\) and \\(\\rho_1 = \\rho^1\\). This constraint captures how the probability \\(\\rho_t\\) evolves while being pushed along the time dependent vector field \\(v_t\\). The Wasserstein distance is the minimal energy cost of performing this transformation from \\(\\rho_0\\) to \\(\\rho_1\\). This defines a metric on probability measures, and consequently it induces a geometry on the space of probabilities. (Here, \\(v_t\\) is the gradient of the local transport map.)"
  },
  {
    "objectID": "posts/Grad_flows.html#what-is-a-gradient-flow-in-the-probability-space",
    "href": "posts/Grad_flows.html#what-is-a-gradient-flow-in-the-probability-space",
    "title": "Wasserstein gradient flows",
    "section": "",
    "text": "Given some energy functional \\(\\mathcal{E}(\\rho)\\) in some probability space \\(\\mathcal{P}(\\Omega)\\) with some metric \\(\\mathcal{G}(\\rho))\\), \\((\\mathcal{P}(\\Omega), \\mathcal{G}(\\rho))\\), a gradient flow is defined as the inverse metric times the differential of the energy function \\[\\begin{equation}\n    \\partial_t \\rho_t = -\\mathcal{G}(\\rho_t)^{-1} \\frac{\\delta \\mathcal{E}(\\rho_t)}{\\delta \\rho_t}.\n\\end{equation}\\] Here, \\(\\rho_t\\) is a distribution at time \\(t\\).\nIntuitively, this means that the considered system of equations follows the trajectory of steepest descend on the energy functional \\(\\mathcal{E}(\\rho)\\). To define this steepest descend we need to define the notion of the gradient. The gradient, in turn, depends on the selected geometry of the space and is computed according to the selected metric.\nIf we consider for energy functional the Kullback Leibler divergence \\(D_{KL}\\), and for (information) metric the Wasserstein metric \\(\\mathcal{W}\\), the considered gradient flow, known as Wasserstein gradient flow, forms the Fokker-Planck equation.\nIn this case the metric inverse is \\(\\nabla \\cdot \\rho_t \\nabla\\), and we can derive the Fokker‚ÄìPlanck equation as follows:\n\\[\\begin{align}\n\\partial_t \\rho_t &= - \\text{grad}^{\\mathcal{W}} D_{KL}(\\rho_t ||\\rho_{ss})\\\\\n&= \\nabla \\cdot \\left(  \\rho_t \\nabla \\left( f + \\log \\rho_t +1 \\right)  \\right)\\\\\n&= \\nabla \\cdot \\left( \\rho_t \\nabla f\\right) + \\nabla \\cdot \\nabla \\rho_t\\\\\n&= \\nabla \\cdot \\left( \\rho_t \\nabla f\\right) + \\Delta \\rho_t.\n\\end{align}\\]\nIn the above equation we have considered that the stationary density is given by \\(\\rho_{ss} \\propto e^{-f}\\), and that the differential of \\(\\frac{\\delta \\mathcal{E}(\\rho_t)}{\\delta \\rho_t} = \\log \\rho + f\\).\nBy considering the Benamou-Brenier formulation (Benamou and Brenier 2000),(Ambrosio et al. 2003) of the Fokker-Planck dynamics we can obtain a better understanding on how the selected geometry (and metric) of the probability space influences the gradient flow dynamics. According to the Benamou-Brenier formalism the gradient flow dynamics for the Fokker-Planck equation has the following optimal transport interpretation: It describes a search over all possible vector fields \\(v_t\\) that will transport probability mass from \\(\\rho_0\\) to \\(\\rho_1\\), with the Wasserstein distance capturing the minimum possible cost of this transfer. Given two probability distributions \\(\\rho_0\\) and \\(\\rho_1\\), we define this distance to be the minimum of the integral of the norm of the vector field \\(v_t\\) \\[\\begin{equation}\nd^2_{OT} (\\rho_0, \\rho_1) = \\inf \\limits_{\\rho_t, v_t} \\int_0^1 \\| v_t \\|^2_{L^2(\\rho_t)} dt = \\mathcal{W}^2_2 (\\rho_0,\\rho_1),\n\\end{equation}\\] under the constraint that the transient probability distribution \\(\\rho_t\\) fulfils the continuity equation \\[\\begin{equation}\n    \\partial_t \\rho_t + \\nabla \\cdot (\\rho_t v_t) = 0,\n\\end{equation}\\] with \\(\\rho_0 = \\rho^0\\) and \\(\\rho_1 = \\rho^1\\). This constraint captures how the probability \\(\\rho_t\\) evolves while being pushed along the time dependent vector field \\(v_t\\). The Wasserstein distance is the minimal energy cost of performing this transformation from \\(\\rho_0\\) to \\(\\rho_1\\). This defines a metric on probability measures, and consequently it induces a geometry on the space of probabilities. (Here, \\(v_t\\) is the gradient of the local transport map.)"
  },
  {
    "objectID": "posts/Flatiron_CCNJunior_Theoretical_Neuro.html",
    "href": "posts/Flatiron_CCNJunior_Theoretical_Neuro.html",
    "title": "Gave a talk on my recent work at the Junior Theoretical Neuroscientist‚Äôs Workshop",
    "section": "",
    "text": "I had the honor to give a talk on my recent work at the Junior Theoretical Neuroscientist‚Äôs Workshop[ official website ] organised by the Flatiron Institute. Was great to be given the opportunity to get feedback on my work and reconect with some stellar junior theoretical neuroscientists."
  },
  {
    "objectID": "posts/21_08_03_probability_flow_dynamics.html",
    "href": "posts/21_08_03_probability_flow_dynamics.html",
    "title": "Probability flow dynamics for constraining stochastic nonlinear systems",
    "section": "",
    "text": "Biological and physical systems are often subjected to intrinsic or extrinsic noise sources that influence their dynamics. Characteristic examples include molecular reactions and chemical kinetics (Gillespie and Petzold 2003), populations of animal species, biological neurons (Saarinen, Linne, and Yli-Harja 2008), and evolution (Lande, Engen, and Saether 2003),(Takahata, Ishii, and Matsuda 1975). Stochastic differential equations (SDEs) effectively capture the phenomenology of the dynamics of such systems, at different precision scales by both considering deterministic and stochastic forces affecting their state variables \\(X_t \\in  \\mathcal{R}^d\\) following\n\\[dX_t = f(X_t,t) dt  + \\sigma dW_t. \\]\nIn Eq.\\((1)\\) the drift \\(f(\\cdot,\\cdot): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) is a smooth typically nonlinear function that captures the deterministic part of the driving forces, while \\(W\\) stands for a k‚Äìdimensional (\\(k\\leq d\\)) vector of independent Wiener processes acting as white noise sources, representing contributions from unaccounted degrees of freedom, thermal fluctuations, or external perturbations. We denote the noise strength by \\(\\sigma \\in \\mathcal{R}\\)1, and define the noise covariance as \\({D =\\sigma ^2}\\). In the following we refer to this system as the system.\nUnder multiple independent realisations, the stochastic nature of Eq.\\((1)\\) gives rise to an ensemble of trajectories starting from an initial state \\(X_0=x_0\\). This ensemble captures the likely evolution of the considered system at later time points. We may characterise the unfolding of this trajectory ensemble in terms of a probability density \\(p_t(x)\\), whose evolution is governed by the Fokker‚ÄìPlanck equation\n\\[\\frac{\\partial p_t(x)}{\\partial t}\n= \\nabla\\cdot \\left[- f(x,t) p_t (x) + \\frac{\\sigma^2}{2} \\nabla p_t(x)\\right]\\] \\[ \\hspace{-57pt}= {\\cal{L}}_f^\\dagger p_t(x) ,\\]\nwith initial condition \\(p_0(x) = \\delta(x-x_0)\\), and \\(\\mathcal{L}_f^\\dagger\\) denoting the Fokker‚ÄìPlanck operator. Due to the stochastic nature of the system of Eq.\\((1)\\), exact pinpointing of its state at some later time point \\(T\\) is in general not possible.\nYet, often, we desire to drive biophysical and biochemical stochastic processes to predefined target states within a specified time interval. Characteristic examples include designing artificial selection strategies for population dynamics (Nourmohammad and Eksin 2021), or triggering phenotype switches during cell fate determination (Wells, Kath, and Motter 2015). Similar needs for manipulation are also relevant for non-biological, but rather technical systems, e.g.¬†for control of robotic or artificial limbs (Todorov 2005), (Todorov 2004). In all these settings, external system interventions become essential.\nHere, we are interested in introducing constraints \\(\\mathcal{C}\\) to the dynamics of the system of Eq.(\\(1\\)) acting within a predefined time interval \\({0 \\leq t \\leq T}\\). The set of possible constraints \\(\\mathcal{C}\\) comprises terminal \\(\\chi(X_T)\\), and/or path constraints $U(x,t), tT $, depending on whether the desired limiting conditions apply for the entire interval or only to the terminal time point. The path constraints $U(x,t): ^{d} $ penalise specific trajectories (paths) to render specific regions of the state space more (un)likely to be visited, while the function \\(\\chi(x): \\mathcal{R}^{d} \\rightarrow \\mathcal{R}\\) influences the terminal system state \\(X_T\\).\nTo incorporate the constraints \\(\\mathcal{C}\\) into the system, we define a modified dynamics, the controlled dynamics, through a change of probability measure of the path ensemble \\(\\mathbb{P}_f\\) induced by the uncontrolled system. More precisely, we consider the path measure \\(\\mathbb{Q}\\) (Appendix A), induced by the controlled system, as equivalent to a reweighting of paths \\(X_{0:T}\\) generated from the uncontrolled dynamics (Eq.\\((1)\\)) over the time interval \\([0,\\; T]\\). Individual path weights are thus given by the likelihood ratio (Radon‚ÄìNikodym derivative)\n\\[\\frac{d\\mathbb{Q}}{d\\mathbb{P}_f} (X_{0:T}) = \\frac{\\chi(X_T)}{Z} \\exp\\left[- \\int_0^T U(X_t,t) dt \\right],\\]\nwhere \\(Z\\) denotes the normalising constant\n\\[Z = \\Bigg \\langle \\chi(X_T) \\exp\\left(- \\int_0^T U(X_t,t) dt \\right) \\Bigg\\rangle_{\\mathbb{P}_f},\\]\nand \\(\\langle \\cdot \\rangle_{\\mathbb{P}_f}\\) denotes the expectation over paths of the uncontrolled system.\nAccording to the Girsanov‚Äôs theorem, the controlled process defined by the weights of Eq.\\((4)\\) is also a diffusion process with the same diffusion constant \\(\\sigma\\), but with a modified, time-dependent drift function \\(g(x,t): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) (Girsanov 1960), (√òksendal 2003). Thus, we express the controlled dynamics as a time- and state- dependent perturbation \\(u(x,t): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) of the deterministic forces \\(f(x,t)\\) acting on the system\n\\[ dX_t = \\Big( f(X_t,t)   + u(X_t,t) \\Big) \\; dt + \\sigma dW_t \\] \\[= \\hspace{25pt}g(X_t,t)\\;\\hspace{5pt} dt \\hspace{30pt}+ \\sigma dW_t.\\]\nOur goal is to identify the optimal time- and state-dependent interventions \\(u(x,t)\\) that minimise intervention costs and path constraints captured by the cost function\n\\[S(x,u,t) =  \\frac{1}{2} u(x,t)^T H u(x,t)+ U(x,t),\\]\nwhile also drive the system towards a predefined target state \\(x^*\\) by time \\(T\\), if a terminal constraint is pertinent. The first part of the cost function penalises large intervention values \\(u(x,t)\\), with \\(H \\in \\mathcal{R}^{d \\times d}\\) determining the cost of intervention along each system dimension, whereas the path cost \\(U(x,t)\\) constrains the transient behaviour of the system.\nSolutions of this type of stochastic control problems rest on the Bellman‚Äôs principle of optimality, according to which an optimal solution over an interval \\([0,\\;T]\\) consists of optimal sub-solutions over the respective sub-intervals \\([t',\\;T]\\) with later starting times \\(t'\\), and appropriate initial conditions (Bellman 1956). This sequence of sub-problems with interdependent initial conditions requires the cost function \\(S(x,u,t)\\) to be minimized over the entire time interval \\([0,\\;T]\\). Therefore, here, we minimize the total expected cost in that interval defined as the sum of the terminal cost \\(\\chi(X_T)\\) and the time integrated path and intervention costs\n\\[ J(x,t=0) = \\min_{u} \\Big\\langle  \\int_{t=0}^T S(x,u,t') \\,  dt' -  \\ln \\chi(X_T) \\Big\\rangle_{\\mathbb{Q}}. \\] In Eq.\\((6)\\), the brackets \\(\\langle \\cdot \\rangle_{\\mathbb{Q}}\\) denote the expectation over the entire path probability measure \\(\\mathbb{Q}\\).\nTo establish the optimality of the interventions, we demand the cost functional \\(J(x,t)\\) to follow the Hamilton‚ÄìJacobi‚ÄìBellman (HJB) equation,\n\\[  -\\frac{\\partial}{\\partial t} J(X_t,t) = \\min_u \\Bigg[ \\frac{1}{2} u^T(X_t) H u(X_t) + U(X_t,t)\\] \\[\\hspace{95pt} + g(X_t,t) \\nabla_x J(X_t,t) + \\frac{1}{2} \\text{Tr}[D \\frac{\\partial^2}{\\partial x^2} J(X_t,t)]  \\Bigg] \\] a nonlinear partial differential equation (PDE) with a terminal condition \\(J(x,T)= \\ln \\chi(X_T)\\), which is, therefore, solved backwards in time. The gradient of the solution of this equation\n\\[u^*(x,t) = - H^{-1}  \\nabla J(x,t),\\]\nprovides the optimal state- and time-dependent interventions for the considered system with constraints \\(\\mathcal{C}\\). Yet, without investigating the structure of the solution, direct solving a second-order nonlinear PDE requires computationally demanding calculations, that grow exponentially with increasing system dimension.\nTo simplify matters, we linearise the Hamilton‚ÄìJacobi‚ÄìBellman equation by employing a logarithmic variable transformation, \\(J(x,t) = - \\log( \\phi(x,t))\\), proposed initially by Nelson in (Nelson 1967), and introduced in the context of stochastic control by Fleming in (Fleming 1977) (Hopf-Cole transform). This requires the minimal assumption of the control costs \\(H\\) and noise covariance \\(D\\) being inversely proportional along each state dimension, \\(H \\propto D^{-1}=\\sigma^{-2}\\), known in the literature as the path integral control condition (Kappen 2005).\nThe logarithmic variable transformation allows us to express the resulting controlled drift\n\\[g(x,t)  = f(x,t) +  \\sigma^2 \\nabla \\ln \\phi(x,t), \\]\nin terms of the solution \\(\\phi_t(x) \\doteq\\phi(x,t)\\) of a linear backward partial differential equation\n\\[\\frac{\\partial \\phi_t(x)}{\\partial t} + {\\cal{L}}_f \\phi_t(x) - U(x,t) \\phi_t(x) = 0 ,\\]\nwith terminal condition \\(\\phi_T(x) = \\chi(X_T)\\), and with \\(\\mathcal{L}_f\\) denoting theadjoint Fokker‚ÄìPlanck operator."
  },
  {
    "objectID": "posts/21_08_03_probability_flow_dynamics.html#dynamics-of-constrained-densities",
    "href": "posts/21_08_03_probability_flow_dynamics.html#dynamics-of-constrained-densities",
    "title": "Probability flow dynamics for constraining stochastic nonlinear systems",
    "section": "",
    "text": "Biological and physical systems are often subjected to intrinsic or extrinsic noise sources that influence their dynamics. Characteristic examples include molecular reactions and chemical kinetics (Gillespie and Petzold 2003), populations of animal species, biological neurons (Saarinen, Linne, and Yli-Harja 2008), and evolution (Lande, Engen, and Saether 2003),(Takahata, Ishii, and Matsuda 1975). Stochastic differential equations (SDEs) effectively capture the phenomenology of the dynamics of such systems, at different precision scales by both considering deterministic and stochastic forces affecting their state variables \\(X_t \\in  \\mathcal{R}^d\\) following\n\\[dX_t = f(X_t,t) dt  + \\sigma dW_t. \\]\nIn Eq.\\((1)\\) the drift \\(f(\\cdot,\\cdot): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) is a smooth typically nonlinear function that captures the deterministic part of the driving forces, while \\(W\\) stands for a k‚Äìdimensional (\\(k\\leq d\\)) vector of independent Wiener processes acting as white noise sources, representing contributions from unaccounted degrees of freedom, thermal fluctuations, or external perturbations. We denote the noise strength by \\(\\sigma \\in \\mathcal{R}\\)1, and define the noise covariance as \\({D =\\sigma ^2}\\). In the following we refer to this system as the system.\nUnder multiple independent realisations, the stochastic nature of Eq.\\((1)\\) gives rise to an ensemble of trajectories starting from an initial state \\(X_0=x_0\\). This ensemble captures the likely evolution of the considered system at later time points. We may characterise the unfolding of this trajectory ensemble in terms of a probability density \\(p_t(x)\\), whose evolution is governed by the Fokker‚ÄìPlanck equation\n\\[\\frac{\\partial p_t(x)}{\\partial t}\n= \\nabla\\cdot \\left[- f(x,t) p_t (x) + \\frac{\\sigma^2}{2} \\nabla p_t(x)\\right]\\] \\[ \\hspace{-57pt}= {\\cal{L}}_f^\\dagger p_t(x) ,\\]\nwith initial condition \\(p_0(x) = \\delta(x-x_0)\\), and \\(\\mathcal{L}_f^\\dagger\\) denoting the Fokker‚ÄìPlanck operator. Due to the stochastic nature of the system of Eq.\\((1)\\), exact pinpointing of its state at some later time point \\(T\\) is in general not possible.\nYet, often, we desire to drive biophysical and biochemical stochastic processes to predefined target states within a specified time interval. Characteristic examples include designing artificial selection strategies for population dynamics (Nourmohammad and Eksin 2021), or triggering phenotype switches during cell fate determination (Wells, Kath, and Motter 2015). Similar needs for manipulation are also relevant for non-biological, but rather technical systems, e.g.¬†for control of robotic or artificial limbs (Todorov 2005), (Todorov 2004). In all these settings, external system interventions become essential.\nHere, we are interested in introducing constraints \\(\\mathcal{C}\\) to the dynamics of the system of Eq.(\\(1\\)) acting within a predefined time interval \\({0 \\leq t \\leq T}\\). The set of possible constraints \\(\\mathcal{C}\\) comprises terminal \\(\\chi(X_T)\\), and/or path constraints $U(x,t), tT $, depending on whether the desired limiting conditions apply for the entire interval or only to the terminal time point. The path constraints $U(x,t): ^{d} $ penalise specific trajectories (paths) to render specific regions of the state space more (un)likely to be visited, while the function \\(\\chi(x): \\mathcal{R}^{d} \\rightarrow \\mathcal{R}\\) influences the terminal system state \\(X_T\\).\nTo incorporate the constraints \\(\\mathcal{C}\\) into the system, we define a modified dynamics, the controlled dynamics, through a change of probability measure of the path ensemble \\(\\mathbb{P}_f\\) induced by the uncontrolled system. More precisely, we consider the path measure \\(\\mathbb{Q}\\) (Appendix A), induced by the controlled system, as equivalent to a reweighting of paths \\(X_{0:T}\\) generated from the uncontrolled dynamics (Eq.\\((1)\\)) over the time interval \\([0,\\; T]\\). Individual path weights are thus given by the likelihood ratio (Radon‚ÄìNikodym derivative)\n\\[\\frac{d\\mathbb{Q}}{d\\mathbb{P}_f} (X_{0:T}) = \\frac{\\chi(X_T)}{Z} \\exp\\left[- \\int_0^T U(X_t,t) dt \\right],\\]\nwhere \\(Z\\) denotes the normalising constant\n\\[Z = \\Bigg \\langle \\chi(X_T) \\exp\\left(- \\int_0^T U(X_t,t) dt \\right) \\Bigg\\rangle_{\\mathbb{P}_f},\\]\nand \\(\\langle \\cdot \\rangle_{\\mathbb{P}_f}\\) denotes the expectation over paths of the uncontrolled system.\nAccording to the Girsanov‚Äôs theorem, the controlled process defined by the weights of Eq.\\((4)\\) is also a diffusion process with the same diffusion constant \\(\\sigma\\), but with a modified, time-dependent drift function \\(g(x,t): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) (Girsanov 1960), (√òksendal 2003). Thus, we express the controlled dynamics as a time- and state- dependent perturbation \\(u(x,t): \\mathcal{R}^d \\times \\mathcal{R} \\rightarrow \\mathcal{R}^d\\) of the deterministic forces \\(f(x,t)\\) acting on the system\n\\[ dX_t = \\Big( f(X_t,t)   + u(X_t,t) \\Big) \\; dt + \\sigma dW_t \\] \\[= \\hspace{25pt}g(X_t,t)\\;\\hspace{5pt} dt \\hspace{30pt}+ \\sigma dW_t.\\]\nOur goal is to identify the optimal time- and state-dependent interventions \\(u(x,t)\\) that minimise intervention costs and path constraints captured by the cost function\n\\[S(x,u,t) =  \\frac{1}{2} u(x,t)^T H u(x,t)+ U(x,t),\\]\nwhile also drive the system towards a predefined target state \\(x^*\\) by time \\(T\\), if a terminal constraint is pertinent. The first part of the cost function penalises large intervention values \\(u(x,t)\\), with \\(H \\in \\mathcal{R}^{d \\times d}\\) determining the cost of intervention along each system dimension, whereas the path cost \\(U(x,t)\\) constrains the transient behaviour of the system.\nSolutions of this type of stochastic control problems rest on the Bellman‚Äôs principle of optimality, according to which an optimal solution over an interval \\([0,\\;T]\\) consists of optimal sub-solutions over the respective sub-intervals \\([t',\\;T]\\) with later starting times \\(t'\\), and appropriate initial conditions (Bellman 1956). This sequence of sub-problems with interdependent initial conditions requires the cost function \\(S(x,u,t)\\) to be minimized over the entire time interval \\([0,\\;T]\\). Therefore, here, we minimize the total expected cost in that interval defined as the sum of the terminal cost \\(\\chi(X_T)\\) and the time integrated path and intervention costs\n\\[ J(x,t=0) = \\min_{u} \\Big\\langle  \\int_{t=0}^T S(x,u,t') \\,  dt' -  \\ln \\chi(X_T) \\Big\\rangle_{\\mathbb{Q}}. \\] In Eq.\\((6)\\), the brackets \\(\\langle \\cdot \\rangle_{\\mathbb{Q}}\\) denote the expectation over the entire path probability measure \\(\\mathbb{Q}\\).\nTo establish the optimality of the interventions, we demand the cost functional \\(J(x,t)\\) to follow the Hamilton‚ÄìJacobi‚ÄìBellman (HJB) equation,\n\\[  -\\frac{\\partial}{\\partial t} J(X_t,t) = \\min_u \\Bigg[ \\frac{1}{2} u^T(X_t) H u(X_t) + U(X_t,t)\\] \\[\\hspace{95pt} + g(X_t,t) \\nabla_x J(X_t,t) + \\frac{1}{2} \\text{Tr}[D \\frac{\\partial^2}{\\partial x^2} J(X_t,t)]  \\Bigg] \\] a nonlinear partial differential equation (PDE) with a terminal condition \\(J(x,T)= \\ln \\chi(X_T)\\), which is, therefore, solved backwards in time. The gradient of the solution of this equation\n\\[u^*(x,t) = - H^{-1}  \\nabla J(x,t),\\]\nprovides the optimal state- and time-dependent interventions for the considered system with constraints \\(\\mathcal{C}\\). Yet, without investigating the structure of the solution, direct solving a second-order nonlinear PDE requires computationally demanding calculations, that grow exponentially with increasing system dimension.\nTo simplify matters, we linearise the Hamilton‚ÄìJacobi‚ÄìBellman equation by employing a logarithmic variable transformation, \\(J(x,t) = - \\log( \\phi(x,t))\\), proposed initially by Nelson in (Nelson 1967), and introduced in the context of stochastic control by Fleming in (Fleming 1977) (Hopf-Cole transform). This requires the minimal assumption of the control costs \\(H\\) and noise covariance \\(D\\) being inversely proportional along each state dimension, \\(H \\propto D^{-1}=\\sigma^{-2}\\), known in the literature as the path integral control condition (Kappen 2005).\nThe logarithmic variable transformation allows us to express the resulting controlled drift\n\\[g(x,t)  = f(x,t) +  \\sigma^2 \\nabla \\ln \\phi(x,t), \\]\nin terms of the solution \\(\\phi_t(x) \\doteq\\phi(x,t)\\) of a linear backward partial differential equation\n\\[\\frac{\\partial \\phi_t(x)}{\\partial t} + {\\cal{L}}_f \\phi_t(x) - U(x,t) \\phi_t(x) = 0 ,\\]\nwith terminal condition \\(\\phi_T(x) = \\chi(X_T)\\), and with \\(\\mathcal{L}_f\\) denoting theadjoint Fokker‚ÄìPlanck operator."
  },
  {
    "objectID": "posts/21_08_03_probability_flow_dynamics.html#footnotes",
    "href": "posts/21_08_03_probability_flow_dynamics.html#footnotes",
    "title": "Probability flow dynamics for constraining stochastic nonlinear systems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the sake of brevity, we consider here a state independent diffusion, but the formalism easily generalises for a state dependent diffusion \\(\\sigma(x)\\), as outlined in the Appendix.}‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/Not_in_photo.html",
    "href": "posts/Not_in_photo.html",
    "title": "Off camera, still there",
    "section": "",
    "text": "Lately, I‚Äôve come to realize that when certain things are left unsaid openly un-documented, well-meaning people1 will bend the story according to how it fits their narrative, regardless of what actually happened. For more than a year I am fighting against someone who systematically tries to damage my credibility.\nBecause of that, I‚Äôve found myself in the uncomfortable position of having to share more personal details online than I normally would, just to preempt false narratives or quiet insinuations2.\nOne such case involves a recent group photo uploaded from a retreat organized in April 2024 by my previous research group. I am not pictured in that photo. And given past patterns, I expect this absence will be used, subtly or not, to imply that I didn‚Äôt participate in the retreat at all.\nSo, here is what actually happened.\nThe retreat took place from April 17‚Äì19, 2024. I was actively involved: I prepared a flash talk on my own work on identifying plasticity rules that explain differencial responses on visual stimuli depedning on familiarity, and co-prepared a joint talk with my colleagues Matt Getz and Pablo Crespo on our low-rank learning framework (presentation). First part was covered by matt, second by myself, and third by Pablo3.\nJust five days later, on April 24, I was scheduled to give a talk at the Flatiron Institute in New York (see here) on a completely different project (an independent one), this one was a continuation of my PhD work, focused on inference of latent stochastic low-dimensional dynamics from spiking neural activity.\nAs you can imagine, time was tight. I would return on Friday 19th evening home, and then depart on Monday 22th of April for New York. On April 18th, the group went for a walk, after which the photo in question was taken. I stayed back in the seminar room to prepare for my upcoming talk. So, obviously I am not in the photo, not because I was not in the retreat, but because I did not have free time for a walk4. In fact, we even joked at the time that one of the window reflections in the uncropped version of the picture looked vaguely like a human silhouette, so we could always say I was in the photo, just inside the building :).\nHere is a message I later found from my former colleague, Betsy Herbert, calling me down to join the group photo. I did not see it at the time, my phone was on silent, as it almost always is."
  },
  {
    "objectID": "posts/Not_in_photo.html#footnotes",
    "href": "posts/Not_in_photo.html#footnotes",
    "title": "Off camera, still there",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nor not so well-meaning ;)‚Ü©Ô∏é\nFor instance, in May 2024, the administrator responsible for handling my PhD documentation at my awarding institution contacted me to ask why people had recently reached out to verify, whether I had in fact earned my doctoral degree. This was puzzling to me, as my thesis had already been officially published online since December 2023 [thesis], I was actively holding a postdoctoral position, and even before I joined that lab, I had received an official preliminary certificate confirming the successful completion of my PhD with summa cum laude, with only remaining step the formal uploading of my thesis in the library, a purelly procedural step, not a condition for the degree itself. There was no indication whatsoever that the final degree could be withheld/not awarded etc. Indeed, when I eventually received the final diploma in January 2024, the document was dated early April 2023 (just a couple of days after my defense) [degree], aligning with the completion timeline. Nonetheless, during that period, when I was applying for postdoctoral positions and grants, and was in conversation with several labs, it became clear that someone had actively seeded doubts or vague insinuations suggesting, that I am leaving my postdoctoral lab because I do not have a PhD or some other degree. In the meanwhile, getting hired for my postdoc required submitting all my academic degrees, even my high school diploma, and both my first EE degree thesis [not so proud but here it is] and my doctoral thesis [PhD thesis] are available online ¬Ø_(„ÉÑ)_/¬Ø.‚Ü©Ô∏é\nThis was work done for the Master‚Äôs thesis of Pablo, who was co-supervised by me and matt. Pablo was performing the computations and was also providing ideas, and me and matt both were shaping the scope of the project and were formulating the specific questions Pablo tackled.‚Ü©Ô∏é\nFortunately, the talk wasn‚Äôt on work-life balance, because I‚Äôm clearly still working on that :)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/The_Brain_Conferences.html",
    "href": "posts/The_Brain_Conferences.html",
    "title": "Attended the Brain Conference: Frontiers of Theoretical Neuroscience and got a travel award",
    "section": "",
    "text": "A couple of weeks ago I had the honour to attend and present my recent work at the Brain Conference: Frontiers of Theoretical Neuroscience [ official website ] that took place in Rungsted Kyst in Denmark. I was also fortunate enough to get awarded a travel scholarship to participate.\nThe Brain Conferences are a series of conferences on neuroscience organised by the Lundbeck foundation taking place bi-annually, usually at Rungstedgaard in the east of Denmark1. This iteration was organised by Larry Abbott, Ila Fiete, and Haim Sompolinsky and was focused on Theoretical Neuroscience.\nOne of the highlights of the conference for me was the poster by the very talented Samuel Liebana from UCL (see picture below), presenting his recent paper on how dopamine dynamics explain the emergence of different behavioural strategies in mice.\nI got really excited about this work, not only because incidentally it happened to be the very first poster I visited in that meeting, but also because it is very closely related to a project I‚Äôve been developing and pitching since December 2024. The idea is to study learning dynamics in a continual learning setting and explain how prior structure resulting from previously learned task explains the emergence of diverse behavioural strategies.\nAt the time, the researchers I reached out to didn‚Äôt seem especially interested in the idea, or so it seemed from their reactions. But perhaps now, with growing experimental momentum in this direction, interest will be rekindled, and people will get more easily convinced."
  },
  {
    "objectID": "posts/The_Brain_Conferences.html#footnotes",
    "href": "posts/The_Brain_Conferences.html#footnotes",
    "title": "Attended the Brain Conference: Frontiers of Theoretical Neuroscience and got a travel award",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt that part of Denmark that is technically Sweden.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "M Dims Blog",
    "section": "",
    "text": "Glad that I got e-mail confimration from former postdoc advisor to pursue my ideas independently (lol)\n\n\n\nnews\n\n\n\nyou should always have written proof ;)\n\n\n\n\n\nAug 13, 2025\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nSuccessfully mentored six project groups at NMA summer schools\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 26, 2025\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nGave a talk on my recent work at the Junior Theoretical Neuroscientist‚Äôs Workshop\n\n\n\nnews\n\n\n\norganised by Simons Foundation and Flatiron\n\n\n\n\n\nJul 12, 2025\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nPresented my recent work at a Satellite meeting of StatPhys29\n\n\n\nnews\n\n\n\n\n\n\n\n\nJul 7, 2025\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nAttended the Brain Conference: Frontiers of Theoretical Neuroscience and got a travel award\n\n\n\nnews\n\n\n\n\n\n\n\n\nJun 23, 2025\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nLow tensor rank learning of neural dynamics\n\n\n\nblog\n\n\n\nor how to track changes in collective dynamics during learning (coming soon)\n\n\n\n\n\nMay 25, 2025\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nOff camera, still there\n\n\n\nblog\n\n\n\nFilling in some gaps\n\n\n\n\n\nApr 8, 2025\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nFrom PDEs to gradient flows for deterministic particle dynamics\n\n\n\nblog\n\njupyter\n\n\n\nIntroduction to deterministic particle methods for PDEs. (adapted text from notes of my PhD thesis - will be updated)\n\n\n\n\n\nNov 2, 2022\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nWasserstein gradient flows\n\n\n\nblog\n\n\n\nUnderstanding the JKO scheme (adapted text from my PhD thesis)\n\n\n\n\n\nApr 4, 2022\n\n\nDimitra Maoutsa\n\n\n\n\n\n\n\n\n\n\n\n\nProbability flow dynamics for constraining stochastic nonlinear systems\n\n\n\nblog\n\njupyter\n\n\n\nDetailed description of Deterministic Particle Flow control of (upcoming paper) on controling stochastic nonlinear systems by deterministacally perturbing their dynamics.\n\n\n\n\n\nAug 3, 2021\n\n\nDimitra Maoutsa\n\n\n\n\n\nNo matching items"
  }
]